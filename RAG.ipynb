{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkKbeMJSbD638GeJxUajH8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akhil7205/Rag-Question-Answers/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "_fWJ9eQqECEy"
      },
      "outputs": [],
      "source": [
        "gemini_api=\"AIzaSyDcipsoB3E1zJBJE1QZsYRPjrJp2IrB6TY\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "payload = {\n",
        "  \"contents\": [\n",
        "    {\n",
        "      \"parts\": [\n",
        "        {\n",
        "          \"text\": \"Explain how AI works in a few words\"\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "\n",
        "payload_json = json.dumps(payload)\n",
        "\n",
        "!curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\" \\\n",
        "  -H 'Content-Type: application/json' \\\n",
        "  -H 'X-goog-api-key: AIzaSyB2Kw5MjUDfWNNz8aWopdrRE0dEGiU5IpQ' \\\n",
        "  -X POST \\\n",
        "  -d \"$payload_json\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLHCo9inzlPC",
        "outputId": "6b7d1593-6796-4b4e-edd1-d754ec2ecd4b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"error\": {\n",
            "    \"code\": 400,\n",
            "    \"message\": \"Invalid JSON payload received. Unexpected token.\\ns: [{parts: [{text: Explain\\n                    ^\",\n",
            "    \"status\": \"INVALID_ARGUMENT\"\n",
            "  }\n",
            "}\n",
            "curl: (6) Could not resolve host: how\n",
            "curl: (6) Could not resolve host: AI\n",
            "curl: (6) Could not resolve host: works\n",
            "curl: (6) Could not resolve host: in\n",
            "curl: (6) Could not resolve host: a\n",
            "curl: (6) Could not resolve host: few\n",
            "curl: (3) unmatched close brace/bracket in URL position 6:\n",
            "words}]}]}\n",
            "     ^\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nDnd3hvlfX8",
        "outputId": "75cd6757-fa50-43d7-87cd-85c71f2bd822"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.29)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=0.3.75 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.75)\n",
            "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.16)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.24.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c51d5df8",
        "outputId": "7168d8cf-95ca-4c9e-a92c-b612b939878f"
      },
      "source": [
        "!pip install -U langchain-community"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.29)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=0.3.75 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.75)\n",
            "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.16)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.24.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader"
      ],
      "metadata": {
        "id": "ODdOA24hmI-q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path=\"/content/genai pdf.pdf\""
      ],
      "metadata": {
        "id": "vEUU6tWqmJcV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyCBT25wm3Bi",
        "outputId": "1afc485e-91c5-443c-9708-f62ae0f3c152"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader=PyPDFLoader(file_path)\n",
        "data=loader.load()"
      ],
      "metadata": {
        "id": "fBDw8gtdmo2t"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bP9_AmRlmw7A",
        "outputId": "5dce3f54-2970-481c-a3fd-5a67ea47b14b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 0, 'page_label': '1'}, page_content=''),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 1, 'page_label': '2'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nCONTENTS\\nIntroduction  ......................................................................................................................................................................................................... 3\\nThe Path to Deploying Production-Quality GenAI Applications ........................................................................................... 5 \\nStage 0: Foundation Models  ........................................................................................................................................................................................................................................................... 5 \\n Use Case: Introducing DBRX: A New State-of-the-Art Open LLM  ................................................................................................................................................................... 5 \\nStage 1: Prompt Engineering  ......................................................................................................................................................................................................................................................... 19 \\n Use Case: Automated Analysis of Product Reviews Using Large Language Models ........................................................................................................................ 20 \\nStage 2: Retrieval Augmented Generation  ....................................................................................................................................................................................................................... 25 \\n Use Case: Improve Your RAG Application Response Quality With Real-Time Structured Data  ................................................................................................ 27 \\nStage 3: Fine-Tuning a Foundation Model  ......................................................................................................................................................................................................................... 33 \\n Use Case: Creating a Bespoke LLM for AI-Generated Documentation  ..................................................................................................................................................... 34 \\n Use Case: Efficient Fine-Tuning With LoRA: A Guide to Optimal Parameter Selection for Large Language Models  ................................................... 43 \\nStage 4: Pretraining  ........................................................................................................................................................................................................................................................................... 60 \\n Use Case: Training Stable Diffusion From Scratch for <$50K With MosaicML  ..................................................................................................................................... 62 \\n Use Case: Deep Dive: How We Trained Stable Diffusion for Less Than $50K  ........................................................................................................................................ 68 \\nStage 5: LLM Evaluation  .................................................................................................................................................................................................................................................................... 81 \\n Use Case : Best Practices for LLM Evaluation of RAG Application  ................................................................................................................................................................. 82 \\n Use Case: Offline LLM Evaluation: Step-by-Step GenAI Application Assessment on Databricks ........................................................................................... 98\\nSummary  ............................................................................................................................................................................................................. 117 \\nGenAI Training  ......................................................................................................................................................................................................................................................................................... 117 \\nAdditional Resources  ........................................................................................................................................................................................................................................................................ 117\\n2'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 2, 'page_label': '3'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nAchieving Production-Quality GenAI Requires New Tools and Skills\\nGenerative AI has opened new worlds of possibilities for businesses and is being emphatically embraced \\nacross organizations. According to a recent MIT Tech Review report, all 600 CIOs surveyed stated they are \\nincreasing their investment in AI, and 71% are planning to build their own custom large language models (LLMs) \\nor other GenAI models. However, many organizations have found it challenging to deploy these applications at \\nproduction quality. To meet the standard of quality required for customer-facing applications, AI output must \\nbe accurate, governed and safe. \\nData Infrastructure Must Evolve to Support  \\nGenAI-Powered Applications\\nMaking the leap to generative AI is not just about deploying a chatbot; it requires a reshaping of the foundational \\naspects of data management. Central to this transformation is the emergence of data lakehouses as the new \\n“modern data stack.” These advanced data architectures are essential to harnessing the full potential of GenAI, \\ndriving faster, more cost-effective and wider democratization of data and AI technologies. As businesses \\nincreasingly rely on GenAI-powered tools and applications for competitive advantage, the underlying data \\ninfrastructure must evolve to support these advanced technologies effectively and securely.\\nNo Matter Where You Are on Your Path to Deploying GenAI Applications, \\nthe Quality of Your Data Matters\\nBusinesses need to achieve production quality with their GenAI applications. Developers need rich tools for \\nunderstanding the quality of their data and model outputs, along with an underlying platform that lets them \\ncombine and optimize all aspects of the GenAI process. GenAI has many components such as data preparation, \\nretrieval models, language models (either SaaS or open source), ranking and post-processing pipelines, prompt \\nengineering, and training models on custom enterprise data.\\nTo help you overcome common enterprise challenges with building GenAI, we’ve compiled a collection of \\ntechnical content and code samples. We’ll start each section with a brief overview and then provide use cases \\nand example code for reference. \\nIntroduction\\n3'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 3, 'page_label': '4'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nIn this eBook, you’ll learn: \\n ■ How to plan a path from basic to advanced GenAI applications, leveraging your organization’s data\\n ■ How to use retrieval augmented generation (RAG) to make an off-the-shelf AI system smarter\\n ■ How to evaluate LLMs and where you want to invest in more powerful AI tools and systems that drive \\nmore significant operational gain\\n ■ How to build a custom LLM that may be better, faster and cheaper for your organization\\n ■ When it might be worth it to pretrain your own model — and more\\nUse cases for GenAI covered:\\n ■ How to use LLMs to gain actionable insights from product reviews\\n ■ How to use RAG for a chatbot to improve the quality of output\\n ■ How to train your own generative AI model in a cost-effective manner\\n ■ How to monitor and evaluate your deployed LLMs and GenAI applications\\n4'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 4, 'page_label': '5'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nThe Path to Deploying \\nProduction-Quality \\nGenAI Applications\\nStage 0: Foundation Models\\nBefore setting off to create production-quality GenAI applications, we need to cover the base language models \\nthat serve as the foundation for layers of increasingly complex techniques. Foundation models commonly refer \\nto large language models that have been trained over extensive datasets to be generally good at some task \\n(chat, instruction following, code generation, etc.).\\nWe won’t cover many models, as it is a constantly shifting landscape, but it is important to note that while \\nunderlying architectures may differ drastically, foundation models generally fall under two categories: \\nproprietary (such as GPT-3.5 and Gemini) and open source (such as Llama2-70B and DBRX). The main difference \\nbetween the two is that while proprietary models historically have an edge on outright performance, users have \\nto send their data out to a third party and don’t have control over the underlying model as they’re often being \\nupdated and changed. \\nOpen source models, on the other hand, offer users full control over the model and the ability to run it on their \\nown terms with their own governance and data privacy. Here’s a current list of many open source GenAI models \\nacross different domains that are all free for commercial use. Databricks has also created their own state-of-\\nthe-art open source foundation model so users can build the highest-quality production GenAI applications.\\nFoundation Model Use Case\\nINTRODUCING DBRX: A NEW STATE-OF-THE-ART OPEN LLM\\nWe are excited to introduce DBRX, an open, general-purpose LLM created by Databricks. Across a range of \\nstandard benchmarks, DBRX sets a new state-of-the-art for established open LLMs. Moreover, it provides \\nthe open community and enterprises building their own LLMs with capabilities that were previously limited \\nto closed model APIs; according to our measurements, it surpasses GPT-3.5, and it is competitive with \\nGemini 1.0 Pro. It is an especially capable code model, surpassing specialized models like CodeLLaMA-70B on \\nprogramming, in addition to its strength as a general-purpose LLM.\\n5'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 5, 'page_label': '6'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nThis state-of-the-art quality comes with marked improvements in training and inference performance. DBRX \\nadvances the state-of-the-art in efficiency among open models thanks to its fine-grained mixture-of-experts \\n(MoE) architecture. Inference is up to 2x faster than LLaMA2-70B, and DBRX is about 40% of the size of Grok-1 in \\nterms of both total and active parameter-counts. When hosted on Mosaic AI Model Serving, DBRX can generate \\ntext at up to 150 tok/s/user. Our customers will find that training MoEs is also about 2x more FLOP-efficient \\nthan training dense models for the same final model quality. End-to-end, our overall recipe for DBRX (including \\nthe pretraining data, model architecture, and optimization strategy) can match the quality of our previous-\\ngeneration MPT models with nearly 4x less compute.\\nFigure 1: DBRX outperforms established open source models on language understanding (MMLU), \\nProgramming (HumanEval), and Math (GSM8K).\\n6'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 6, 'page_label': '7'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nThe weights of the base model (DBRX Base) and the fine-tuned model (DBRX Instruct) are available on Hugging \\nFace under an open license. Starting today, DBRX is available for Databricks customers to use via APIs, and \\nDatabricks customers can pretrain their own DBRX-class models from scratch or continue training on top of  \\none of our checkpoints using the same tools and science we used to build it. DBRX is already being integrated \\ninto our GenAI-powered products, where — in applications like SQL — early rollouts have surpassed GPT-3.5 \\nTurbo and are challenging GPT-4 Turbo. It is also a leading model among open models and GPT-3.5 Turbo on \\nRAG tasks.\\nTraining mixture-of-experts models is hard. We had to overcome a variety of scientific and performance \\nchallenges to build a pipeline robust enough to repeatedly train DBRX-class models in an efficient manner. Now \\nthat we have done so, we have a one-of-a-kind training stack that allows any enterprise to train world-class MoE \\nfoundation models from scratch. We look forward to sharing that capability with our customers and sharing our \\nlessons learned with the community.\\nDownload DBRX today from Hugging Face (DBRX Base, DBRX Instruct), or try out DBRX Instruct in our HF Space, \\nor see our model repository on github: databricks/dbrx.\\nWhat Is DBRX?\\nDBRX is a transformer-based decoder-only large language model (LLM) that was trained using next-token \\nprediction. It uses a fine-grained mixture-of-experts (MoE) architecture with 132B total parameters of which \\n36B parameters are active on any input. It was pre-trained on 12T tokens of text and code data. Compared \\nto other open MoE models like Mixtral and Grok-1, DBRX is fine-grained, meaning it uses a larger number of \\nsmaller experts. DBRX has 16 experts and chooses 4, while Mixtral and Grok-1 have 8 experts and choose 2. This \\nprovides 65x more possible combinations of experts and we found that this improves model quality. DBRX uses \\nrotary position encodings (RoPE), gated linear units (GLU), and grouped query attention (GQA). It uses the GPT-\\n4 tokenizer as provided in the tiktoken repository. We made these choices based on exhaustive evaluation and \\nscaling experiments.\\n7'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 7, 'page_label': '8'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nDBRX was pretrained on 12T tokens of carefully curated data and a maximum context length of 32k tokens. We \\nestimate that this data is at least 2x better token-for-token than the data we used to pretrain the MPT family of \\nmodels. This new dataset was developed using the full suite of Databricks tools, including Apache Spark™ and \\nDatabricks notebooks for data processing, Unity Catalog for data management and governance, and MLflow for \\nexperiment tracking. We used curriculum learning for pretraining, changing the data mix during training in ways \\nwe found to substantially improve model quality.\\nQuality on Benchmarks vs. Leading Open Models\\nTable 1 shows the quality of DBRX Instruct and leading established, open models. DBRX Instruct is the leading \\nmodel on composite benchmarks, programming and mathematics benchmarks, and MMLU. It surpasses all chat \\nor instruction fine-tuned models on standard benchmarks.\\nComposite benchmarks. We evaluated DBRX Instruct and peers on two composite benchmarks: the Hugging \\nFace Open LLM Leaderboard (the average of ARC-Challenge, HellaSwag, MMLU, TruthfulQA, WinoGrande,  \\nand GSM8k) and the Databricks Model Gauntlet (a suite of over 30 tasks spanning six categories: world \\nknowledge, commonsense reasoning, language understanding, reading comprehension, symbolic problem \\nsolving, and programming).\\nAmong the models we evaluated, DBRX Instruct scores the highest on two composite benchmarks: the Hugging \\nFace Open LLM Leaderboard (74.5% vs. 72.7% for the next highest model, Mixtral Instruct) and the Databricks \\nGauntlet (66.8% vs. 60.7% for the next highest model, Mixtral Instruct).\\nProgramming and mathematics. DBRX Instruct is especially strong at programming and mathematics. It scores \\nhigher than the other open models we evaluated on HumanEval (70.1% vs. 63.2% for Grok-1, 54.8% for Mixtral \\nInstruct, and 32.2% for the best-performing LLaMA2-70B variant) and GSM8k (66.9% vs. 62.9% for Grok-1, 61.1% \\nfor Mixtral Instruct, and 54.1% for the best-performing LLaMA2-70B variant). DBRX outperforms Grok-1, the next \\nbest model on these benchmarks, despite the fact that Grok-1 has 2.4x as many parameters. On HumanEval, \\nDBRX Instruct even surpasses CodeLLaMA-70B Instruct, a model built explicitly for programming, despite the \\nfact that DBRX Instruct is designed for general-purpose use (70.1% vs. 67.8% on HumanEval as reported by Meta \\nin the CodeLLaMA blog).\\nMMLU. DBRX Instruct scores higher than all other models we consider on MMLU, reaching 73.7%.\\n8'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 8, 'page_label': '9'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nMODEL DBRX  \\nINSTRUCT\\nMIXTRAL \\nINSTRUCT\\nMIXTRAL \\nBASE\\nLLAMA2-70  \\nB CHAT\\nLLAMA2-70  \\nB BASE GROK-11\\nOpen LLM Leaderboard2 \\n(Avg of next 6 rows) 74.5% 72.7% 68.4% 62.4% 67.9% —\\nARC-challenge 25-shot 68.9% 70.1% 66.4% 64.6% 67.3% —\\nHellaSwag 10-shot 89.0% 87.6% 86.5% 85.9% 87.3% —\\nMMLU 5-shot 73.7% 71.4% 71.9% 63.9% 69.8% 73.0%\\nTruthful QA 0-shot 66.9% 65.0% 46.8% 52.8% 44.9% —\\nWinoGrande 5-shot 81.8% 81.1% 81.7% 80.5% 83.7% —\\nGSM8k CoT 5-shot \\nmaj@13 66.9% 61.1% 57.6% 26.7% 54.1% 62.9% \\n(8-shot)\\nGauntlet v0.34 \\n(Avg of 30+ diverse tasks) 66.8% 60.7% 56.8% 52.8% 56.4% —\\nHumanEval5 \\n0-Shot, pass@1 \\n(Programming)\\n70.1% 54.8% 40.2% 32.2% 31.0% 63.2%\\nLLaMA2-70B Base\\nTable 1: Quality of DBRX Instruct and leading open models. See footnotes for details on how numbers were collected. \\nBolded and underlined is the highest score.\\n9'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 9, 'page_label': '10'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nQuality on Benchmarks vs. Leading Closed Models\\nTable 2 shows the quality of DBRX Instruct and leading closed models. According to the scores reported by \\neach model creator, DBRX Instruct surpasses GPT-3.5 (as described in the GPT-4 paper), and it is competitive \\nwith Gemini 1.0 Pro and Mistral Medium.\\nAcross nearly all benchmarks we considered, DBRX Instruct surpasses or - at worst - matches GPT-3.5. DBRX \\nInstruct outperforms GPT-3.5 on general knowledge as measured by MMLU (73.7% vs. 70.0%) and commonsense \\nreasoning as measured by HellaSwag (89.0% vs. 85.5%) and WinoGrande (81.8% vs. 81.6%). DBRX Instruct \\nespecially shines on programming and mathematical reasoning as measured by HumanEval (70.1% vs. 48.1%) and \\nGSM8k (72.8% vs. 57.1%).\\nDBRX Instruct is competitive with Gemini 1.0 Pro and Mistral Medium. Scores for DBRX Instruct are higher than \\nGemini 1.0 Pro on Inflection Corrected MTBench, MMLU, HellaSwag, and HumanEval, while Gemini 1.0 Pro is \\nstronger on GSM8k. Scores for DBRX Instruct and Mistral Medium are similar for HellaSwag, while Mistral Medium \\nis stronger on Winogrande and MMLU and DBRX Instruct is stronger on HumanEval, GSM8k, and Inflection \\nCorrected MTBench.\\n10'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 10, 'page_label': '11'}, page_content='MODEL DBRX  \\nINSTRUCT GPT-3.57 GPT-48 CLAUDE  \\n3 HAIKU\\nCLAUDE 3 \\nSONNET\\nCLAUDE 3 \\nOPUS\\nGEMINI  \\n1.0 PRO\\nGEMINI  \\n1.5 PRO\\nMISTRAL  \\nMEDIUM\\nMISTRAL \\nLARGE\\nMT Bench  \\n(Inflection corrected , n=5) 8.39 ± 0.08 — — 8.41 ± \\n0.04 \\n8.54 ± \\n0.09\\n9.03 ± \\n0.06\\n8.23 ± 0.08 — 8.05 ± 0.12 8.90 ± \\n0.06\\nMMLU 5-shot 73.7% 70.0% 86.4% 75.2% 79.0% 86.8% 71.8% 81.9% 75.3% 81.2%\\nHellaSwag 10-shot 89.0% 85.5% 95.3% 85.9% 89.0% 95.4% 84.7% 92.5% 88.0% 89.2%\\nHumanEval 0-Shot \\npass@1 \\n(Programming)\\n70.1%  \\ntemp=0, \\nN=1\\n48.1% 67.0% 75.9% 73.0% 84.9% 67.7% 71.9% 38.4% 45.1%\\nGSM8k CoT maj@1 72.8% \\n(5-shot)\\n57.1%  \\n(5-shot)\\n92.0%  \\n(5-shot) 88.9% 92.3% 95.0%\\n86.5% \\n(maj1@32)\\n91.7%  \\n(11-shot)\\n66.7%  \\n(5-shot)\\n81.0%  \\n(5-shot)\\nWinoGrande 5-shot 81.8% 81.6% 87.5% — — — — — 88.0% 86.7%\\nTable 2: Quality of DBRX Instruct and leading closed models. Other than Inflection Corrected MTBench (which we measured ourselves on model \\nendpoints), numbers were as reported by the creators of these models in their respective whitepapers. See footnotes for additional details.\\n11\\nTHE BIG BOOK OF GENERATIVE AI'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 11, 'page_label': '12'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nQuality on Long-Context Tasks and RAG\\nDBRX Instruct was trained with up to a 32K token context window. Table 3 compares its performance to that of \\nMixtral Instruct and the latest versions of the GPT-3.5 Turbo and GPT-4 Turbo APIs on a suite of long-context \\nbenchmarks (KV-Pairs from the Lost in the Middle paper and HotpotQAXL, a modified version of HotPotQA that \\nextends the task to longer sequence lengths). GPT-4 Turbo is generally the best model at these tasks. However, \\nwith one exception, DBRX Instruct performs better than GPT-3.5 Turbo at all context lengths and all parts of the \\nsequence. Overall performance for DBRX Instruct and Mixtral Instruct are similar.\\nMODEL DBRX  \\nINSTRUCT\\nMIXTRAL  \\nINSTRUCT\\nGPT-3.5 TURBO \\n(API)\\nGPT-4 TURBO \\n(API)\\nAnswer in Beginning Third of Context 45.1% 41.3% 37.3%* 49.3%\\nAnswer in Middle Third of Context 45.3% 42.7% 37.3%* 49.0%\\nAnswer in Last Third of Context 48.0% 44.4% 37.0%* 50.9%\\n2K Context 59.1% 64.6% 36.3% 69.3%\\n4K Context 65.1% 59.9% 35.9% 63.5%\\n8K Context 59.5% 55.3% 45.0% 61.5%\\n16K Context 27.0% 20.1% 31.7% 26.0%\\n32K Context 19.9% 14.0% — 28.5%\\nTable 3: The average performance of models on the KV-Pairs and HotpotQAXL benchmarks. Bold is the highest score. Underlined is the highest score \\nother than GPT-4 Turbo. GPT-3.5 Turbo supports a maximum context length of 16K, so we could not evaluate it at 32K. *Averages for the beginning, \\nmiddle, and end of the sequence for GPT-3.5 Turbo include only contexts up to 16K.\\n12'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 12, 'page_label': '13'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nOne of the most popular ways to leverage a model’s context is retrieval augmented generation (RAG). \\nIn RAG, content relevant to a prompt is retrieved from a database and presented alongside the prompt to \\ngive the model more information than it would otherwise have. Table 4 shows the quality of DBRX on two RAG \\nbenchmarks — Natural Questions and HotPotQA — when the model is also provided with the top 10 passages \\nretrieved from a corpus of Wikipedia articles using the embedding model bge-large-en-v1.5. DBRX Instruct  \\nis competitive with open models like Mixtral Instruct and LLaMA2-70B Chat and the current version  \\nof GPT-3.5 Turbo.\\nMODEL DBRX  \\nINSTRUCT\\nMIXTRAL  \\nINSTRUCT\\nLLAMA2-70B \\nCHAT\\nGPT 3.5 TUR -\\nBO (API)\\nGPT 4 TURBO \\n(API)\\nNatural Questions 60.0% 59.1% 56.5% 57.7% 63.9%\\nHotPotQA 55.0% 54.2% 54.7% 53.0% 62.9%\\nTable 4: The performance of the models measured when each model is given the top 10 passages retrieved from a Wikipedia corpus \\nusing bge-large-en-v1.5. Accuracy is measured by matching within the model’s answer. Bold is the highest score. Underlined is the \\nhighest score other than GPT-4 Turbo. \\nTraining Efficiency\\nModel quality must be placed in the context of how efficient the model is to train and use. This is especially \\nso at Databricks, where we build models like DBRX to establish a process for our customers to train their own \\nfoundation models.\\nWe found training mixture-of-experts models to provide substantial improvements in compute-efficiency for \\ntraining (Table 5). For example, training a smaller member of the DBRX family called DBRX MoE-B (23.5B total \\nparameters, 6.6B active parameters) required 1.7x fewer FLOPs to reach a score of 45.5% on the Databricks LLM \\nGauntlet than LLaMA2-13B required to reach 43.8%. DBRX MoE-B also contains half as many active parameters \\nas LLaMA2-13B.\\n13'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 13, 'page_label': '14'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nLooking holistically, our end-to-end LLM pretraining pipeline has become nearly 4x more compute-efficient \\nin the past ten months. On May 5, 2023, we released MPT-7B, a 7B parameter model trained on 1T tokens that \\nreached a Databricks LLM Gauntlet score of 30.9%. A member of the DBRX family called DBRX MoE-A (7.7B total \\nparameters, 2.2B active parameters) reached a Databricks Gauntlet score of 30.5% with 3.7x fewer FLOPs. This \\nefficiency is the result of a number of improvements, including using an MoE architecture, other architecture \\nchanges to the network, better optimization strategies, better tokenization, and - very importantly - better \\npretraining data.\\nIn isolation, better pretraining data made a substantial impact on model quality. We trained a 7B model on 1T \\ntokens (called DBRX Dense-A) using the DBRX pretraining data. It reached 39.0% on the Databricks Gauntlet \\ncompared to 30.9% for MPT-7B. We estimate that our new pretraining data is at least 2x better token-for-token \\nthan the data used to train MPT-7B. In other words, we estimate that half as many tokens are necessary to reach \\nthe same model quality. We determined this by training DBRX Dense-A on 500B tokens; it outperformed MPT-7B \\non the Databricks Gauntlet, reaching 32.1%. In addition to better data quality, another important contributor to \\nthis token-efficiency may be the GPT-4 tokenizer, which has a large vocabulary and is believed to be especially \\ntoken-efficient. These lessons about improving data quality translate directly into practices and tools that our \\ncustomers use to train foundation models on their own data.\\nMODEL TOTAL PARAMS ACTIVE PARAMS GAUNTLET SCORE RELATIVE FLOPS\\nDBRX MoE-A 7.7B 2.2B 30.5% 1x\\nMPT-7B (1T tokens) — 6.7B 30.9% 3.7x\\nDBRX Dense-A (1T tokens) — 6.7B 39.0% 3.7x\\nDBRX Dense-A (500B tokens) — 6.7B 32.1% 1.85x\\nDBRX MoE-B 23.5B 6.6B 45.5% 1x\\nLLaMA2-13B — 13.0B 43.8% 1.7x\\nTable 5:  Details of several test articles that we used to validate the training efficiency of the DBRX MoE architecture and end-to-end training pipeline.\\n14'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 14, 'page_label': '15'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nInference Efficiency\\nFigure 2 shows the end-to-end inference efficiency of serving DBRX and similar models using NVIDIA \\nTensorRT-LLM with our optimized serving infrastructure and 16-bit precision. We aim for this benchmark \\nto reflect real-world usage as closely as possible, including multiple users simultaneously hitting the same \\ninference server. We spawn one new user per second, each user request contains an approximately 2000 \\ntoken prompt, and each response comprises 256 tokens.\\nIn general, MoE models are faster at inference than their total parameter-counts would suggest. This is due \\nto the fact that they use relatively few parameters for each input. We find that DBRX is no exception in this \\nrespect. DBRX inference throughput is 2-3x higher than a 132B non-MoE model.\\nInference efficiency and model quality are typically in tension: bigger models typically reach higher quality, but \\nsmaller models are more efficient for inference. Using an MoE architecture makes it possible to attain better \\ntradeoffs between model quality and inference efficiency than dense models typically achieve. For example, \\nDBRX is both higher quality than LLaMA2-70B and - thanks to having about half as many active parameters - \\nDBRX inference throughput is up to 2x faster (Figure 2). Mixtral is another point on the improved pareto frontier \\nattained by MoE models: it is smaller than DBRX, and it is correspondingly lower in terms of quality but reaches \\nhigher inference throughput. Users of the Databricks Foundation Model APIs can expect to see up to 150 \\ntokens per second for DBRX on our optimized model serving platform with 8-bit quantization.\\n15'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 15, 'page_label': '16'}, page_content='Figure 2:  Inference throughput for various model configurations on our optimized serving infrastructure using NVIDIA TensorRT-LLM at 16-bit \\nprecision with the best optimization flags we could find. Models are run in tensor-parallel across the entire node. The input prompt contains \\napproximately 2000 prompt tokens and we generate 256 output tokens. One new user spawns every second.\\n16\\nTHE BIG BOOK OF GENERATIVE AI'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 16, 'page_label': '17'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nHow We Built DBRX\\nDBRX was trained on 3072 NVIDIA H100s connected by 3.2Tbps Infiniband. The main process of building DBRX \\n- including pretraining, post-training, evaluation, red-teaming, and refining - took place over the course of \\nthree months. It was the continuation of months of science, dataset research, and scaling experiments, not to \\nmention years of LLM development at Databricks that includes the MPT and Dolly projects and the thousands \\nof models we have built and brought to production with our customers.\\nTo build DBRX, we leveraged the same suite of Databricks tools that are available to our customers. We \\nmanaged and governed our training data using Unity Catalog. We explored this data using newly acquired  \\nLilac AI. We processed and cleaned this data using Apache Spark™ and Databricks notebooks. We trained \\nDBRX using optimized versions of our open-source training libraries: MegaBlocks, LLM Foundry, Composer, \\nand Streaming. We managed large scale model training and finetuning across thousands of GPUs using our \\nMosaic AI Training service. We logged our results using MLflow. We collected human feedback for quality and \\nsafety improvements through Mosaic AI Model Serving and Inference Tables. We manually experimented with \\nthe model using the Databricks Playground. We found the Databricks tools to be best-in-class for each of their \\npurposes, and we benefited from the fact that they were all part of a unified product experience.\\nGet Started With DBRX on Databricks\\nIf you’re looking to start working with DBRX right away, it’s easy to do so with the Databricks Mosaic AI \\nFoundation Model APIs. You can quickly get started with our pay-as-you-go pricing and query the model from \\nour AI Playground chat interface. For production applications, we offer a provisioned throughput option to \\nprovide performance guarantees, support for finetuned models, and additional security and compliance. To \\nprivately host DBRX, you can download the model from the Databricks Marketplace and deploy the model on \\nModel Serving.\\n17'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 17, 'page_label': '18'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nConclusions\\nAt Databricks, we believe that every enterprise should have the ability to control its data and its destiny in the \\nemerging world of GenAI. DBRX is a central pillar of our next generation of GenAI products, and we look forward \\nto the exciting journey that awaits our customers as they leverage the capabilities of DBRX and the tools we \\nused to build it. In the past year, we have trained thousands of LLMs with our customers. DBRX is only one \\nexample of the powerful and efficient models being built at Databricks for a wide range of applications, from \\ninternal features to ambitious use-cases for our customers.\\nAs with any new model, the journey with DBRX is just the beginning, and the best work will be done by those \\nwho build on it: enterprises and the open community. This is also just the beginning of our work on DBRX, and \\nyou should expect much more to come.\\nContributions\\nThe development of DBRX was led by the Mosaic team that previously built the MPT model family, in \\ncollaboration with dozens of engineers, lawyers, procurement and finance specialists, program managers, \\nmarketers, designers, and other contributors from across Databricks. We are grateful to our colleagues, friends, \\nfamily, and the community for their patience and support over the past months.\\nIn creating DBRX, we stand on the shoulders of giants in the open and academic community. By making DBRX \\navailable openly, we intend to invest back in the community in hopes that we will build even greater technology \\ntogether in the future. With that in mind, we gratefully acknowledge the work and collaboration of Trevor Gale \\nand his MegaBlocks project (Trevor’s PhD adviser is Databricks CTO Matei Zaharia), the PyTorch team and \\nthe FSDP project, NVIDIA and the TensorRT-LLM project, the vLLM team and project, EleutherAI and their \\nLLM evaluation project, Daniel Smilkov and Nikhil Thorat at Lilac AI, and our friends at the Allen Institute for \\nArtificial Intelligence (AI2).\\n18'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 18, 'page_label': '19'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nStage 1: Prompt Engineering\\nMany companies still remain in the foundational stages of adopting generative AI technology. They have  \\nno overarching AI strategy in place, no clear use cases to pursue and no access to a team of data scientists and \\nother professionals who can help guide the company’s AI adoption journey.\\nIf this is like your business, a good starting point is an off-the-shelf LLM. While these LLMs lack the domain-\\nspecific expertise of custom AI models, experimentation can help you plot your next steps. Your employees can \\ncraft specialized prompts and workflows to guide their usage. Your leaders can get a better understanding of \\nthe strengths and weaknesses of these tools as well as a clearer vision of what early success in AI might look \\nlike. Your organization can use things like the Databricks AI Playground to figure out where to invest in more \\npowerful AI tools and systems that drive more significant operational gain and even use LLMs as a judge to help \\nevaluate responses.\\nPRACTICAL APPLICATIONS OF GENAI TECHNOLOGY\\nLet’s delve into a compelling use case that illustrates the power of prompt engineering with off-the-shelf \\nLLMs. Consider the challenge many businesses face: sifting through vast amounts of product reviews  \\nto glean actionable insights. Without a dedicated team of data scientists or a clear AI strategy, this task  \\nmight seem daunting. However, leveraging the flexibility of LLMs through prompt engineering offers a \\nstraightforward solution.\\n19'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 19, 'page_label': '20'}, page_content=\"THE BIG BOOK OF GENERATIVE AI\\nPrompt Engineering Use Case\\nAutomated Analysis of Product Reviews Using Large Language Models\\nKeep track of customer feedback at scale\\nCheck out our LLM Solution Accelerators for Retail for more details and to download the notebooks.\\nWhile conversational AI has garnered a lot of media attention in recent months, the capabilities of large \\nlanguage models (LLMs) extend well beyond conversational interactions. It's in these less prominent  \\ncapabilities such as query response, summarization, classification and search that many organizations  \\nare finding immediate opportunities to supercharge their workforce and up-level customer experiences.\\nThe potential of these applications is staggering. By one estimate, LLMs (and other generative AI \\n technologies) could, in the near future, address tasks that today occupy 60%–70% of employees’ time.  \\nThrough augmentation, numerous studies have shown that the time to complete various tasks performed  \\nby knowledge workers such as background research, data analysis and document writing can be cut in half.  \\nAnd still other studies have shown that the use of these technologies can dramatically reduce the time for  \\nnew workers to achieve full productivity.\\nBut before these benefits can be fully realized, organizations must first rethink the management of the \\nunstructured information assets on which these models depend and find ways to mitigate the issues of bias \\nand accuracy that affect their output. This is why so many organizations are currently focusing their efforts \\non focused, internal applications where a limited scope provides opportunities for better information access \\nand human oversight can serve as a check to errant results. These applications, aligned with core capabilities \\nalready residing within the organization, have the potential to deliver real and immediate value, while LLMs and \\ntheir supporting technologies continue to evolve and mature.\\n20\"),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 20, 'page_label': '21'}, page_content=\"THE BIG BOOK OF GENERATIVE AI\\nPRODUCT REVIEW SUMMARIZATION COULD USE A BOOST\\nTo illustrate the potential of a more focused approach to LLM adoption, we consider a fairly simple and common \\ntask performed within many online retail organizations: product review summarization. Today, most organizations \\nemploy a modestly-sized team of workers to read and digest user feedback for insights that may help improve a \\nproduct's performance or otherwise identify issues related to customer satisfaction.\\nThe work is important but anything but sexy. A worker reads a review, takes notes, and moves on to the next. \\nIndividual reviews that require a response are flagged and a summary of the feedback from across multiple \\nreviews are compiled for review by product or category managers.\\nThis is a type of work that's ripe for automation. The volume of reviews that pour into a site mean the more \\ndetailed portions of this work are often performed on a limited subset of products across variable windows \\ndepending on a products importance. In more sophisticated organizations, rules detecting course or \\ninappropriate language and models estimating user sentiment or otherwise classifying reviews for positive, \\nnegative or neutral experiences may be applied to help identify problematic content and draw a reviewer's \\nattention to it. But either way, a lot is missed simply because we can't throw enough bodies at the problem to \\nkeep up and those bodies tend to become bored or fatigued with the monotony of the work.\\nLARGE LANGUAGE MODELS CAN AUTOMATE PRODUCT REVIEW ANALYSIS\\nBy using an LLM, issues of scale and consistency can be easily addressed. All we need to do is bring the product \\nreviews to the model and ask:\\n ■ What are the top three points of negative feedback found across these reviews?\\n ■ What features do our customers like best about this product?\\n ■ Do customers feel they are receiving sufficient value from the product relative to what they are being \\nasked to pay?\\n ■ Are there any reviews that are especially negative or are using inappropriate language?\\n21\"),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 21, 'page_label': '22'}, page_content=\"THE BIG BOOK OF GENERATIVE AI\\nWithin seconds we can have a tidy response, allowing our product managers to focus on responding to issues \\ninstead of simply detecting them.\\nBut what about the problem of accuracy and bias? Standards for identifying inaccuracies and bias in LLM \\noutput are evolving as are techniques for better ensuring that outputs align with an organization's expectations, \\nand the fine-tuning of models using approved content can go a long way to ensure models have a preference to \\ngenerate content that's at least aligned with how an organization prefers to communicate.\\nThis is a long-winded way of saying there is no ideal solution to the problem as of yet. But when compared  \\nto where we are with human-driven processes and more simplistic models or rules-based approaches,  \\nthe results are expected to be better or at a minimum no worse than what we currently experience.  \\nAnd given that these review summaries are for internal consumption, the impact of an errant model can \\nbe easily managed.\\nYOU CAN BUILD A SOLUTION FOR THIS TODAY\\nTo demonstrate exactly how this work could be performed, we have built a Solution Accelerator for summarizing \\nproduct reviews. This is based heavily on a previously published blog from Sean Owen that addressed some of \\nthe core technical challenges of tuning an LLM on the Databricks platform. For the accelerator, we are using the \\nAmazon Product Reviews Dataset, which contains 51 million user-generated reviews across 2 million distinct \\nbooks as this provides access to a wide range of reviewer content and presents a scaling challenge many \\norganizations will recognize.\\nWe imagine a scenario in which a team of product managers receives customer feedback through online \\nreviews. These reviews are important for identifying issues that may need to be addressed regarding a \\nparticular item and for steering future books to be offered by the site. Without the use of technology, this team \\nstruggles to read all the feedback and summarize into a workable set notes. As a result, they limit their attention \\nto just the most critical items and are able to only process the feedback on a sporadic basis.\\n22\"),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 22, 'page_label': '23'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nBut using Databricks, they are able to set up a pipeline to collect feedback from a wider range of products \\nand summarize these on a regular basis. Recognizing that positively rated products are likely to highlight the \\nstrengths of these books while lower rated products are likely to focus on their weaknesses, they separate  \\nthese reviews based on user-provided ratings and task an LLM to extract different sets of information from  \\neach high-level category of reviews.\\nSummary metrics are provided to allow product managers an overview of the feedback received and are \\nbacked by more detailed summaries generated by the LLM (Figure 1).\\nFigure 1: Summary metrics and bullet-point details extracted from user reviews extracted using an LLM\\n23'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 23, 'page_label': '24'}, page_content=\"THE BIG BOOK OF GENERATIVE AI\\nDATABRICKS BRINGS TOGETHER ALL THE COMPONENTS OF A SOLUTION\\nThe scenario demonstrated above depends on the use of an LLM. In months prior, the use of such an LLM \\nrequired access to specialized computational infrastructures, but with advances in the open source community \\nand investments in the Databricks platform, we are now able to run the LLM in our local Databricks environment.\\nIn this particular scenario, the sensitivity of the data was not a motivating factor for this choice. Instead, we \\nfound that the volume of reviews to be processed tipped the cost scales toward the use of Databricks, allowing \\nus to trim about one-third of the cost of implementing a similar solution using a third-party service.\\nIn addition, we found that by implementing our own infrastructure, we were able to scale the environment up \\nfor faster processing, tackling as many as 760,000 reviews per hour in one test without having to be concerned \\nwith constraints imposed by an external service. While most organizations will not have the need to scale quite \\nto that level, it's nice to know it is there should it be.\\nBut this solution is more than just an LLM. To bring together the whole solution we needed to develop a data \\nprocessing workflow to receive incoming reviews, prepare them for submission to the model and to capture \\nmodel output for further analysis. As a unified data platform, Databricks provides us the means to address \\n both data engineering and data science requirements without data replication. And when we are done \\nprocessing the reviews, our analysts can use their tools of choice to query the output and make business \\ndecisions. Through Databricks, we have access to the full array of capabilities for us to build a solution aligned \\nwith our business’ needs.\\n24\"),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 24, 'page_label': '25'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nStage 2: Retrieval Augmented Generation\\nRetrieval augmented generation (RAG) lets you bring in supplemental knowledge resources to make an  \\noff-the-shelf AI system smarter. RAG won’t change the underlying behavior of the model, but it will improve  \\nthe quality and accuracy of the responses.\\nHowever, at this point, your business should not be uploading its “mission-critical” data. Instead, the RAG \\nprocess typically involves smaller amounts of nonsensitive information.\\nFor example, plugging in an employee handbook can allow your workers to start asking the underlying model \\nquestions about the organization’s vacation policy. Uploading instruction manuals can help power a service \\nchatbot. With the ability to query support tickets using AI, support agents can get answers quicker; however, \\ninputting confidential financial data so employees can inquire about the company’s performance is likely a step \\ntoo far.\\nTo get started, your team should first consolidate and cleanse the data you intend to use. With RAG, it’s vital \\nthat your company stores the data in sizes that will be appropriate for the downstream models. Often, that \\nrequires users to splice it into smaller segments.\\nThen, you should seek out a tool like Databricks Vector Search, which enables users to quickly set up their own \\nvector database. And because it’s governed by Unity Catalog, granular controls can be put in place to ensure \\nemployees are only accessing the datasets for which they have credentials.\\nFinally, you can then plug that endpoint into a LLM. A tool like Databricks MLflow helps to centralize the \\nmanagement of those APIs.\\n25'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 25, 'page_label': '26'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nAmong the benefits of RAG are reduced hallucinations, more up-to-date and accurate responses,  \\nand better domain-specific intelligence. RAG-assisted models are also a more cost-effective approach  \\nfor most organizations.\\nWhile RAG will help improve the results from commercial models, there are still many limitations to the use \\nof RAG. If your business is unable to get the results it wants, it’s time to move on to heavier-weight solutions, \\nbut moving beyond RAG-supported models often requires a much deeper commitment. The additional \\ncustomization costs more and requires a lot more data.\\nThat’s why it’s key that organizations first build a core understanding of how to use LLMs. By reaching the \\nperformance limitations of off-the-shelf models before moving on, you and your leadership can further hone  \\nin on where to allocate resources.\\nEnhance the Performance of Off-the-Shelf AI Models With RAG\\nLet’s explore a practical use case that demonstrates how real-time structured data can significantly improve \\nthe response quality of your RAG applications. This example will showcase how integrating dynamic information \\ncan transform the effectiveness and applicability of AI in your business operations.\\n26'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 26, 'page_label': '27'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nRAG Use Case\\nImprove Your RAG Application Response Quality With Real-Time Structured Data\\nby Mani Parkhe, Aakrati Talati, Sue Ann Hong, Craig Wiley, Chenen Liang and Mingyang Ge\\nRetrieval augmented generation (RAG) is an efficient mechanism to provide relevant data as context in \\nGenAI applications. Most RAG applications typically use vector indexes to search for relevant context from \\nunstructured data such as documentation, wikis, and support tickets. Yesterday, we announced Databricks \\nVector Search Public Preview that helps with exactly that. However, GenAI response quality can be enhanced by \\naugmenting these text-based contexts with relevant and personalized structured data. Imagine a GenAI tool on \\na retail website where customers inquire, \"Where’s my recent order?\" This AI must understand that the query \\nis about a specific purchase, then gather up-to-date shipment information for line items, before using LLMs to \\ngenerate a response. Developing these scalable applications demands substantial work, integrating technologies \\nfor handling both structured and unstructured data with GenAI capabilities.\\nWe are excited to announce the public preview of Databricks Feature & Function Serving, a low latency real-\\ntime service designed to serve structured data from the Databricks Data Intelligence Platform. You can instantly \\naccess pre-computed ML features as well as perform real-time data transformations by serving any Python \\nfunction from Unity Catalog. The retrieved data can then be used in real-time rule engines, classical ML, and \\nGenAI applications.\\nUsing Feature and Function Serving (AWS)(Azure) for structured data in coordination with Databricks Vector \\nSearch (AWS)(Azure) for unstructured data significantly simplifies productionalization of GenAI applications. \\nUsers can build and deploy these applications directly in Databricks and rely on existing data pipelines, \\ngovernance, and other enterprise features. Databricks customers across various industries are using these \\ntechnologies along with open source frameworks to build powerful GenAI applications such as the ones \\ndescribed in the table below.\\n27'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 27, 'page_label': '28'}, page_content='INDUSTRY USE CASE\\nRetail  ■ Product Recommendations / Search Ranking using user preferences, search history, location . . . etc.\\n ■ Image and metadata based product search\\n ■ Inventory management and forecasting using sales data, seasonal trends and market/competitive analysis\\nEducation  ■ Personalized learning plans based on past mistakes, historical trends and  cohorts\\n ■ Automated grading, feedback, follow-ups and progress reporting\\n ■ Content filtering for issued devices\\nFinancial Services  ■ Natural language apps for analysts and investors to correlate earning calls and reports with market intelligence and historical trends\\n ■ Fraud and risk analysis\\n ■ Personalized wealth management, retirement planning, what-if analysis and next-best actions \\nTravel and Hospitality  ■ Chatbots for personalized customer interactions and tailored travel recommendations\\n ■ Dynamic route planning using weather, live traffic patterns, and historical data\\n ■ Dynamic price optimization using competitive analysis and demand-based pricing\\nHealthcare and Life Sciences  ■ Patient/member engagement and health summaries\\n ■ Support apps for personalized care, clinical decisions and care coordination\\n ■ R&D report summarization, clinical trial analysis, drug repurposing\\nInsurance  ■ Risk assessment for mortgage underwriting using text and structured data about properties and neighborhoods\\n ■ User chatbots for questions about policies, risk and what-if analysis\\n ■ Claim processing automation\\nTechnology and Manufacturing  ■ Prescriptive maintenance and diagnostics for equipment using guided instruction\\n ■ Anomaly detection on live data stream against historical statistics\\n ■ Automated analysis for daily production / shift analysis and future planning\\nMedia and Entertainment  ■ In-app content discovery and recommendations, personalized email and digital marketing\\n ■ Content localization\\n ■ Personalized gaming experiences and game review\\n28\\nTHE BIG BOOK OF GENERATIVE AI'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 28, 'page_label': '29'}, page_content=\"THE BIG BOOK OF GENERATIVE AI\\nSERVING STRUCTURED DATA TO RAG APPLICATIONS\\nTo demonstrate how structured data can help enhance the quality of a GenAI application, we use the following \\nexample for a travel planning chatbot. The example shows how user preferences (example: “ocean view” or \\n“family friendly”) can be paired with unstructured information sourced about hotels to search for hotel matches. \\nTypically hotel prices dynamically change based on demand and seasonality. A price calculator built into the \\nGenAI application ensures that the recommendations are within the user's budget. The GenAI application that \\npowers the bot uses Databricks Vector Search and Databricks Feature and Function Serving as building blocks \\nto serve the necessary personalized user preferences and budget and hotel information using LangChain’s \\nagents API.\\nYou can find the complete notebook for this RAG Chain application as depicted above. This application can be \\nrun locally within the notebook or deployed as an endpoint accessible by a chatbot user interface.\\n29\"),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 29, 'page_label': '30'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nACCESS YOUR DATA AND FUNCTIONS AS REAL-TIME ENDPOINTS\\nWith Feature Engineering in Unity Catalog you can already use any table with a primary key to serve features \\nfor training and serving. Databricks Model Serving supports using Python functions to compute features on-\\ndemand. Built using the same technology available under the hood for Databricks Model Serving, feature and \\nfunction endpoints can be used to access any pre-computed feature or compute them on-demand. With a \\nsimple syntax you can define a feature spec function in Unity Catalog that can encode the directed acyclic \\ngraph to compute and serve features as a REST endpoint.\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\n27\\n28\\nfrom databricks.feature_engineering import (\\n  FeatureFunction,\\n  FeatureLookup,\\n  FeatureEngineeringClient,\\n)\\nfeatures = [\\n  # Lookup columns `latitude` and `longitude` from `restaurants` table in UC using the input `restaurant_id` as key\\n  FeatureLookup(\\n    table_name=\"main.default.restaurants\",\\n    lookup_key=\"restaurant_id\",\\n    features=[\"latitude”, “longitude\"]\\n  ),\\n  # Calculate a new feature called `distance` using the restaurant and user\\'s current location\\n  FeatureFunction(\\n    udf_name=\"main.default.distance\",\\n    output_name=\"distance\",\\n    # bind the function parameter with input from other features or from request.\\n    input_bindings={\"user_latitude\": \"user_latitude\", \"user_longitude\": \"user_longitude\",\\n                    \"restaurant_latitude\": \"latitude\", \"restaurant_longitude\": \"longitude\"},\\n  ),\\n]\\nfe = FeatureEngineeringClient()\\n# Create a feature spec with the features listed above.\\n# The FeatureSpec can be accessed in UC as a Function.\\nfe.create_feature_spec(\\n  name=\"main.default.restaurant_features\",\\n  features=features,\\n)\\n30'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 30, 'page_label': '31'}, page_content='THE BIG BOOK OF GENERATIVE AI\\n \\n1\\n2\\n3\\n4\\n \\n5\\n6\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\nfrom databricks.feature_engineering.entities.feature_serving_endpoint import (\\n  ServedEntity,\\n  EndpointCoreConfig,\\n)\\nfe.create_feature_serving_endpoint(\\n  name=\"restaurant-features\",\\n    config=EndpointCoreConfig(\\n    served_entities=ServedEntity(\\n      feature_spec_name=\"main.default.restaurant_features\",\\n      workload_size=\"Small\",\\n      scale_to_zero_enabled=True\\n    )\\n  )\\n)\\nThis feature spec function can be served in real-time as a REST endpoint. All endpoints are accessible in \\nthe Serving left navigation tab including features, function, custom trained models, and foundation models. \\nProvision the endpoint using this API.\\nThe endpoint can also be created using a UI workflow as shown in the following graphic\\n31'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 31, 'page_label': '32'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nNow features can be accessed in real time by querying the endpoint:\\nTo serve structured data to real-time AI applications, precomputed data needs to be deployed to operational \\ndatabases. Users can already use external online stores as a source of precomputed features — for example \\nDynamoDB and Cosmos DB are commonly used to serve features in Databricks Model Serving. Databricks \\nOnline Tables (AWS)(Azure) adds new functionality that simplifies synchronization of precomputed features to a \\ndata format optimized for low latency data lookups. You can sync any table with a primary key as an online table \\nand the system will set up an automatic pipeline to ensure data freshness.\\n \\n1\\n2\\n3\\n4\\n5\\n \\n6\\ncurl \\\\\\n  -u token:$DATABRICKS_TOKEN \\\\\\n  -X POST \\\\\\n  -H \"Content-Type: application/json\" \\\\\\n  -d \\'{\"dataframe_records\": [{\"user_latitude\": 37.9711, \"user_longitude\": -122.3940, \"restaurant_id\": 5}]}\\' \\\\ \\n  https://<databricks-instance>/serving-endpoints/restaurant-features/invocations\\n32'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 32, 'page_label': '33'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nStage 3: Fine-Tuning a Foundation Model\\nMoving beyond RAG to model fine-tuning lets you start building models that are much more deeply \\npersonalized to the business. If you have already been experimenting with commercial models across your \\noperations, you are likely ready to advance to this stage. There’s a clear understanding at the executive level of \\nthe value of generative AI, as well as an understanding of the limitations of publicly available LLMs. Specific use \\ncases have been established. And now, you and your enterprise are ready to go deeper.\\nWith fine-tuning, you can take a general-purpose model and train it on your own specific data. For example, \\ndata management provider Stardog relies on the Mosaic AI tools from Databricks to fine-tune the off-the-shelf \\nLLMs they use as a foundation for their Knowledge Graph Platform. This enables Stardog’s customers to query \\ntheir own data across the different silos simply by using natural language.\\nIt’s imperative that organizations at this stage have an underlying architecture in place that will help ensure the \\ndata supporting the models is secure and accurate. Fine-tuning an AI system requires an immense amount of \\nproprietary information and, as your business advances on the AI maturity curve, the number of models running \\nwill only grow, increasing the demand for data access.\\nThat’s why you need to have the right mechanisms in place to track data from the moment it’s generated to \\nwhen it’s eventually used, and why Unity Catalog is such a popular feature among Databricks customers. With \\nUnity Catalog’s data lineage capabilities, businesses always know where data is moving and who is accessing it.\\nAny Unity Catalog table with primary keys can be used to serve features in GenAI applications using Databricks \\nOnline Tables.\\n33'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 33, 'page_label': '34'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nFine-Tuning Use Cases\\nCreating a Bespoke LLM for AI-Generated Documentation \\nIt’s easier than you think: 2 engineers, 1 month and less than $1,000\\nby Matthew Hayes, Hongyi Zhang, Tao Feng, Jan van der Vegt, Zaheera Valani and Reynold Xin\\nIn this example, we share our experience from prototyping a hackathon project using off-the-shelf SaaS-based \\nLLMs to creating a bespoke LLM that is better, faster, and cheaper. The new model took 2 engineers, 1 month \\nand less than $1,000 in compute cost to develop. We hope you will find the learnings useful, as we believe \\nthey apply to a wide class of GenAI use cases. More importantly, it has allowed us to take advantage of rapid \\nadvances being made in open-source LLMs.\\nWHAT IS AI-GENERATED DOCUMENTATION?\\nAt the center of each data platform lies a (potentially enormous) collection of datasets (often in the form of \\ntables). In virtually every organization we have worked with, the vast majority of tables are not documented. The \\nabsence of documentation provides a number of challenges, including making it difficult for humans to discover \\nthe data needed for answering a business question, or more recently, for AI agents to automatically find \\ndatasets to use in response to questions (a key capability in our platform that we’re calling Data Intelligence).\\n34'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 34, 'page_label': '35'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nRather than relying on humans to document these datasets, we prototyped as part of our quarterly hackathon \\na new workflow using an off-the-shelf SaaS-based LLM to automatically generate documentation for tables \\nand their columns based on their schema. This new workflow would automatically suggest descriptions for the \\ntables and columns and allow users to either individually accept, bulk accept, or modify the suggestions for \\nhigher fidelity, as shown below. When we showed this prototype to some users, their immediate question was \\nuniversally, “When can I have it?!”\\n35'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 35, 'page_label': '36'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nCHALLENGES WITH LLMS\\nAs we moved toward launching this feature to all our customers, we ran into three challenges with the model:\\n1. Quality: The ultimate success of this feature depends on the quality of the generated documentation. \\nAlthough we could measure the quality (in terms of how often they are accepted), we had limited knobs \\nat our disposal to improve it, aside from basic prompting. During the private preview period, we also \\nsometimes noticed the quality of the suggestions degrading, without any change to our codebase. Our \\nspeculation is that the SaaS LLM controller rolled out updates to the model that sometimes affected \\nperformance on specific tasks.\\n2. Performance (throughput): We had limited API quota provisioned with the SaaS LLM provider. We \\nwork with tens of thousands of organizations, and it is not uncommon that a single organization would \\nhave millions of tables. It would take too long to generate documentation for all the tables based on the \\nthroughput quota.\\n3. Cost: Related to the above, it was not cost-effective unless we started charging customers for using this \\nspecific feature.\\nWe have heard similar concerns from a variety of customers as they try to move their LLM-based applications \\nfrom a proof-of-concept to production and saw this as an excellent opportunity for us to explore alternatives \\nfor an organization like ours.\\nWe experimented with different versions of the SaaS LLMs, but they all had the same challenges. This is not \\nsurprising in hindsight. The SaaS LLMs are an engineering marvel, but they are very general models that need to \\naddress all the use cases from table generation to conversing about the meaning of life. The generality means \\nit needs to have an extremely large number of parameters, which limits how fast and how cheap it can return \\nanswers. As it continues to evolve to optimize for different use cases, it might also regress the narrower use case \\nwe have.\\n36'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 36, 'page_label': '37'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nBUILDING A BESPOKE MODEL\\nTo address the aforementioned challenges, we started building a bespoke model. It took a team of two \\nengineers one month to build a customized, smaller LLM that was better, faster, and cheaper:\\n ■ Quality: Based on our evaluation (see the following section), the model is significantly better than the \\ncheaper version of the SaaS model, and roughly equivalent to the more expensive version.\\n ■ Performance (throughput): Because the bespoke model is a lot smaller, it can fit in A10 GPUs, and we \\ncan increase the inference throughput with horizontal scaling. The smaller GPUs are also more available, \\nwhich enables us to generate the descriptions for all tables faster.\\n ■ Cost: Each fine-tuning run of the model only costs a few dollars, and in aggregate, it cost less than $1000 \\nto develop because we did a lot of experiments. It also resulted in a 10 fold reduction in inference cost.\\nThe first step was to treat this as an applied machine learning problem. “Applied machine learning” sounds \\ndaunting and complicated, but all it meant was that we needed to:\\n ■ Find training datasets so we can bootstrap an initial model\\n ■ Identify an evaluation mechanism so we can measure the quality, before rolling it out to production\\n ■ Train and select models\\n ■ Collect real-world usage metrics, so we can monitor how well a monitor does in production\\n ■ Iterate and roll out new models to continuously improve the three dimensions: quality, performance, cost\\n37'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 37, 'page_label': '38'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nTRAINING DATA\\nWe created the initial training dataset for this fine-tuning task, using two different sources of data:\\n1. North American Industry Classification System (NAICS) codes. This is a public dataset used by Federal \\nstatistical agencies in classifying business establishments for the purpose of collecting, analyzing, and \\npublishing statistical data related to the U.S. business economy.\\n2. Databricks’ internal use case taxonomy curation datasets. This is a series of internal datasets created by \\nour solution architects to show customers best practice architectures.\\nThen we synthesized CREATE TABLE statements using the above use cases to yield a diverse set of tables and \\ngenerated sample responses including table descriptions and column comments using another LLM. In total,  \\nwe generated ~3600 training examples. \\nNotably, we didn’t use any customer data for training this powerful feature that all of our customers can  \\nbenefit from. \\nBOOTSTRAPPING MODEL EVALUATION\\nAfter the feature launch, we could measure a model’s quality through production metrics such as the rate of \\nusers accepting the suggestions. But before we made it to the launch, we needed a way to evaluate the model’s \\nquality against that of the SaaS LLM.\\nTo do that in an unbiased fashion, we set up a simple double-blind evaluation framework in which we asked \\n4 employees to rate table descriptions generated from the two models we wanted to compare using a set of \\n62 unseen tables. Our framework then generated a sheet where each row showed the input and showed both \\noutputs in a randomized order. The evaluator would vote on the better sample (or give a tie). The framework then \\nprocessed the votes from different evaluators to generate a report; it also summarizes the degree to which each \\nof the evaluators agreed.\\nBased on our experiences so far, having an evaluation dataset of tens to hundreds of data points is a sufficient \\ninitial milestone and can be generalized to other use cases as well.\\n38'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 38, 'page_label': '39'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nMODEL SELECTION AND FINE-TUNING\\nWe considered the following criteria for model selection:\\n ■ Whether the license supports commercial use\\n ■ Performance (quality) of the model for text generation\\n ■ Speed of the model\\nBased on these criteria, MPT-7B and Llama2-7B were the leading candidates, as shown in our LLM guide. We \\nconsidered larger models such as MPT-30B and Llama-2-13B. In the end we chose MPT-7B, as it has the best \\ncombination of quality and inference performance:\\n ■ There was no discernable difference in the quality between the MPT-7B and Llama-2-7B fine-tuned \\nmodels for this task.\\n ■ The smaller 7B models, after fine-tuning, were already meeting the quality bar. It was significantly better \\nthan the cheaper version of the SaaS model, and roughly equivalent to the more expensive version.\\n ■ We did not yet observe a measurable benefit of using larger models for this task that would justify the \\nincreased serving costs.\\n ■ The latency for the smaller models was significantly better than the larger models while offering \\ncomparable quality so we could deliver a much snappier product experience.\\n ■ The smaller model could fit comfortably and be served using A10 GPUs, which were more readily \\navailable. Their abundance would mean higher inference throughput for the task.\\nThe total time it took to fine-tune the model on the ~3600 examples was only around 15 minutes!\\nWhile we chose MPT-7B for our model, we believe the LLM landscape is changing rapidly and the best model \\ntoday won’t be the best model tomorrow. That’s why we consider this to be an iterative and continuous process \\nand are focused on using tools that make our evaluation efficient and fast.\\n39'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 39, 'page_label': '40'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nKEY ARCHITECTURAL COMPONENTS OF OUR PRODUCTION PIPELINE\\nWe were able to build this quickly by relying on the following key components of the Databricks Data \\nIntelligence Platform:\\n ■ Databricks LLM fine-tuning: It provides a very simple infrastructure for fine-tuning the models for our \\ntask. We prepared the training data in JSON format, and with a one-line CLI command, we were able to \\nfine-tune the LLMs.\\n ■ Unity Catalog: The models that we use in production are registered in Unity Catalog (UC), providing the \\ngovernance we need to not just for the data, but also the models. With its end-to-end lineage feature, UC \\nalso gives us traceability from the models back to the datasets they are trained on.\\n ■ Delta Sharing: We used Delta Sharing to distribute the model to all production regions we have around \\nthe world for faster serving.\\n ■ Databricks optimized LLM serving: Once the models are registered in UC, they can be served using the \\nnew optimized LLM serving, which provides significant performance improvement in terms of throughput \\nand latency improvement compared to traditional serving for LLM serving.\\n40'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 40, 'page_label': '41'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nCOST\\nThe fine-tuning compute cost for the whole project was less than $1000 (each fine-tuning run cost only a few \\ndollars). And the final result is a more than 10-fold reduction in cost. Why is the cost-saving so significant? It is \\nnot surprising if we consider the following:\\n ■ As mentioned earlier, the SaaS LLMs need to address all the use cases, including acting as a general \\nchatbot. The generality requires an extremely large number of parameters, which incurs significant \\ncompute costs in inference.\\n ■ When we fine-tune for a more specific task, we can use a much smaller prompt. Larger, general-purpose \\nmodels require longer prompts that include detailed instructions on what the input is and what form the \\noutput should take. Fine-tuned models can bake instructions and expected structure into the model \\nitself. We found we were able to reduce the number of input tokens with no impact on performance by \\nmore than half.\\n ■ Inference costs scale with the number of input and output tokens, and costs scale linearly for SaaS \\nservices that are charged per token. With Databricks’ LLM Serving offering, we offer provisioned \\nthroughput charged per hour, which provides consistent latencies, uptime SLAs, and autoscaling. Because \\nsmaller LLMs can fit in smaller GPUs that are much cheaper and more available and because we offer a \\nhighly optimized runtime, we can aggressively drive down costs. Also, smaller LLMs scale up and down \\nfaster, meaning we can quickly scale up to meet peaks of demand and aggressively scale down when \\nusage is lighter, creating substantial cost efficiency in production.\\n41'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 41, 'page_label': '42'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nCONCLUSION\\nHaving well-documented data is critical to all data users, and growing more important day-by-day to power \\nAI-based data platforms (what we’re calling Data Intelligence). We started with SaaS LLMs for prototyping this \\nnew GenAI feature but ran into challenges with quality, performance, and cost. We built a bespoke model to do \\nthe same task at better quality, and yet resulting in higher throughput with scale-out and 10x cost reduction. To \\nrecap what it took:\\n ■ 2 engineers\\n ■ 1 month\\n ■ Less than $1,000 in compute for training and experimentation\\n ■ MPT-7B fine-tuned on 3600 synthetically generated examples, in under 15 minutes\\n ■ 4 human evaluators, with 62 initial evaluation examples\\nThis experience demonstrates how easy it is to develop and deploy bespoke LLMs for specific tasks. This model \\nis now live on Databricks in Amazon Web Services and Google Cloud and is being used to power most data \\nannotations on the platform.\\n42'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 42, 'page_label': '43'}, page_content=\"THE BIG BOOK OF GENERATIVE AI\\nEfficient Fine-Tuning With LoRA: A Guide to Optimal Parameter Selection for Large Language Models\\nby Avinash Sooriyarachchi\\nWith the rapid advancement of neural network-based techniques and large language model (LLM) research, \\nbusinesses are increasingly interested in AI applications for value generation. They employ various machine \\nlearning approaches, both generative and non-generative, to address text-related challenges such as \\nclassification, summarization, sequence-to-sequence tasks, and controlled text generation. Organizations can \\nopt for third-party APIs, but fine-tuning models with proprietary data offers domain-specific and pertinent \\nresults, enabling cost-effective and independent solutions deployable across different environments in  \\na secure manner.\\nEnsuring efficient resource utilization and cost-effectiveness is crucial when choosing a strategy for fine-\\ntuning. This blog explores arguably the most popular and effective variant of such parameter efficient \\nmethods, Low Rank Adaptation (LoRA), with a particular emphasis on QLoRA (an even more efficient variant of \\nLoRA). The approach here will be to take an open large language model and fine-tune it to generate fictitious \\nproduct descriptions when prompted with a product name and a category. The model chosen for this \\nexercise is OpenLLaMA-3b-v2, an open large language model with a permissive license (Apache 2.0), and the \\ndataset chosen is Red Dot Design Award Product Descriptions, both of which can be downloaded from the \\nHuggingFace Hub at the links provided.\\nFINE-TUNING, LORA AND QLORA\\nIn the realm of language models, fine-tuning an existing language model to perform a specific task on specific \\ndata is a common practice. This involves adding a task-specific head, if necessary, and updating the weights of \\nthe neural network through backpropagation during the training process. It is important to note the distinction \\nbetween this fine-tuning process and training from scratch. In the latter scenario, the model's weights are \\nrandomly initialized, while in fine-tuning, the weights are already optimized to a certain extent during the \\npretraining phase. The decision of which weights to optimize or update, and which ones to keep frozen, depends \\non the chosen technique.\\nFull fine-tuning involves optimizing or training all layers of the neural network. While this approach typically \\nyields the best results, it is also the most resource-intensive and time-consuming.\\n43\"),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 43, 'page_label': '44'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nFortunately, there exist parameter-efficient approaches for fine-tuning that have proven to be effective. \\nAlthough most such approaches have yielded less performance, Low Rank Adaptation (LoRA) has bucked \\nthis trend by even outperforming full fine-tuning in some cases, as a consequence of avoiding catastrophic \\nforgetting (a phenomenon which occurs when the knowledge of the pretrained model is lost during the  \\nfine-tuning process).\\nLoRA is an improved fine-tuning method where instead of fine-tuning all the weights that constitute the  \\nweight matrix of the pretrained large language model, two smaller matrices that approximate this larger matrix \\nare fine-tuned. These matrices constitute the LoRA adapter. This fine-tuned adapter is then loaded to the \\npretrained model and used for inference.\\nQLoRA is an even more memory efficient version of LoRA where the pretrained model is loaded to GPU  \\nmemory as quantized 4-bit weights (compared to 8-bits in the case of LoRA), while preserving similar \\neffectiveness to LoRA. Probing this method, comparing the two methods when necessary, and figuring out the \\nbest combination of QLoRA hyperparameters to achieve optimal performance with the quickest training time \\nwill be the focus here.\\nLoRA is implemented in the Hugging Face Parameter Efficient Fine-Tuning (PEFT) library, offering ease  \\nof use and QLoRA can be leveraged by using bitsandbytes and PEFT together. HuggingFace Transformer \\nReinforcement Learning (TRL) library offers a convenient trainer for supervised fine-tuning with seamless \\nintegration for LoRA. These three libraries will provide the necessary tools to fine-tune the chosen pretrained \\nmodel to generate coherent and convincing product descriptions once prompted with an instruction  \\nindicating the desired attributes.\\n44'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 44, 'page_label': '45'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nPREPPING THE DATA FOR SUPERVISED FINE-TUNING\\nTo probe the effectiveness of QLoRA for fine-tuning a model for instruction following, it is essential to transform \\nthe data to a format suited for supervised fine-tuning. Supervised fine-tuning in essence, further trains a \\npretrained model to generate text conditioned on a provided prompt. It is supervised in that the model is fine-\\ntuned on a dataset that has prompt-response pairs formatted in a consistent manner.\\nAn example observation from our chosen dataset from the Hugging Face hub looks as follows:\\nAs useful as this dataset is, this is not well formatted for fine-tuning of a language model for instruction following \\nin the manner described.\\nThe following code snippet loads the dataset from the Hugging Face hub into memory, transforms the \\nnecessary fields into a consistently formatted string representing the prompt, and inserts the response (i.e., \\nthe description), immediately afterward. This format is known as the ‘Alpaca format’ in large language model \\nresearch circles as it was the format used to fine-tune the original LlaMA model from Meta to result in the \\nAlpaca model, one of the first widely distributed instruction-following large language models (although not \\nlicensed for commercial use).\\nPRODUCT CATEGORY DESCRIPTION TEXT\\n“Biamp Rack Products” “Digital Audio Processors\" “High recognition value, uniform \\naesthetics and practical \\nscalability — this has been \\nimpressively achieved with the \\nBiamp brand language . . . “\\n“Product Name: Biamp Rack Products; \\nProduct Category: Digital Audio \\nProcessors; Product Description: High \\nrecognition value, uniform aesthetics \\nand practical scalability — this has been \\nimpressively achieved with the Biamp \\nbrand language . . . “\\n45'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 45, 'page_label': '46'}, page_content='THE BIG BOOK OF GENERATIVE AI\\n \\n1\\n2\\n3\\n4 \\n5\\n6\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\nimport pandas as pd\\nfrom datasets import load_dataset\\nfrom datasets import Dataset\\n#Load the dataset from the HuggingFace Hub\\nrd_ds = load_dataset(\"xiyuez/red-dot-design-award-product-description\")\\n#Convert to pandas dataframe for convenient processing\\nrd_df = pd.DataFrame(rd_ds[\\'train\\'])\\n#Combine the two attributes into an instruction string\\nrd_df[\\'instruction\\'] = \\'Create a detailed description for the following product: \\'+ rd_df[\\'product\\']+\\', belonging to \\ncategory: \\'+ rd_df[\\'category\\']\\nrd_df = rd_df[[\\'instruction\\', \\'description\\']]\\n#Get a 5000 sample subset for fine-tuning purposes\\nrd_df_sample = rd_df.sample(n=5000, random_state=42)\\n#Define template and format data into the template for supervised fine-tuning\\ntemplate = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the \\nrequest.\\n### Instruction:\\n{}\\n### Response:\\\\n\"\"\"\\nrd_df_sample[\\'prompt\\'] = rd_df_sample[\"instruction\"].apply(lambda x: template.format(x))\\nrd_df_sample.rename(columns={\\'description\\': \\'response\\'}, inplace=True)\\nrd_df_sample[\\'response\\'] = rd_df_sample[\\'response\\'] + \"\\\\n### End\"\\nrd_df_sample = rd_df_sample[[\\'prompt\\', \\'response\\']]\\nrd_df[\\'text\\'] = rd_df[\"prompt\"] + rd_df[\"response\"]\\nrd_df.drop(columns=[\\'prompt\\', \\'response\\'], inplace=True)\\nThe resulting prompts are then loaded into a hugging face dataset for supervised fine-tuning. Each such prompt \\nhas the following format.\\n46'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 46, 'page_label': '47'}, page_content='THE BIG BOOK OF GENERATIVE AI\\n \\n1\\n2\\n3\\n4 \\n5\\n6\\n8\\n9\\n10\\n11\\n12\\n13\\n```\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n### Instruction:\\nCreate a detailed description for the following product: Beseye Pro, belonging to category: Cloud-Based Home Security \\nCamera\\n### Response:\\nBeseye Pro combines intelligent home monitoring with decorative art. The camera, whose form is reminiscent of a water \\ndrop, is secured in the mounting with a neodymium magnet and can be rotated by 360 degrees. This allows it to be \\neasily positioned in the desired direction. The camera also houses modern technologies, such as infrared LEDs, cloud-\\nbased intelligent video analyses and SSL encryption.\\n### End\\n```\\nTo facilitate quick experimentation, each fine-tuning exercise will be done on a 5000 observation subset  \\nof this data.\\nTESTING MODEL PERFORMANCE BEFORE FINE-TUNING\\nBefore any fine-tuning, it’s a good idea to check how the model performs without any fine-tuning to get a \\nbaseline for pretrained model performance.\\nThe model can be loaded in 8-bit as follows and prompted with the format specified in the model card on \\nHugging Face.\\n47'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 47, 'page_label': '48'}, page_content='THE BIG BOOK OF GENERATIVE AI\\n \\n1\\n2\\n3\\n4 \\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\nimport torch\\nfrom transformers import LlamaTokenizer, LlamaForCausalLM\\nmodel_path = \\'openlm-research/open_llama_3b_v2\\'\\ntokenizer = LlamaTokenizer.from_pretrained(model_path)\\nmodel = LlamaForCausalLM.from_pretrained(\\nmodel_path, load_in_8bit=True, device_map=\\'auto\\',\\n)\\n#Pass in a prompt and infer with the model\\nprompt = \\'Q: Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: \\nOptical Mouse\\\\nA:\\'\\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\\ngeneration_output = model.generate(\\ninput_ids=input_ids, max_new_tokens=128\\n)\\nprint(tokenizer.decode(generation_output[0]))\\n \\n1\\n2\\n3\\n4 \\n5\\n6\\nQ: Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical \\nMouse A: The Corelogic Smooth Mouse is a wireless optical mouse that has a 1000 dpi resolution. It has a 2.4 GHz \\nwireless connection and a 12-month warranty. Q: What is the price of the Corelogic Smooth Mouse? A: The Corelogic \\nSmooth Mouse is priced at $29.99. Q: What is the weight of the Corelogic Smooth Mouse? A: The Corelogic Smooth Mouse \\nweighs 0.1 pounds. Q: What is the dimensions of the Corelogic Smooth Mouse? A: The Corelogic Smooth Mouse has a \\ndimension\\nThe output obtained is not quite what we want.\\nThe first part of the result is actually satisfactory, but the rest of it is more of a rambling mess.\\n48'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 48, 'page_label': '49'}, page_content='THE BIG BOOK OF GENERATIVE AI\\n \\n1\\n2\\n3\\n4 \\n5\\n6\\n7\\n8\\n9\\nprompt= \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n### Instruction:\\nCreate a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical Mouse\\n### Response:\"\"\"\\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\\ngeneration_output = model.generate(\\ninput_ids=input_ids, max_new_tokens=128\\n)\\nprint(tokenizer.decode(generation_output[0]))\\n \\n1\\n2\\n3\\n4 \\n5\\n6\\nCorelogic Smooth Mouse is a mouse that is designed to be used by people with disabilities. It is a wireless mouse \\nthat is designed to be used by people with disabilities. It is a wireless mouse that is designed to be used by people \\nwith disabilities. It is a wireless mouse that is designed to be used by people with disabilities. It is a wireless \\nmouse that is designed to be used by people with disabilities. It is a wireless mouse that is designed to be used by \\npeople with disabilities. It is a wireless mouse that is designed to be used by people with disabilities. It is a \\nwireless mouse that is designed to be used by\\nSimilarly, if the model is prompted with the input text in the ‘Alpaca format’ as discussed before, the output is \\nexpected to be just as suboptimal:\\nAnd sure enough, it is:\\nThe model performs what it was trained to do, predicts the next most probable token. The point of supervised \\nfine-tuning in this context is to generate the desired text in a controllable manner. Please note that in the \\nsubsequent experiments, while QLoRA leverages a model loaded in 4-bit with the weights frozen, the  \\ninference process to examine output quality is done once the model has been loaded in 8-bit as shown  \\nabove for consistency.\\n49'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 49, 'page_label': '50'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nTHE TURNABLE KNOBS\\nWhen using PEFT to train a model with LoRA or QLoRA (note that, as mentioned before, the primary difference \\nbetween the two is that in the latter, the pretrained models are frozen in 4-bit during the fine-tuning process), \\nthe hyperparameters of the low rank adaptation process can be defined in a LoRA config as shown below:\\nTwo of these hyperparameters, r and target_modules are empirically shown to affect adaptation quality \\nsignificantly and will be the focus of the tests that follow. The other hyperparameters are kept constant at the \\nvalues indicated above for simplicity.\\nr represents the rank of the low rank matrices learned during the fine-tuning process. As this value is increased, \\nthe number of parameters needed to be updated during the low-rank adaptation increases. Intuitively, a lower \\nr may lead to a quicker, less computationally intensive training process, but may affect the quality of the model \\nthus produced. However, increasing r beyond a certain value may not yield any discernible increase in quality of \\nmodel output. How the value of r affects adaptation (fine-tuning) quality will be put to the test shortly.\\n \\n1\\n2\\n3\\n4 \\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\nfrom peft import LoraConfig\\n...\\n...\\n#If only targeting attention blocks of the model\\ntarget_modules = [\"q_proj\", \"v_proj\"]\\n#If targeting all linear layers\\ntarget_modules = [\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'o_proj\\',\\'gate_proj\\',\\'down_proj\\',\\'up_proj\\',\\'lm_head\\']\\nlora_config = LoraConfig(\\nr=16,\\ntarget_modules = target_modules,\\nlora_alpha=8,\\nlora_dropout=0.05,\\nbias=\"none\",\\ntask_type=\"CAUSAL_LM\",}\\n50'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 50, 'page_label': '51'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nWhen fine-tuning with LoRA, it is possible to target specific modules in the model architecture. The adaptation \\nprocess will target these modules and apply the update matrices to them. Similar to the situation with \"r,\" \\ntargeting more modules during LoRA adaptation results in increased training time and greater demand for \\ncompute resources. Thus, it is a common practice to only target the attention blocks of the transformer. \\nHowever, recent work as shown in the QLoRA paper by Dettmers et al. suggests that targeting all linear layers \\nresults in better adaptation quality. This will be explored here as well.\\nNames of the linear layers of the model can be conveniently appended to a list with the following code snippet:\\nTUNING THE FINE-TUNING WITH LORA\\nThe developer experience of fine-tuning large language models in general have improved dramatically over the \\npast year or so. The latest high level abstraction from Hugging Face is the SFTTrainer class in the TRL library. To \\nperform QLoRA, all that is needed is the following:\\n1. Load the model to GPU memory in 4-bit (bitsandbytes enables this process)\\n2. Define the LoRA configuration as discussed previously\\n3. Define the train and test splits of the prepped instruction following data into Hugging Face  \\nDataset objects\\n4. Define training arguments: These include the number of epochs, batch size and other training \\nhyperparameters which will be kept constant during this exercise\\n5. Pass these arguments into an instance of SFTTrainer\\nThese steps are clearly indicated in the source file in the repository associated with this blog.\\n \\n1\\n2\\n3\\n4 \\n5\\n6\\n7\\n8\\n9\\nimport re\\nmodel_modules = str(model.modules)\\npattern = r\\'\\\\((\\\\w+)\\\\): Linear\\'\\nlinear_layer_names = re.findall(pattern, model_modules)\\nnames = []\\n# Print the names of the Linear layers\\nfor name in linear_layer_names:\\n    names.append(name)\\ntarget_modules = list(set(names))\\n51'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 51, 'page_label': '52'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nThe actual training logic is abstracted away nicely as follows:\\nIf MLflow autologging is enabled in the Databricks workspace, which is highly recommended, all the training \\nparameters and metrics are automatically tracked and logged with the MLflow tracking server. This functionality \\nis invaluable in monitoring long-running training tasks. Needless to say, the fine-tuning process is performed \\nusing a compute cluster (in this case, a single node with a single A100 GPU) created using the latest Databricks \\nMachine Runtime with GPU support.\\n \\n1\\n2\\n3\\n4 \\n5\\n6\\n7\\n8\\n9\\n10\\n11\\ntrainer = SFTTrainer(\\nmodel,\\ntrain_dataset=dataset[\\'train\\'],\\neval_dataset = dataset[\\'test\\'],\\ndataset_text_field=\"text\",\\nmax_seq_length=256,\\nargs=training_args,\\n)\\n# Initiate the training process\\nwith mlflow.start_run(run_name= ‘run_name_of_choice’):\\ntrainer.train()\\n52'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 52, 'page_label': '53'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nHYPERPARAMETER COMBINATION #1: QLoRA with r=8 and targeting “q_proj”, “v_proj”\\nThe first combination of QLoRA hyperparameters attempted is r=8 and targets only the attention blocks, namely \\n“q_proj” and “v_proj” for adaptation.\\nThe following code snippets gives the number of trainable parameters:\\nThese choices result in 2,662,400 parameters being updated during the fine-tuning process (~2.6 million) from a \\ntotal of ~3.2 billion parameters the model consists of. This is less than 0.1% of the model parameters. The entire \\nfine-tuning process on a single Nvidia A100 with 80 GBs of GPU for 3 epochs only takes roughly 12 minutes. The \\nGPU utilization metrics can be conveniently viewed at the metrics tab of the cluster configurations.\\n \\n1\\n2\\nmodel = get_peft_model(model, lora_config)\\nmodel.print_trainable_parameters()\\n53'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 53, 'page_label': '54'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nAt the end of the training process, the fine-tuned model is obtained by loading the adapter weights to the \\npretrained model as follows:\\nThis model can now be used for inference as any other model.\\nQualitative Evaluation \\nA couple of example prompt-response pairs are listed below\\nPrompt (passed to the model in the Alpaca format, not shown for conciseness here): \\nCreate a detailed description for the following product: Corelogic Smooth Mouse, belonging to category:  \\nOptical Mouse\\nResponse:\\nPrompt: \\nCreate a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless \\nVacuum Cleaner\\nResponse:\\nThe model has clearly been adapted for generating more consistent descriptions. However the response to the \\nfirst prompt about the optical mouse is quite short and the following phrase “The vacuum cleaner is equipped \\nwith a dust container that can be emptied via a dust container” is logically flawed.\\n \\n1 peft_model = PeftModel.from_pretrained(model, adapter_location)\\n \\n1\\n2\\nThe Corelogic Smooth Mouse is a wireless optical mouse with a smooth surface. The mouse is equipped with a 1000 DPI \\nsensor and a 1000 Hz polling rate. The mouse is available in black and white.\\n \\n1\\n2\\n3\\n4\\nThe Hoover Lightspeed is a cordless vacuum cleaner that is equipped with a lithium-ion battery. The battery is \\ncharged via a USB cable. The vacuum cleaner is equipped with a 2-in-1 brush and a turbo brush. The brush is suitable \\nfor cleaning carpets and hard floors. The turbo brush is suitable for cleaning carpets and hard floors. The vacuum \\ncleaner is equipped with a dust container that can be emptied via a dust container.\\n54'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 54, 'page_label': '55'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nHYPERPARAMETER COMBINATION #2: QLoRA with r=16 and targeting all linear layers\\nSurely, things can be improved here. It is worth exploring increasing the rank of low rank matrices learned during \\nadaptation to 16, i.e., double the value of r to 16 and keep all else  the same. This doubles the number of trainable \\nparameters to 5,324,800 (~5.3 million)\\nQualitative Evaluation \\nThe quality of output, however, remains unchanged for the same exact prompts.\\nPrompt: \\nCreate a detailed description for the following product: Corelogic Smooth Mouse, belonging to category:  \\nOptical Mouse\\nResponse:\\n \\n1\\n2\\nThe Corelogic Smooth Mouse is a high-quality optical mouse with a smooth surface. The mouse is equipped with a 1000 \\nDPI sensor and a 1000 Hz polling rate. The mouse is available in black and white.\\n \\n1\\n2\\n3\\n4\\n5\\nThe Hoover Lightspeed is a cordless vacuum cleaner that is equipped with a lithium-ion battery. The battery is \\ncharged via a USB cable. The vacuum cleaner is equipped with a 2-in-1 brush and a motorized brush. The brush is used \\nto clean hard floors and the motorized brush is used to clean carpets. The vacuum cleaner is equipped with a dust \\ncontainer that can be emptied via a dust container. The vacuum cleaner is equipped with a LED display that shows the \\nremaining battery capacity.\\nPrompt: \\nCreate a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless \\nVacuum Cleaner\\nResponse:\\nThe same lack of detail and logical flaws in detail where details are available persists. If this fine tuned model is \\nused for product description generation in a real-world scenario, this is not acceptable output.\\n55'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 55, 'page_label': '56'}, page_content=\"THE BIG BOOK OF GENERATIVE AI\\nHYPERPARAMETER COMBINATION #3: QLoRA with r=8 and targeting all linear layers\\nGiven that doubling r does not seemingly result in any perceivable increase in output quality, it is worth \\nchanging the other important knob. i.e., targeting all linear layers instead of just the attention blocks. Here, the \\nLoRA hyperparameters are r=8 and target_layers are  'q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_\\nproj' and 'lm_head'. This increases the number of parameters updated to 12,994,560 and increases the training \\ntime to roughly 15.5 minutes.\\nQualitative Evaluation \\nPrompting the model with the same prompts yield the following:\\nPrompt: \\nCreate a detailed description for the following product: Corelogic Smooth Mouse, belonging to category:  \\nOptical Mouse\\nResponse:\\nPrompt: \\nCreate a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless \\nVacuum Cleaner\\nResponse:\\nNow it is possible to see a somewhat longer coherent description of the fictitious optical mouse and there are \\nno logical flaws in the description of the vacuum cleaner. The product descriptions are not only logical, but \\nrelevant. Just as a reminder, these relatively high-quality results are obtained by fine-tuning less than a 1% of the \\nmodel’s weights with a total dataset of 5000 such prompt-description pairs formatted in a consistent manner.\\n \\n1\\n2\\n3\\nThe Corelogic Smooth Mouse is a wireless optical mouse with a smooth surface. The mouse is equipped with a 1000 DPI \\nsensor and a 1000 Hz polling rate. The mouse is equipped with a 2.4 GHz wireless connection and a USB receiver. The \\nmouse is available in black and white.\\n \\n1\\n2\\n3\\n4\\nThe Hoover Lightspeed cordless vacuum cleaner is a high-performance device that is easy to use. The device is \\nequipped with a lithium-ion battery that provides up to 40 minutes of runtime. The device is equipped with a \\nhigh-performance motor that provides a suction power of up to 200 watts. The device is equipped with a 2-in-1 dusting \\nbrush and a crevice tool. The device is equipped with a LED display that shows the remaining battery charge.\\n56\"),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 56, 'page_label': '57'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nHYPERPARAMETER COMBINATION #4: LoRA with  r=8 and targeting all linear transformer layers\\nIt is also worth exploring whether the quality of output from the model improves if the pretrained model is \\nfrozen in 8-bit instead of 4-bit. In other words, replicating the exact fine-tuning process using LoRA instead of \\nQLoRA. Here, the LoRA hyperparameters are kept the same as before, in the new-found optimal configuration, \\ni.e., r=8 and targeting all linear transformer layers during the adaptation process.\\nQualitative Evaluation \\nThe results for the two prompts used throughout the article are as given below:\\nPrompt: \\nCreate a detailed description for the following product: Corelogic Smooth Mouse, belonging to category:  \\nOptical Mouse\\nResponse:\\nPrompt: \\nCreate a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless \\nVacuum Cleaner\\nResponse:\\nAgain, there isn’t much of an improvement in the quality of the output text. \\n \\n1\\n2\\n3\\nThe Corelogic Smooth Mouse is a high-quality optical mouse with a smooth surface. The mouse is equipped with a 1000 \\nDPI sensor and a 1000 Hz polling rate. The mouse is available in three colours: black, white and red. The mouse is \\nequipped with a USB receiver and is compatible with Windows and Mac.\\n \\n1\\n2\\n3 \\n4\\nThe Hoover Lightspeed cordless vacuum cleaner is a compact and lightweight device that is easy to use. The device is \\nequipped with a lithium-ion battery that provides up to 40 minutes of cleaning time. The vacuum cleaner is equipped \\nwith a high-performance filter that ensures that the air is cleaned of dust and allergens. The device is equipped \\nwith a 2-in-1 dusting brush and a crevice tool that can be used to clean hard-to-reach areas.\\n57'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 57, 'page_label': '58'}, page_content=\"THE BIG BOOK OF GENERATIVE AI\\nKEY OBSERVATIONS\\nBased on the above set of trials, and further evidence detailed in the excellent publication presenting QLoRA, \\nit can be deduced that the value of r (the rank of matrices updated during adaptation) does not improve \\nadaptation quality beyond a certain point. The biggest improvement is observed in targeting all linear layers \\nin the adaptation process, as opposed to just the attention blocks, as commonly documented in technical \\nliterature detailing LoRA and QLoRA. The trials executed above and other empirical evidence suggest that \\nQLoRA does not indeed suffer from any discernible reduction in quality of text generated, compared to LoRA.\\nFURTHER CONSIDERATIONS FOR USING LORA ADAPTERS IN DEPLOYMENT\\nIt's important to optimize the usage of adapters and understand the limitations of the technique. The size of the \\nLoRA adapter obtained through fine-tuning is typically just a few megabytes, while the pretrained base model \\ncan be several gigabytes in memory and on disk. During inference, both the adapter and the pretrained LLM \\nneed to be loaded, so the memory requirement remains similar.\\nFurthermore, if the weights of the pre-trained LLM and the adapter aren’t merged, there will be a slight increase \\nin inference latency. Fortunately, with the PEFT library, the process of merging the weights with the adapter can \\nbe done with a single line of code as shown here:\\nThe figure below outlines the process from fine-tuning an adapter to model deployment.\\n \\n1 merged_model = peft_model.merge_and_unload()\\n58\"),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 58, 'page_label': '59'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nWhile the adapter pattern offers significant benefits, merging adapters is not a universal solution. One \\nadvantage of the adapter pattern is the ability to deploy a single large pretrained model with task-specific \\nadapters. This allows for efficient inference by utilizing the pretrained model as a backbone for different \\ntasks. However, merging weights makes this approach impossible. The decision to merge weights depends on \\nthe specific use case and acceptable inference latency. Nonetheless, LoRA/ QLoRA continues to be a highly \\neffective method for parameter efficient fine-tuning and is widely used.\\nCONCLUSION\\nLow Rank Adaptation is a powerful fine-tuning technique that can yield great results if used with the right \\nconfiguration. Choosing the correct value of rank and the layers of the neural network architecture to target \\nduring adaptation could decide the quality of the output from the fine-tuned model. QLoRA results in further \\nmemory savings while preserving the adaptation quality. Even when the fine-tuning is performed,  there are \\nseveral important engineering considerations to ensure the adapted model is deployed in the correct manner.\\nIn summary, a concise table indicating the different combinations of LoRA parameters attempted, text quality \\noutput and number of parameters updated when fine-tuning OpenLLaMA-3b-v2 for 3 epochs on 5000 \\nobservations on a single A100 is shown below.\\nTry this on Databricks! Clone the GitHub repository associated with the blog into a Databricks Repo to get \\nstarted. More thoroughly documented examples to fine-tune models on Databricks are available here.\\nR TARGET_MODULES BASE MODEL \\nWEIGHTS QUALITY OF OUTPUT NUMBER OF PARAMETERS UPDATED  \\n(IN MILLIONS)\\n8 Attention blocks 4 low 2.662\\n16 Attention blocks 4 low 5.324\\n8 All linear layers 4 high 12.995\\n8 All linear layers 8 high 12.995\\n59'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 59, 'page_label': '60'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nStage 4: Pretraining\\nPretraining a model from scratch refers to the process of training a language model on a large corpus of data \\n(e.g., text, code) without using any prior knowledge or weights from an existing model. This is in contrast to fine-\\ntuning, where an already pretrained model is further adapted to a specific task or dataset. The output of full \\npretraining is a base model that can be directly used or further fine-tuned for downstream tasks.\\nWHEN TO USE PRETRAINING\\nChoosing to pretrain an LLM from scratch is a significant commitment, both in terms of data and computational \\nresources. Here are some scenarios where it makes sense:\\n1. Unique data sources: If you possess a unique and extensive corpus of data that is distinct from what \\navailable pretrained LLMs have seen, it might be worth pretraining a model to capture this uniqueness\\n2. Domain specificity: Organizations might want a base model tailored to their specific domain (e.g., \\nmedical, legal, code) to ensure even the foundational knowledge of the model is domain-specific\\n3. Full control over training data: Pretraining from scratch offers transparency and control over the data \\nthe model is trained on. This may be essential for ensuring data security, privacy and custom tailoring of \\nthe model’s foundational knowledge.\\n4. Avoiding third-party biases: Pretraining ensures that your LLM application does not inherit biases or \\nlimitations from third-party pretrained models.\\n60'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 60, 'page_label': '61'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nPRETRAINING IN PRACTICE\\nGiven the resource-intensive nature of pretraining, careful planning and sophisticated tooling are required. \\nLibraries like PyTorch FSDP and Deepspeed, mentioned in the fine-tuning section, are similarly required for \\ntheir distributed training capabilities when pretraining an LLM from scratch. The following only scratches the \\nsurface on some of the considerations one must take into account when pretraining an LLM: \\n ■ Large-scale data preprocessing: A pretrained model is only as good as the data it is trained on. Thus, \\nit becomes vitally important to ensure robust data preprocessing is conducted prior to model training. \\nGiven the scale of the training data involved, this preprocessing typically requires distributed frameworks \\nlike Apache Spark™. Consideration must be given to factors such as dataset mix and deduplication \\ntechniques to ensure the model is exposed to a wide variety of unique data points.\\n ■ Hyperparameter selection and tuning: Before executing full-scale training of an LLM, determining \\nthe set of optimal hyperparameters is crucial. Given the high computational cost associated with LLM \\ntraining, extensive hyperparameter sweeps are not always feasible. Instead, informed decisions based \\non smaller-scale searches or prior research are employed. Once a promising set is identified, these \\nhyperparameters are used for the full training run. Tooling like MLflow is essential to manage and track \\nthese experiments.\\n ■ Maximizing resource utilization: Given the high costs associated with long-running distributed GPU \\ntraining jobs, it is hugely important to maximize resource utilization. MosaicML’s composer is an example \\nof a library that uses PyTorch FSDP with additional optimizations to maximize Model FLOPs Utilization \\n(MFU) and Hardware FLOPs Utilization (HFU) during training.\\n ■ Handling GPU failures: Training large models can run for days or even weeks. During such large-scale \\ntraining for this length of time, hardware failures, especially GPU failures, can (and typically do) occur.  \\nIt is essential to have mechanisms in place to handle such failures gracefully. \\n ■ Monitoring and evaluation: Close monitoring of the training process is essential. Saving model \\ncheckpoints regularly and evaluating validation sets not only act as safeguards but also provide insights \\ninto model performance and convergence trends.\\nIn cases where pretraining an LLM from scratch is required, Mosaic AI Training provides a platform to conduct \\ntraining of multibillion-parameter models in a highly optimized and automated manner. Automatically handling \\nGPU failures and resuming training without human intervention and leveraging Mosaic AI Streaming for efficient \\nstreaming of data into the training process are just some of the capabilities provided out of the box.\\n61'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 61, 'page_label': '62'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nThe Value of Training Models From Scratch on Databricks \\nAfter diving into the details of starting a model’s training from scratch, why you might do it and the advanced \\ntools needed, let’s look at a real-world example to show that training top-notch language models isn’t as \\ncomplex or expensive as it might seem. This shift highlights that even organizations watching their budget can \\nstart training their own models, with Databricks providing the necessary support and infrastructure. Databricks \\nstands out as uniquely capable to help customers train their own models from scratch, enabling them to fully \\nown their AI assets.\\nPretraining Use Cases\\nTraining Stable Diffusion From Scratch for <$50K With MosaicML\\nby Mihir Patel, Cory Stephenson, Landan Seguin, Austin Jacobson and Erica Ji Yuen\\nWe’ve replicated Stable Diffusion 2 for less than $50K, and we’ve open sourced the training code so you can \\ntoo! This is a 3x cost reduction from our last blog post and an 8x reduction from the original Stable Diffusion 2, \\nmaking training large-scale diffusion models from scratch more accessible than ever before.\\nToday, we are excited to show the results of our own training run: under $50K to train Stable Diffusion 2 base1 \\nfrom scratch in 7.45 days using the MosaicML platform.\\n62'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 62, 'page_label': '63'}, page_content=\"THE BIG BOOK OF GENERATIVE AI\\nFigure 1: Imagining mycelium couture. Integrating image generation into the design process pushes creative boundaries. All images in this mood board \\nwere created with our internal diffusion model trained from scratch on the MosaicML Platform.\\nTraining your own image generation model on your own data is now easy and accessible. By training your own \\ndiffusion models, you can:\\n ■ Use your proprietary data\\n ■ Tune the representations for certain art or photography styles\\n ■ Avoid violating intellectual property laws so your models can be used commercially\\nWe’ve open sourced our code and methods to train a diffusion model from scratch so that you can train your \\nown; check it out here! If you're interested in training your own models, contact us for a demo, and read on to \\nlearn more about our engineering setup!\\n63\"),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 63, 'page_label': '64'}, page_content=\"THE BIG BOOK OF GENERATIVE AI\\nSETUP\\nModel: Our diffusion model is a ComposerModel \\ncomposed of a Variational Autoencoder (VAE), a \\nCLIP model, a U-Net, and a diffusion noise scheduler, \\nall from the HuggingFace's Diffusers library. All of \\nthe model configurations were based on stabilityai/\\nstable-diffusion-2-base.\\nFigure 2: Getting creative and embracing serendipity. A variety of subjects, art, and photography styles are generated by our diffusion model.\\nFigure 3: Simplified diagram of the diffusion model.\\n64\"),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 64, 'page_label': '65'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nData: We trained on a subset of LAION-5B that includes samples with English-only captions and an aesthetic \\nscore of 4.5+. Similar to Stable Diffusion 2 base, we did two phases of training based on the image resolution of \\nthe training data. For the first phase of training, we used all images with resolution >=256x256, amounting to 790 \\nmillion image-caption samples. For the second phase of training, we only used images with resolution >=512x512, \\namounting to 300 million image-caption samples.\\nCompute: Both phases of training ran on 128 NVIDIA A100 GPUs. The first training phase was run for 550k \\niterations in 1.6 days while the second phase was run for 850k iterations in 4.9 days, for a total of 20,051 A100 \\nhours for training. In addition to the training time, we pre-computed the latents for the VAE and CLIP model \\nto reduce training time and cost when making multiple passes over the dataset. Pre-computing the latents \\nrequired an additional 3,784 A100 hours, resulting in 23,835 A100 hours in total. Assuming a cost of $2 / A100 \\nhour, the total price tag is $47.7k.\\nTech Stack: We used Composer for our training framework, StreamingDataset to load our 100TB of data, and \\nthe MosaicML platform for overcoming infrastructure challenges when training and evaluating on 128 GPUs.\\nFigure 4: Loss curve for our training run. Our platform caught two hardware failures and automatically restarted the run with no human intervention. \\nThe loss discontinuity is because phase 2 increases the resolution from 256x256 to 512x512.\\n65'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 65, 'page_label': '66'}, page_content=\"THE BIG BOOK OF GENERATIVE AI\\nCHALLENGES AND SOLUTIONS\\nWhether for diffusion models or large language models, training at scale has significant challenges. We trained \\nour diffusion model using the MosaicML platform, which addresses these challenges automatically so you can \\nfocus on training the best possible model. Below are three main challenges with large-scale training and how our \\nplatform solves them.\\nINFRASTRUCTURE\\nTraining large models on large datasets requires significant compute. The MosaicML platform effortlessly \\norchestrates hundreds of GPUs on any cloud provider. For example, our primary training run took place on \\na cluster of 128 A100 GPUs. To ensure evaluating the model didn't slow training, we automatically kicked off \\nevaluation runs at every checkpoint on different clusters using different cloud providers, seamlessly scaling up \\nto 64 GPUs and back down to 8 GPUs depending on availability.\\nEven after training is underway, software or hardware failures can halt training, leaving GPUs idle until someone \\nnotices or requiring someone on-call 24/7 to babysit the run. Thankfully, the Node Doctor and Watchdog \\nfeatures of the MosaicML platform automatically detect failed nodes and resume jobs as needed. With auto-\\nresumption, we recover from failures and continue training with zero human intervention, avoiding expensive \\ndowntime and human babysitting. Just launch and train!\\nEFFICIENT SOFTWARE\\nSoftware is difficult to configure optimally. Our PyTorch-based Composer library maximizes training efficiency \\nat scale. As shown in our previous blog post, Composer demonstrated excellent throughput scaling as the \\nnumber of GPUs increased. For this update, we added further optimizations (Low Precision GroupNorm and Low \\nPrecision LayerNorm, Fully Sharded Data Parallel) to achieve near-perfect strong scaling up to 128 GPUs, bringing \\nthe cost down to $50k. We also used Composer's native Exponential Moving Average (EMA) algorithm, which \\nallowed us to start EMA close to the end of training (iteration 800k of the final phase) to gain all the benefits of \\nEMA while saving on memory and compute for the majority of training.\\n66\"),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 66, 'page_label': '67'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nMANAGING 100TB OF DATA\\nWe trained with a subset of LAION-5B that contained 790 million samples, amounting to >100TB of data. The \\nsheer size of the dataset makes it difficult to manage, especially when working with multiple clusters with \\nseparate local storage. The MosaicML StreamingDataset library makes working with massive datasets much \\nsimpler and faster. There were three key features of the StreamingDataset library that were especially useful for \\nthis training run:\\n1. Mixing datasets stored in different locations. We bucketed samples based on image resolution into \\ndifferent datasets. At training time, we used the MosaicML StreamingDataset library to train on a mixture \\nof resolutions from these datasets.\\n2. Instant mid-epoch resumption. We were able to instantly resume training in the middle of an epoch. This \\nsaved hours by avoiding the need to iterate over the entire dataset to get back to where we left off.\\n3. Elastic determinism. The MosaicML StreamingDataset library deterministically shuffles data, even when \\nchanging the number of GPUs used for training. This made it possible for us to exactly reproduce training \\nruns, dramatically simplifying debugging.\\nHUMAN EVALUATION RESUL TS\\nEvaluating image generation models is difficult, and there is no substitute for human evaluation. In a blind human \\nevaluation, we measured user preferences in image quality and prompt alignment between Stable Diffusion 2 \\nand our diffusion model. Based on user preferences, we concluded that the two models were comparable in \\nquality (see Figure 5) All images were generated based on prompts from the Drawbench benchmark proposed \\nin the Imagen paper. For more details, see our follow-up blog post coming soon.\\n67'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 67, 'page_label': '68'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nFigure 5: Results from our human evaluation of image quality (left) and prompt alignment (right). Error bars show 95% confidence intervals. In both ex-\\nperiments, the difference in user preference rates between the two models was comparable to the uncertainty in the measurement, so we conclude \\nthat the two models are of comparable overall quality.\\nDeep Dive: How We Trained Stable Diffusion for Less Than $50K \\nby Mihir Patel, Erica Ji Yuen, Cory Stephenson and Landan Seguin\\nIn our previous example, we showed how we used the MosaicML platform, Streaming datasets, and the \\nComposer library to train a Stable Diffusion model from scratch for less than $50,000. Now, we do a deep dive \\ninto the technical details behind this speedup, demonstrating how we were able to replicate the Stable Diffusion \\n2 base model in just 6.8 days.\\nTry out our code here!\\nMany organizations require high-performing large AI models tailored to their specific use cases. However, \\ntraining such models is often prohibitively time-consuming and expensive, requiring vast amounts of \\ncomputation and expertise. This is where MosaicML comes in: we provide a comprehensive solution that \\nsimplifies and accelerates the process of training these models.\\n68'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 68, 'page_label': '69'}, page_content=\"THE BIG BOOK OF GENERATIVE AI\\nIn our previous blog post, we announced that we have trained a diffusion model comparable to Stable Diffusion \\n2 from scratch for $47.7K. In this post, we dive into the technical details to highlight how we achieved an 8x \\nspeedup/cost reduction from the number reported by StabilityAI and a 3x cost reduction over our own \\nbaseline. All our code is open source and easy to modify for custom use cases. If you're interested in learning \\nmore about our stack, please contact us for a demo.\\nACCELERATING TRAINING\\nWe’ve introduced a variety of techniques, from fusions to sharding strategies, that dramatically speed up \\ntraining and lower costs by almost 3x.\\nFigure 1: Stable Diffusion 2 model architecture. For training, the VAE image encoder, CLIP text encoder and U-Net are used. For inference,  \\nthe CLIP Text Encoder, U-Net, and VAE image decoder are used. Only the U-Net weights are updated during training; CLIP and VAE are fixed.\\n69\"),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 69, 'page_label': '70'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nXFORMERS FLASHATTENTION\\nFigure 2: xFormers accelerates cross attention blocks in the U-Net.\\n70'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 70, 'page_label': '71'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nThe attention layers in the Stable Diffusion architecture can be slow with a naive implementation, so most \\ncodebases use faster implementations that rely on fused kernels. In our stack, we leverage xFormers \\nFlashAttention.\\nWhile this was enabled in our original blog post, we found an issue with the usage that resulted in extra memory \\nbeing consumed on rank 0. After fixing this bug, we were able to increase our device microbatch size1 from 4 to \\n8. This yielded a sizable speedup, since A100s are more efficient at larger matrix sizes.\\nPRECOMPUTING LATENTS\\nFigure 3: Two phase training with precomputed latents. \\nFirst, all VAE and CLIP latents are precomputed and stored. \\nThen, the U-Net diffusion model is trained using these \\nprecomputed latents.\\n71'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 71, 'page_label': '72'}, page_content=\"THE BIG BOOK OF GENERATIVE AI\\nStable Diffusion is a combination of three models: a variational autoencoder (VAE), a text encoder (CLIP), and a \\nU-Net. During diffusion training, only the U-Net is trained, and the other two models are used to compute the \\nlatent encodings of the image and text inputs. Standard training involves computing the VAE and CLIP latents for \\nevery batch, but this does a lot of duplicate work when training for multiple epochs: latents are re-computed for \\neach image every time it is used. Instead, we precompute the latents once before training. Empirically, we have 2 \\nepochs at 256 resolution and 5 epochs at 512 resolution, so we avoid 6 extra VAE and CLIP calls per image-text \\npair in the dataset.\\nAdditionally, when pre-computing the latents, we can lower the precision of the VAE and CLIP models to \\nfp16. This could lead to numerical instability if we were training the VAE and CLIP and used this precision for \\nthe backward pass. However, since we're only using them for inference, we can safely lower the precision, \\nwhich increases speed. The extra memory savings also let us use far larger batch sizes and improve hardware \\nutilization during the latent precomputation.\\n72\"),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 72, 'page_label': '73'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nLOW PRECISION LAYERNORM AND GROUPNORM\\nFigure 4: Low Precision LayerNorm and Low Precision GroupNorm. Low precision gives faster training and lower memory usage, enabling larger \\nmicrobatches.\\n73'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 73, 'page_label': '74'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nDiffusion training is done in automatic mixed precision by default. This uses half precision (fp16) in most \\nlayers, but fp32 in a few numerically unstable layers like normalization and softmax. The Stable Diffusion U-Net \\narchitecture uses several LayerNorm and GroupNorm layers, which by default are run in fp32.\\nMotivated by our finding that half precision LayerNorms are safe to use in language models, we decided to \\ntry out half precision LayerNorm and GroupNorm layers. This change resulted in identical loss curves and no \\ninstability in our experiments.\\nWhile we did observe some throughput improvement, the real benefit was decreased memory usage. Now, \\nalong with removing the VAE and CLIP memory by precomputing latents, we have enough space on our 40GB \\nA100 to increase our microbatch size from 8 to 16, 4x larger than what we started with!\\n74'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 74, 'page_label': '75'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nFULLY SHARDED DATA PARALLELISM\\nFigure 5: Fully Sharded Data Parallel with SHARD_GRAD_OP speeds up the gradient update step and enables linear scaling.\\n75'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 75, 'page_label': '76'}, page_content=\"THE BIG BOOK OF GENERATIVE AI\\nMosaicML Composer, our go-to training library, includes support for PyTorch Fully Sharded Data Parallelism \\n(FSDP). We primarily use this to shard large scale models like 10B+ parameter LLMs that don't fit in a single \\ndevice across hundreds of GPUs for incredibly fast training. Stable Diffusion doesn't require sharding since it  \\nfits in a single GPU. However, some of the distributed features in FSDP are still useful for speeding up training  \\non a large number of GPUs.\\nWhen batches don’t fit into memory, we do several forward and backward passes on smaller microbatches, \\nfollowed by a single gradient update. If we use a small number of GPUs to train, we have far more forward and \\nbackward passes per gradient update, so the time spent on the gradient update doesn't matter. However, at \\n128+ GPUs with a microbatch size of 16, we're only doing one forward and one backward pass for each gradient \\nupdate. At this scale, the gradient update step starts to become a significant bottleneck.\\nTo tackle this problem, we use FSDP's SHARD_GRAD_OP mode. In normal training, each GPU communicates all \\nits gradients to every other GPU, and then each GPU updates its local copy of the model. With this FSDP variant, \\neach GPU only gets the gradients and updates the weights for a small part of the model before sending the \\nupdated weights for that part of the model to all of the other GPUs. By dividing the update step across all the \\nGPUs, we can ensure the amount of work per GPU decreases as we increase the number of GPUs, helping us \\nachieve linear scaling.\\n76\"),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 76, 'page_label': '77'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nSCHEDULED EMA\\nFigure 6: Loss curve of our training run with the scheduled exponential moving average (EMA) period highlighted.\\n77'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 77, 'page_label': '78'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nStable Diffusion 2 uses Exponential Moving Averaging (EMA), which maintains an exponential moving average \\nof the weights. At every time step, the EMA model is updated by taking 0.9999 times the current EMA model \\nplus 0.0001 times the new weights after the latest forward and backward pass. By default, the EMA algorithm is \\napplied after every gradient update for the entire training period. However, this can be slow due to the memory \\noperations required to read and write all the weights at every step.\\nTo avoid this costly procedure, we start with a key observation: since the old weights are decayed by a factor of \\n0.9999 at every batch, the early iterations of training only contribute minimally to the final average. This means \\nwe only need to take the exponential moving average of the final few steps. Concretely, we train for 1,400,000 \\nbatches and only apply EMA for the final 50,000 steps, which is about 3.5% of the training period. The weights \\nfrom the first 1,350,000 iterations decay away by (0.9999)^50000, so their aggregate contribution would have \\na weight of less than 1% in the final model. Using this technique, we can avoid adding overhead for 96.5% of \\ntraining and still achieve a nearly equivalent EMA model.\\n78'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 78, 'page_label': '79'}, page_content=\"THE BIG BOOK OF GENERATIVE AI\\nFINAL TIME AND COST ESTIMATES\\nFigure 7: Throughput at 512x512 images on 128 GPUs as each speedup optimization is enabled. We achieve a total cumulative speedup of 2.71x over \\nthe baseline.\\nWe’ve shown how we obtained nearly a 3x reduction in time and cost to train Stable Diffusion compared to our \\noriginal results. With xFormers, precomputed latents, low precision LayerNorm, low precision GroupNorm, FSDP, \\nand scheduled EMA, Table 1 shows it's possible to train Stable Diffusion in just 6.79 days using 21,000 A100-\\nhours for a total cost of less than $42,000. We estimated these times and costs by measuring throughput for \\ntraining 1.1 billion 256x256 images and 1.7 billion 512x512 images with a max tokenized length of 77 at a global \\nbatch size of 2048, as detailed in the Stable Diffusion 2 base model card. This is slightly cheaper than our \\npreviously reported run with a cost of $47.7k as it does not account for any time spent on evaluation or restarts \\ndue to hardware failures.\\n79\"),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 79, 'page_label': '80'}, page_content=\"THE BIG BOOK OF GENERATIVE AI\\nNUMBER  \\nOF A100S\\nTHROUGHPUT  \\nFOR U-NET \\n@ 256X256 \\n(IMAGES / \\nSECOND)\\nTHROUGHPUT \\nFOR U-NET \\n@ 512X512 \\n(IMAGES / \\nSECOND)\\nTHROUGHPUT \\nFOR U-NET @ \\n512X512 WITH \\nEMA (IMAGES /  \\nSECOND)\\nDAYS TO TRAIN \\nON MOSAICML \\nCLOUD\\nAPPROX. COST \\nON MOSAICML \\nCLOUD\\n8 1100 290 290 101.04 $38,800\\n16 2180 585 580 50.29 $38,630\\n32 4080 1195 1160 25.01 $38,420\\n64 8530 2340 2220 12.63 $38,800\\n128 11600 4590 3927 6.79 $41,710\\nTable 1: Estimated time and cost to train a Stable Diffusion model on 1.1 billion images at 256x256 resolution, followed by 1.7 billion images at 512x512 \\nresolution. Different rows show different numbers of NVIDIA 40GB A100 GPUs at a global batch size of 2048.\\nThese optimizations show that training image generation models from scratch is within reach for everyone. For \\nupdates on our latest work, join our Community Slack or follow us on Twitter. If your organization wants to start \\ntraining diffusion models today, please schedule a demo online or email us at demo@mosaicml.com.\\n1 When training large models with big batches that don't fit in memory in a single pass, each batch is divided into smaller microbatches. On each \\ndevice, we can do a forward and backward pass for each microbatch and sum the gradients at the end to compute a gradient update equivalent to \\na single forward and backward pass with the entire batch all at once.\\n80\"),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 80, 'page_label': '81'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nStage 5: LLM Evaluation\\nConstant evaluation and monitoring of deployed large language models (LLMs) and generative AI applications \\nare crucial due to the dynamic nature of both the data they interact with and the environments in which they \\noperate. These systems learn from vast datasets and can evolve over time, potentially leading to shifts in \\nperformance, accuracy or even the emergence of biases. Continuous monitoring ensures that any deviation \\nfrom expected behavior can be detected and corrected promptly, maintaining the integrity and reliability \\nof the AI application. As user needs and societal norms change, ongoing evaluation allows these models to \\nadapt, ensuring their outputs remain relevant, appropriate and effective. This vigilance not only mitigates risks \\nassociated with AI deployments, such as ethical concerns and regulatory compliance, but also maximizes the \\nvalue and utility these technologies bring to organizations and end users. \\nEvaluating LLMs is a challenging and evolving domain, primarily because LLMs often demonstrate uneven \\ncapabilities across different tasks. An LLM might excel in one benchmark, but slight variations in the prompt \\nor problem can drastically affect its performance. The dynamic nature of LLMs and their vast potential \\napplications only amplify the challenge of establishing comprehensive evaluation standards.\\n81'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 81, 'page_label': '82'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nPresent challenges involved with evaluating LLM-powered applications include the following:\\n ■ Variable performance: LLMs can be sensitive to prompt variations, demonstrating high proficiency in \\none task but faltering with slight deviations in prompts.\\n ■ Lack of ground truth: Since most LLMs output natural language, it is very difficult to evaluate the outputs \\nvia traditional NLP metrics (BLEU, ROUGE, etc.). For example, suppose an LLM were used to summarize \\na news article. Two equally good summaries might have almost completely different words and word \\norders, so even defining a “ground truth” label becomes difficult or impossible.\\n ■ Domain-specific evaluation: For domain-specific fine-tuned LLMs, popular generic benchmarks \\nmay not capture their nuanced capabilities. Such models are tailored for specialized tasks, making \\ntraditional metrics less relevant. This divergence often necessitates the development of domain-specific \\nbenchmarks and evaluation criteria. See the example of Replit’s code generation LLM. \\n ■ Reliance on human judgment: It is often the case that LLM performance is being evaluated in domains \\nwhere text is scarce or there is a reliance on subject matter expert knowledge. In such scenarios, \\nevaluating LLM output can be costly and time-consuming.\\nTo help give examples of how this can be accomplished, here are two great examples of how you can monitor \\nand evaluate your deployed LLMs and generative AI applications using Databricks.\\nLLM Evaluation Examples\\nBest Practices for LLM Evaluation of RAG Applications  \\nA Case Study on the Databricks Documentation Bot\\nby Quinn Leng, Kasey Uhlenhuth and Alkis Polyzotis\\nChatbots are the most widely adopted use case for leveraging the powerful chat and reasoning capabilities \\nof large language models (LLM). The retrieval augmented generation (RAG) architecture is quickly becoming \\nthe industry standard for developing chatbots because it combines the benefits of a knowledge base (via a \\nvector store) and generative models (e.g., GPT-3.5 and GPT-4) to reduce hallucinations, maintain up-to-date \\ninformation, and leverage domain-specific knowledge. However, evaluating the quality of chatbot responses \\nremains an unsolved problem today. With no industry standards defined, organizations resort to human grading \\n(labeling) –which is time-consuming and hard to scale.\\n82'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 82, 'page_label': '83'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nWe applied theory to practice to help form best practices for LLM automated evaluation so you can deploy RAG \\napplications to production quickly and with confidence. This blog represents the first in a series of investigations \\nwe’re running at Databricks to provide learnings on LLM evaluation. All research in this post was conducted by \\nQuinn Leng, Senior Software Engineer at Databricks and creator of the Databricks Documentation AI Assistant. \\nCHALLENGES WITH AUTO-EVALUATION IN PRACTICE\\nRecently, the LLM community has been exploring the use of “LLMs as a judge” for automated evaluation with \\nmany using powerful LLMs such as GPT-4 to do the evaluation for their LLM outputs. The lmsys group’s research \\npaper explores the feasibility and pros/cons of using various LLMs (GPT-4, ClaudeV1, GPT-3.5) as the judge for \\ntasks in writing, math, and world knowledge.\\nDespite all this great research, there are still many unanswered questions about how to apply LLM judges  \\nin practice:\\n ■ Alignment With Human Grading: Specifically for a document-Q&A chatbot, how well does an \\nLLM judge’s grading reflect the actual human preference in terms of correctness, readability and \\ncomprehensiveness of the answers? \\n ■ Accuracy Through Examples: What’s the effectiveness of providing a few grading examples to the LLM \\njudge and how much does it increase the reliability and reusability of the LLM judge on different metrics?\\n ■ Appropriate Grade Scales: What grading scale is recommended because different grading scales are \\nused by different frameworks (e.g., AzureML uses 0 to 100 whereas langchain uses binary scales)?\\n ■ Applicability Across Use Cases: With the same evaluation metric (e.g. correctness), to what extent can \\nthe evaluation metric be reused across different use cases (e.g. casual chat, content summarization, \\nretrieval augmented generation)?\\n83'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 83, 'page_label': '84'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nAPPLYING EFFECTIVE AUTO-EVALUATION FOR RAG APPLICATIONS\\nWe explored the possible options for the questions outlined above in the context of our own chatbot \\napplication at Databricks. We believe that our findings generalize and can thus help your team effectively \\nevaluate RAG-based chatbots at a lower cost and faster speed:\\n ■ LLM-as-a-judge agrees with human grading on over 80% of judgments. Using LLMs-as-a-judge for our \\ndocument-based chatbot evaluation was as effective as human judges, matching the exact score in over \\n80% of judgments and being within a 1-score distance (using a scale of 0-3) in over 95% of judgments.\\n ■ Save costs by using GPT-3.5 with examples. GPT-3.5 can be used as an LLM judge if you provide \\nexamples for each grading score. Because of the context size limit it’s only practical to use a low-\\nprecision grading scale. Using GPT-3.5 with examples instead of GPT-4 drives down the cost of LLM judge \\nby 10x and improves the speed by more than 3x.\\n ■ Use low-precision grading scales for easier interpretation. We found lower-precision grading scores like 0, \\n1, 2, 3 or even binary (0, 1) can largely retain precision compared to higher precision scales like 0 to 10.0 or \\n0 to 100.0, while making it considerably easier to provide grading rubrics to both human annotators and \\nLLM judges. Using a lower precision scale also allows consistency of grading scales among different LLM \\njudges (e.g., between GPT-4 and claude2).\\n ■ RAG applications require their own benchmarks. A model might have good performance on a published \\nspecialized benchmark (e.g. casual chat, math, or creative writing) but that doesn’t guarantee good \\nperformance on other tasks (e.g. answering questions from a given context). Benchmarks should only be \\nused if the use case matches, i.e., a RAG application should only be evaluated with a RAG benchmark.\\nBased on our research, we recommend the following procedure when using an LLM judge: \\n ■ Use a 1-5 grading scale\\n ■ Use GPT-4 as an LLM judge with no examples to understand grading rules\\n ■ Switch your LLM judge to GPT-3.5 with one example per score\\n84'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 84, 'page_label': '85'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nOUR METHODOLOGY FOR ESTABLISHING BEST PRACTICES\\nThe remainder of this post will walk through the series of experiments we conducted to form these  \\nbest practices. \\nEXPERIMENT SETUP\\n85'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 85, 'page_label': '86'}, page_content='THE BIG BOOK OF GENERATIVE AI\\n The experiment had three steps: \\n1. Generate evaluation dataset: We created a dataset from 100 questions and context from Databricks \\ndocuments. The context represents (chunks of) documents that are relevant to the question. \\n2. Generate answer sheets: Using the evaluation dataset, we prompted different language models to \\ngenerate answers and stored the question-context-answer pairs in a dataset called “answer sheets”. In \\nthis investigation, we used GPT-4, GPT-3.5, Claude-v1, Llama2-70b-chat, Vicuna-33b, and mpt-30b-chat.\\n3. Generate grades: Given the answer sheets, we used various LLMs to generate grades and reasoning \\nfor the grades. The grades are a composite score of Correctness (weighted: 60%), Comprehensiveness \\n(weighted: 20%) and Readability (weighted: 20%). We chose this weighting scheme to reflect our \\npreference for Correctness in the generated answers. Other applications may tune these weights \\ndifferently but we expect Correctness to remain a dominant factor.\\n86'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 86, 'page_label': '87'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nAdditionally, the following techniques were used to avoid positional bias and improve reliability:\\n ■ Low temperature (temperature 0.1) to ensure reproducibility\\n ■ Single-answer grading instead of pairwise comparison\\n ■ Chain of thoughts to let the LLM reason about the grading process before giving the final score\\n ■ Few-shots generation where the LLM is provided with several examples in the grading rubric for each \\nscore value on each factor (Correctness, Comprehensiveness, Readability)\\nEXPERIMENT 1: ALIGNMENT WITH HUMAN GRADING\\nTo confirm the level of agreement between human annotators and LLM judges, we sent answer sheets  \\n(grading scale 0-3) from gpt-3.5-turbo and vicuna-33b to a labeling company to collect human labels,  \\nand then compared the result with GPT-4’s grading output. Below are the findings:\\nHuman and GPT-4 judges can reach above 80% agreement on the correctness and readability score.  \\nAnd if we lower the requirement to be smaller or equal than 1 score difference, the agreement level can  \\nreach above 95%. The Comprehensiveness metric has less alignment, which matches what we’ve heard  \\nfrom business stakeholders who shared that “comprehensive” seems more subjective than metrics like \\nCorrectness or Readability.\\n87'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 87, 'page_label': '88'}, page_content=\"THE BIG BOOK OF GENERATIVE AI\\nEXPERIMENT 2: ACCURACY THROUGH EXAMPLES\\nThe lmsys paper uses this prompt to instruct the LLM judge to evaluate based on the helpfulness, relevance, \\naccuracy, depth, creativity, and level of detail of the response. However, the paper doesn’t share specifics on the \\ngrading rubric. From our research, we found many factors can significantly affect the final score, for example:\\n ■ The importance of different factors: Helpfulness, Relevance, Accuracy, Depth, Creativity\\n ■ The interpretation of factors like Helpfulness is ambiguous \\n ■ If different factors conflict with each other, where an answer is helpful but is not accurate \\nWe developed a rubric for instructing an LLM judge for a given grading scale, by trying the following:\\n1. Original Prompt: Here is the original prompt used in the lmsys paper:\\nWe adapted the original lmsys paper prompt to emit our metrics about correctness, comprehensiveness and \\nreadability, and also prompt the judge to provide one line justification before giving each score (to benefit from \\nchain-of-thought reasoning). Below are the zero-shot version of the prompt which doesn’t provide any example, \\nand the few-shot version of the prompt which provides one example for each score. Then we used the same \\nanswer sheets as input and compared the graded results from the two prompt types.\\n2. Zero Shot Learning: Require the LLM judge to emit our metrics about correctness, comprehensiveness \\nand readability, and also prompt the judge to provide one line justification for each score.\\n \\nPlease act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question \\ndisplayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and \\nlevel of detail of the response. Begin your evaluation by providing a short explanation. Be as objective as possible. After \\nproviding your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format\\n \\nPlease act as an impartial judge and evaluate the quality of the provided answer which attempts to answer the provided \\nquestion based on a provided context. \\n  You'll be given a function grading_function which you'll call for each provided context, question and answer to submit your \\nreasoning and score for the correctness, comprehensiveness and readability of the answer\\n88\"),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 88, 'page_label': '89'}, page_content='THE BIG BOOK OF GENERATIVE AI\\n3. Few Shots Learning: We adapted the zero shot prompt to provide explicit examples for each score in the \\nscale. The new prompt:\\n \\nPlease act as an impartial judge and evaluate the quality of the provided answer which attempts to answer the provided \\nquestion based on a provided context.\\n  You\\'ll be given a function grading_function which you\\'ll call for each provided context, question and answer to submit your \\nreasoning and score for the correctness, comprehensiveness and readability of the answer. \\n  \\n  Below is your grading rubric: \\n- Correctness: If the answer correctly answer the question, below are the details for different scores:\\n  - Score 0: the answer is completely incorrect, doesn’t mention anything about the question or is completely contrary to the \\ncorrect answer.\\n      - For example, when asked “How to terminate a databricks cluster”, the answer is empty string, or content that’s \\ncompletely irrelevant, or sorry I don’t know the answer.\\n  - Score 1: the answer provides some relevance to the question and answers one aspect of the question correctly.\\n      - Example:\\n          - Question: How to terminate a databricks cluster\\n          - Answer: Databricks cluster is a cloud-based computing environment that allows users to process big data and run \\ndistributed data processing tasks efficiently.\\n          - Or answer:  In the Databricks workspace, navigate to the \"Clusters\" tab. And then this is a hard question that I \\nneed to think more about it\\n  - Score 2: the answer mostly answer the question but is missing or hallucinating on one critical aspect.\\n      - Example:\\n          - Question: How to terminate a databricks cluster”\\n          - Answer: “In the Databricks workspace, navigate to the \"Clusters\" tab.\\n          Find the cluster you want to terminate from the list of active clusters.\\n          And then you’ll find a button to terminate all clusters at once”\\n  - Score 3: the answer correctly answer the question and not missing any major aspect\\n      - Example:\\n          - Question: How to terminate a databricks cluster\\n          - Answer: In the Databricks workspace, navigate to the \"Clusters\" tab.\\n          Find the cluster you want to terminate from the list of active clusters.\\n          Click on the down-arrow next to the cluster name to open the cluster details.\\n          Click on the \"Terminate\" button. A confirmation dialog will appear. Click \"Terminate\" again to confirm the action.”\\n- Comprehensiveness: How comprehensive is the answer, does it fully answer all aspects of the question and provide \\ncomprehensive explanation and other necessary information. Below are the details for different scores:\\n  - Score 0: typically if the answer is completely incorrect, then the comprehensiveness is also zero score.\\n  - Score 1: if the answer is correct but too short to fully answer the question, then we can give score 1 for \\ncomprehensiveness.\\n      - Example:\\n          - Question: How to use databricks API to create a cluster?\\n          - Answer: First, you will need a Databricks access token with the appropriate permissions. You can generate this \\ntoken through the Databricks UI under the \\'User Settings\\' option. And then (the rest is missing)\\n  - Score 2: the answer is correct and roughly answer the main aspects of the question, but it’s missing description about \\ndetails. Or is completely missing details about one minor aspect.  \\n89'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 89, 'page_label': '90'}, page_content='THE BIG BOOK OF GENERATIVE AI\\n      - Example:\\n          - Question: How to use databricks API to create a cluster?\\n          - Answer: You will need a Databricks access token with the appropriate permissions. Then you’ll need to set up the \\nrequest URL, then you can make the HTTP Request. Then you can handle the request response.\\n      - Example:\\n          - Question: How to use databricks API to create a cluster?\\n          - Answer: You will need a Databricks access token with the appropriate permissions. Then you’ll need to set up the \\nrequest URL, then you can make the HTTP Request. Then you can handle the request response.\\n  - Score 3: the answer is correct, and covers all the main aspects of the question\\n- Readability: How readable is the answer, does it have redundant information or incomplete information that hurts the \\nreadability of the answer.\\n  - Score 0: the answer is completely unreadable, e.g. fully of symbols that’s hard to read; e.g. keeps repeating the words \\nthat it’s very hard to understand the meaning of the paragraph. No meaningful information can be extracted from the answer.\\n  - Score 1: the answer is slightly readable, there are irrelevant symbols or repeated words, but it can roughly form a \\nmeaningful sentence that cover some aspects of the answer.\\n      - Example:\\n          - Question: How to use databricks API to create a cluster?\\n          - Answer: You you  you  you  you  you  will need a Databricks access token with the appropriate permissions. And \\nthen then you’ll need to set up the request URL, then you can make the HTTP Request. Then Then Then Then Then Then Then Then \\nThen\\n  - Score 2: the answer is correct and mostly readable, but there is one obvious piece that’s affecting the readability \\n(mentioning of irrelevant pieces, repeated words)\\n      - Example:\\n          - Question: How to terminate a databricks cluster\\n          - Answer: In the Databricks workspace, navigate to the \"Clusters\" tab.\\n          Find the cluster you want to terminate from the list of active clusters.\\n          Click on the down-arrow next to the cluster name to open the cluster details.\\n          Click on the \"Terminate\" button…………………………………..\\n          A confirmation dialog will appear. Click \"Terminate\" again to confirm the action.\\n  - Score 3: the answer is correct and reader friendly, no obvious piece that affect readability.\\n- Then final rating:\\n    - Ratio: 60% correctness + 20% comprehensiveness + 20% readability\\nFrom this experiment, we learned several things:\\n ■ Using the Few Shots prompt with GPT-4 didn’t make an obvious difference in the consistency of results. \\nWhen we included the detailed grading rubric with examples we didn’t see a noticeable improvement in \\nGPT-4’s grading results across different LLM models. Interestingly, it caused a slight variance in the range \\nof the scores. \\n90'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 90, 'page_label': '91'}, page_content='THE BIG BOOK OF GENERATIVE AI\\n91'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 91, 'page_label': '92'}, page_content='THE BIG BOOK OF GENERATIVE AI\\n ■ Including few examples for GPT-3.5-turbo-16k significantly improves the consistency of the scores,  \\nand makes the result usable. Including detailed grading rubric/examples has very obvious improvement \\non the grading result from GPT-3.5. Though the actual average score value is slightly different between \\nGPT-4 and GPT-3.5 (score 3.0 vs score 2.6), the ranking and precision remains fairly consistent\\n ■ On the contrary, using GPT-3.5 without a grading rubric gets very inconsistent results and is  \\ncompletely unusable\\n ■ Note that we are using GPT-3.5-turbo-16k instead of GPT-3.5-turbo since the prompt can be larger than \\n4k tokens. \\n92'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 92, 'page_label': '93'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nEXPERIMENT 3: APPROPRIATE GRADE SCALES\\nThe LLM-as-judge paper uses a non-integer 0~10 scale (i.e., float) for the grading scale; in other words, it uses \\na high precision rubric for the final score. We found these high-precision scales cause issues downstream with \\nthe following:\\n ■ Consistency: Evaluators–both human and LLM–struggled to hold the same standard for the same score \\nwhen grading on high precision. As a result, we found that output scores are less consistent across judges \\nif you move from low-precision to high-precision scales. \\n ■ Explainability: Additionally, if we want to cross-validate the LLM-judged results with human-judged \\nresults we must provide instructions on how to grade answers. It is very difficult to provide accurate \\ninstructions for each “score” in a high-precision grading scale–for example, what’s a good example for an \\nanswer that’s scored at 5.1 as compared to 5.6? \\n93'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 93, 'page_label': '94'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nWe experimented with various low-precision grading scales to provide guidance on the “best” one to use, \\nultimately we recommend an integer scale of 0-3 or 0-4 (if you want to stick to the Likert scale). We tried  \\n0-10, 1-5, 0-3, and 0-1 and learned:\\n ■ Binary grading works for simple metrics like “usability” or “good/bad”.\\n ■ Scales like 0-10 are difficult to come up with distinguishing criteria between all scores.\\n94'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 94, 'page_label': '95'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nAs shown in these plots, both GPT-4 and GPT-3.5 can retain consistent ranking of results using different  \\nlow-precision grading scales, thus using a lower grading scale like 0~3 or 1~5 can balance the precision  \\nwith explainability).\\nThus, we recommend 0-3 or 1-5 as a grading scale to make it easier to align with human labels, reason about \\nscoring criteria, and provide examples for each score in the range. \\n95'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 95, 'page_label': '96'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nEXPERIMENT 4: APPLICABILITY ACROSS USE CASES\\nThe LLM-as-judge paper shows that both LLM and human judgment ranks the Vicuna-13B model as a close \\ncompetitor to GPT-3.5:\\nHowever, when we benchmarked the set of models for our document Q&A use cases, we found that even the \\nmuch larger Vicuna-33B model has a noticeably worse performance than GPT-3.5 when answering questions \\nbased on context. These findings are also verified by GPT-4, GPT-3.5 and human judges (as mentioned in \\nExperiment 1) which all agree that Vicuna-33B is performing worse than GPT-3.5.\\nFigure 4: Average win rate of nine models under different judges on Chatbot Arena.\\n96'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 96, 'page_label': '97'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nWe looked closer at the benchmark dataset proposed by the paper and found that the 3 categories of tasks \\n(writing, math, knowledge) don’t directly reflect or contribute to the model’s ability to synthesize an answer \\nbased on a context. Instead, intuitively, document Q&A use cases need benchmarks on reading comprehension \\nand instruction following. Thus evaluation results can’t be transferred between use cases and we need to build \\nuse-case-specific benchmarks in order to properly evaluate how good a model can meet customer needs.\\n97'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 97, 'page_label': '98'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nUSE MLFLOW TO LEVERAGE OUR BEST PRACTICES\\nWith the experiments above, we explored how different factors can significantly affect the evaluation of a \\nchatbot and confirmed that LLM as a judge can largely reflect human preferences for the document Q&A use \\ncase. At Databricks, we are evolving the MLflow Evaluation API to help your team effectively evaluate your LLM \\napplications based on these findings. MLflow 2.4 introduced the Evaluation API for LLMs to compare various \\nmodels’ text output side-by-side, MLflow 2.6 introduced LLM-based metrics for evaluation like toxicity and \\nperplexity, and we’re working to support LLM-as-a-judge in the near future!\\nIn the meantime, we compiled the list of resources we referenced in our research below:\\n ■ Doc_qa repository\\n ■ The code and data we used to conduct the experiments\\n ■ LLM-as-Judge Research paper from lmsys group \\n ■ The paper is the first research for using LLM as judge for the casual chat use cases, it extensively \\nexplored the feasibility and pros and cons of using LLM (GPT-4, ClaudeV1, GPT-3.5) as the judge for \\ntasks in writing, math, world knowledge\\nOffline LLM Evaluation: Step-by-Step GenAI Application Assessment on Databricks\\nby Abe Omorogbe, Liang Zhang, Sunish Sheth, Corey Zumar, Maheswaran Venkatachalam, Emil Lysgaard  \\nand Mathias Christiansen\\nBACKGROUND\\nIn an era where retrieval augmented generation (RAG) is revolutionizing the way we interact with AI-driven \\napplications, ensuring the efficiency and effectiveness of these systems has never been more essential. \\nDatabricks and MLflow are at the forefront of this innovation, offering streamlined solutions for the critical \\nevaluation of GenAI applications. \\nThis blog post guides you through the simple and effective process of leveraging the Databricks Data \\nIntelligence Platform to enhance and evaluate the quality of the three core components of your GenAI \\napplications: Prompts, Retrieval System, and Foundation LLM, ensuring that your GenAI applications continue  \\nto generate accurate results.\\n98'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 98, 'page_label': '99'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nUSE CASE\\nWe are going to be creating a QA chatbot that will answer questions from the MLflow documentation and then \\nevaluate the results.\\n99'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 99, 'page_label': '100'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nSET UP EXTERNAL MODELS IN DATABRICKS\\nDatabricks Model Serving feature can be used to manage, govern, and access external models from various \\nlarge language model (LLM) providers, such as Azure OpenAI GPT, Anthropic Claude, or AWS Bedrock, within \\nan organization. It offers a high-level interface that simplifies the interaction with these services by providing a \\nunified endpoint to handle specific LLM related requests.\\nMajor advantages of using Model Serving:\\n ■ Query Models Through a Unified Interface:  Simplifies the interface to call multiple LLMs in your \\norganization. Query models through a unified OpenAI-compatible API and SDK and manage all models \\nthrough a single UI.\\n ■ Govern and Manage Models: Centralizes endpoint management of multiple LLMs in your organization.  \\nThis includes the ability to manage permissions and track usage limits.\\n ■ Central Key Management: Centralizes API key management in a secure location, which enhances \\norganizational security by minimizing key exposure in the system and code, and reduces the burden  \\non end-users.\\n100'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 100, 'page_label': '101'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nCREATE A SERVING ENDPOINT WITH AN EXTERNAL MODEL IN DATABRICKS\\n \\n1\\n2\\n3 \\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n23\\n25\\n26\\nimport mlflow\\nimport mlflow.deployments\\nclient = mlflow.deployments.get_deploy_client(\"databricks\")\\nendpoint_name = f\"test-endpoint-{uuid.uuid4()}\"\\nclient.create_endpoint(\\nname=endpoint_name,\\nconfig={\\n        \"served_entities\": [\\n            {\\n                \"name\": \"test\",\\n                \"external_model\": {\\n                    \"name\": \"gpt-3.5-turbo-instruct\",\\n                    \"provider\": \"openai\",\\n                    \"task\": \"llm/v1/completions\",\\n                    \"openai_config\": {\\n                        \"openai_api_type\": \"azure\",\\n                        \"openai_api_key\": \"{{secrets/<your-scope-name>/<your-key-name>}}\", ## Use Databricks Secrets. \\n                        \"openai_api_base\": \"https://<your-endpoint>.openai.azure.com/\",\\n                        \"openai_deployment_name\": \"<your-deployment-name>\",\\n                        \"openai_api_version\": \"2023-05-15\",\\n                    },\\n                },\\n            }\\n        ],\\n     },\\n)\\n101'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 101, 'page_label': '102'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nEXPLORE PROMPTS WITH THE DATABRICKS AI PLAYGROUND\\nIn this section, we will understand: How well do different prompts perform with the chosen LLM?\\nWe recently introduced the Databricks AI Playground, which provides a best-in-class experience for crafting the \\nperfect prompt. With no code required, you can try out multiple LLMs served as Endpoints in Databricks, and \\ntest different parameters and prompts.\\nMajor advantages of the Databricks AI Playground are:\\n ■ Quick Testing: Quickly test deployed models directly in Databricks.\\n ■ Easy Comparison: Central location to compare multiple models on different prompts and parameters for \\ncomparison and selection.\\nUSING DATABRICKS AI PLAYGROUND\\nWe delve into testing relevant prompts with OpenAI GPT 3.5 Turbo, leveraging the Databricks AI Playground. \\n102'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 102, 'page_label': '103'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nCOMPARING DIFFERENT PROMPTS AND PARAMETERS\\nIn the Playground, you are able to compare the output of multiple prompts to see which gives better results. \\nDirectly in the Playground, you can try several prompts,  models, and parameters to figure out which \\ncombination provides the best results. The model and parameters combo can then be added to the GenAI  \\napp and used for answer generation with the right context.\\n103'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 103, 'page_label': '104'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nADDING MODEL AND PARAMETERS TO YOUR GENAI APPLICATION\\nAfter playing with a few prompts and parameters, you can use the same settings and model in your  \\nGenAI application.\\n104'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 104, 'page_label': '105'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nExample of how to import the same external model in LangChain. We will cover how we turn this into a GenAI \\nPOC in the next section.\\nCREATE GENAI POC WITH LANGCHAIN AND LOG WITH MLFLOW\\nNow that we have found a good model and prompt parameters for your use case, we are going to create a \\nsample GenAI app that is a QA chatbot that will answer questions from the MLflow documentation using a \\nvector database, embedding model with the Databricks Foundation Model API and Azure OpenAI GPT 3.5 as \\nthe generation model.\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\nfrom langchain.llms import Databricks\\nllm = Databricks(\\n    endpoint_name=\"<endpoint-name>\",\\n    extra_params={\"temperature\": 0.1,\\n                 \"top_p\": 0.1,\\n                 \"max_tokens\": 500,\\n                 } #parameters used in AI Playground\\n)\\n105'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 105, 'page_label': '106'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nCREATE A SAMPLE GENAI APP WITH LANGCHAIN USING DOCS FROM THE MLFLOW WEBSITE\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\n27\\n28\\n29\\n30\\n31\\n32\\n33\\n34\\n35\\n36\\n37\\nimport os\\nimport pandas as pd\\nimport mlflow\\nimport chromadb\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.document_loaders import WebBaseLoader\\nfrom langchain.llms import Databricks\\nfrom langchain.embeddings.databricks import DatabricksEmbeddings\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\\nloader = WebBaseLoader(\\n    [ \\n     \"https://mlflow.org/docs/latest/index.html\",\\n     \"https://mlflow.org/docs/latest/tracking/autolog.html\", \\n     \"https://mlflow.org/docs/latest/getting-started/tracking-server-overview/index.html\",\\n     \"https://mlflow.org/docs/latest/python_api/mlflow.deployments.html\" ])\\ndocuments = loader.load()\\nCHUNK_SIZE = 1000\\ntext_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=0)\\ntexts = text_splitter.split_documents(documents)\\nllm = Databricks(\\n    endpoint_name=\"<endpoint-name>\",\\n    extra_params={\"temperature\": 0.1,\\n                 \"top_p\": 0.1,\\n                 \"max_tokens\": 500,\\n                 } #parameters used in AI Playground\\n)\\n# create the embedding function using Databricks Foundation Model APIs\\nembedding_function = DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\")\\ndocsearch = Chroma.from_documents(texts, embedding_function)\\nqa = RetrievalQA.from_chain_type(\\n    llm=llm,\\n    chain_type=\"stuff\",\\n    retriever=docsearch.as_retriever(fetch_k=3),\\n    return_source_documents=True,\\n)\\n106'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 106, 'page_label': '107'}, page_content=\"THE BIG BOOK OF GENERATIVE AI\\nFor customers wanting to scale the retriever used in their GenAI application, we advise using Databricks Vector \\nSearch, a serverless similarity search engine that allows you to store a vector representation of your data, \\nincluding metadata, in a vector database.\\nEVALUATION OF RETRIEVAL SYSTEM WITH MLFLOW\\nIn this section, we will understand: How well does the retriever work with a given query?\\nIn MLflow 2.9.1, Evaluation for retrievers was introduced and provides a way for you to assess the efficiency \\nof their retriever with the MLflow evaluate API. You can use this API to evaluate the effectiveness of your \\nembedding model, the top K threshold choice, or the chunking strategy.\\nCREATING A GROUND TRUTH DATASET\\nCurating a ground truth dataset for evaluating your GenAI often involves the meticulous task of manually \\nannotating test sets, a process that demands both time and domain expertise. In this blog, we’re taking a \\ndifferent route. We’re leveraging the power of an LLM to generate synthetic data for testing, offering a quick-\\nstart approach to get a sense of your GenAI app’s retrieval capability, and a warm-up for all the in-depth \\nevaluation work that may follow. To our readers and customers, we emphasize the importance of crafting a \\ndataset that mirrors the expected inputs and outputs of your GenAI application. It’s a journey worth taking for \\nthe incredible insights you’ll gain!\\nYou can explore with the full dataset but let's demo with a subset of the generated data. The question column \\ncontains all the questions that will be evaluated and the source column is the expected source for the answer \\nfor the questions as an ordered list of strings.\\n107\"),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 107, 'page_label': '108'}, page_content='THE BIG BOOK OF GENERATIVE AI\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\neval_data = pd.DataFrame(\\n    {\\n        \"question\": [\\n            \"What is MLflow?\",\\n            \"What is Databricks?\",\\n            \"How to serve a model on Databricks?\",\\n            \"How to enable MLflow Autologging for my workspace by default?\",\\n        ],\\n        \"source\": [\\n            [\"https://mlflow.org/docs/latest/index.html\"],\\n            [\"https://mlflow.org/docs/latest/getting-started/tracking-server-overview/index.html\"],\\n            [\"https://mlflow.org/docs/latest/python_api/mlflow.deployments.html\"],\\n            [\"https://mlflow.org/docs/latest/tracking/autolog.html\"],\\n        ],\\n    }\\n)\\nEVALUATE THE EMBEDDING MODEL WITH MLFLOW\\nThe quality of your embedding model is pivotal for accurate retrieval. In MLflow 2.9.0, we introduced three built-\\nin metrics mlflow.metrics.precision_at_k(k), mlflow.metrics.recall_at_k(k) and mlflow.metrics.ndcg_at_k(k) \\nto help determine how effective your retriever is at predicting the most relevant results for you. For example; \\nSuppose the vector database returns 10 results (k=10), and out of these 10 results, 4 are relevant to your query. \\nThe precision_at_10 would be 4/10 or 40%. \\n108'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 108, 'page_label': '109'}, page_content='THE BIG BOOK OF GENERATIVE AI\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\ndef evaluate_embedding(embedding_function):\\n    CHUNK_SIZE = 1000\\n    list_of_documents = loader.load()\\n    text_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=0)\\n    docs = text_splitter.split_documents(list_of_documents)\\n    retriever = Chroma.from_documents(docs, embedding_function).as_retriever()\\n    def retrieve_doc_ids(question: str) -> List[str]:\\n        docs = retriever.get_relevant_documents(question)\\n        doc_ids = [doc.metadata[\"source\"] for doc in docs]\\n        return doc_ids\\n    def retriever_model_function(question_df: pd.DataFrame) -> pd.Series:\\n        return question_df[\"question\"].apply(retrieve_doc_ids)\\n    with mlflow.start_run() as run:\\n        evaluate_results = mlflow.evaluate(\\n                model=retriever_model_function,\\n                data=eval_data,\\n                model_type=\"retriever\",\\n                targets=\"source\",\\n                evaluators=\"default\",\\n            )\\n    return evaluate_results\\nresult1 = evaluate_embedding(DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\"))result2 = evaluate_embed-\\nding(<another-embedding-function>)\\neval_results_of_retriever_df_bge = result1.tables[\"eval_results_table\"]\\ndisplay(eval_results_of_retriever_df_bge)\\n109'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 109, 'page_label': '110'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nThe evaluation will return a table with the results of your evaluation for each question. i.e., for this test, we can \\nsee that the retriever seems to performing great for the questions \"How to enable MLflow Autologging for my \\nworkspace by default?” with a Precision @ K score is 1, and is not retrieving any of the right documentation for \\nthe questions \"What is MLflow?” since the precision @ K score is 0. With this insight, we can debug the retriever \\nand improve the retriever for questions like “What is MLflow?”\\nEvaluation results when using databricks-bge-large-en embedding model\\nEVALUATE RETRIEVER WITH DIFFERENT TOP K VALUES WITH MLFLOW\\nYou can quickly calculate the metrics for different Ks by specifying the extra_metrics argument.\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\nwith mlflow.start_run() as run:\\n        evaluate_results = mlflow.evaluate(\\n        data=eval_results_of_retriever_df_bge,\\n        targets=\"source\",\\n        predictions=\"outputs\",\\n        evaluators=\"default\",\\n        extra_metrics=[\\n            mlflow.metrics.precision_at_k(1),\\n            mlflow.metrics.precision_at_k(2),\\n            mlflow.metrics.precision_at_k(3),\\n            mlflow.metrics.recall_at_k(1),\\n            mlflow.metrics.recall_at_k(2),\\n            mlflow.metrics.recall_at_k(3),\\n            mlflow.metrics.ndcg_at_k(1),\\n            mlflow.metrics.ndcg_at_k(2),\\n            mlflow.metrics.ndcg_at_k(3),\\n        ],\\n    )\\ndisplay(evaluate_results.tables[\"eval_results_table\"])\\n110'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 110, 'page_label': '111'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nThe evaluation will return a table with the results of your evaluation for each question, and you can better \\nunderstand which K value to use when retrieving documents. i.e., for this test we can see changing the top K \\nvalue can positively affect the precision of the retriever for questions like “What is Databricks?”\\nEvaluation result with all precision at K values\\n111'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 111, 'page_label': '112'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nEVALUATE THE CHUNKING STRATEGY WITH MLFLOW\\nThe effectiveness of your chunking strategy is critical. We explore how MLflow can assist in this evaluation, \\nfocusing on the retrieval model type and its impact on overall performance.\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\ndef evaluate_chunk_size(chunk_size):\\n  list_of_documents = loader.load()\\n  text_splitter = CharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=0)\\n  docs = text_splitter.split_documents(list_of_documents)\\n  embedding_function = DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\")\\n  retriever = Chroma.from_documents(docs, embedding_function).as_retriever()\\n  \\n  def retrieve_doc_ids(question: str) -> List[str]:\\n    docs = retriever.get_relevant_documents(question)\\n    doc_ids = [doc.metadata[\"source\"] for doc in docs]\\n    return doc_ids\\n   \\n  def retriever_model_function(question_df: pd.DataFrame) -> pd.Series:\\n    return question_df[\"question\"].apply(retrieve_doc_ids)\\n  with mlflow.start_run() as run:\\n      evaluate_results = mlflow.evaluate(\\n          model=retriever_model_function,\\n          data=eval_data,\\n          model_type=\"retriever\",\\n          targets=\"source\",\\n          evaluators=\"default\",\\n      )\\n  return evaluate_results\\nresult1 = evaluate_chunk_size(500)\\nresult2 = evaluate_chunk_size(2000)\\ndisplay(result1.tables[\"eval_results_table\"])\\ndisplay(result2.tables[\"eval_results_table\"])\\n112'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 112, 'page_label': '113'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nThe evaluation will return 2 tables with the results of your evaluation for each question using 2 different chunk \\nsizes, and you can better understand which chunk size to use when retrieving documents (i.e., for this example, \\nit seems like changing the chunk size did not affect any metric).\\nEvaluation result with Chunk size of 1000\\nEvaluation result with Chunk size of 2000\\nCheck out the in-depth notebook on retrieval evaluation\\n113'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 113, 'page_label': '114'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nEVALUATION OF GENAI RESUL TS WITH MLFLOW\\nIn this section, we will understand: How good is the response of the GenAI app with a given prompt and context?\\nAssessing the quality of generated responses is key. We will augment the manual process of evaluating with \\nquestions and answers by leveraging MLflow\\'s QA metrics, and comparing them against a GPT-4 model as a \\nbenchmark to understand the effectiveness of the generated answers. \\nUsing an LLM like GPT-4 as a judge to assist in evaluation can offer several benefits, here are some key benefits:\\n ■ Rapid and Scalable Experimentation:  In many situations, we think LLM judges represent a sweet-spot: \\nthey can evaluate unstructured outputs (like a response from a chat-bot) automatically, rapidly, and  \\nat low-cost.  \\n ■ Cost-Effective: By automating some evaluations with LLMs, we consider it a worthy companion to human \\nevaluation, which is slower and more expensive but represents the gold standard of model evaluation.\\nUSE MLFLOW EVALUATE AND LLM AS A JUDGE\\nWe take some sample questions and use the LLM as a judge, and inspect the results with MLflow, providing a \\ncomprehensive analysis of the outcome with built-in metrics. We are going to judge the GenAI app on relevance \\n(how relevant is the output with respect to both the input and the context).\\nCreate a simple function that runs each input through the chain\\n \\n1\\n2\\ndef model(input_df):\\n    return input_df[\"questions\"].map(qa).tolist()\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\neval_df = pd.DataFrame(\\n    {\\n        \"questions\": [\\n            \"What is MLflow?\",\\n            \"What is Databricks?\",\\n            \"How to serve a model on Databricks?\",\\n            \"How to enable MLflow Autologging for my workspace by default?\",\\n        ],\\n    }\\n)\\n114'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 114, 'page_label': '115'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nUse relevance metric to determine the relevance of the answer and context. There are other metrics you can \\nuse too.\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\nfrom mlflow.deployments import set_deployments_target\\nfrom  mlflow.metrics.genai.metric_definitions import relevance\\nset_deployments_target(\"databricks\") #To retrieve all endpoint in your Databricks Workspace\\nrelevance_metric = relevance(model=f\"endpoints:/{endpoint_name}\") #You can also use any model you have hosted on Da-\\ntabricks, models from the Marketplace or models in the Foundation model API\\nwith mlflow.start_run():\\n    results =  mlflow.evaluate(\\n        model,\\n        eval_df,\\n        model_type=\"question-answering\",\\n        evaluators=\"default\",\\n        predictions=\"result\",\\n        extra_metrics=[relevance_metric, mlflow.metrics.latency()],\\n        evaluator_config={\\n            \"col_mapping\": {\\n                \"inputs\": \"questions\",\\n                \"context\": \"source_documents\",\\n            }\\n        }\\n    )\\n    print(results.metrics)\\n115'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 115, 'page_label': '116'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nIn your Databricks Workspace, you can compare and evaluate all your inputs and outputs, as well as the source \\ndocuments, relevance and any other metrics you added to your evaluation function.\\nCheck out more in depth notebooks on LLM evaluation\\n116'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 116, 'page_label': '117'}, page_content='THE BIG BOOK OF GENERATIVE AI\\nSummary Whether you’re looking to disrupt traditional industries, enhance creative endeavors or solve complex problems \\nin novel ways, the potential applications of generative AI are limited only by your imagination and willingness to \\nexperiment. Remember, every significant advancement in this field began with a simple idea and the courage to \\nexplore it further.\\nFor those seeking more knowledge or simply curious about the latest developments in the realm of generative \\nAI, we’ve provided some resources on training, demos and product information. \\nGenAI Training\\nGenerative AI Engineer Learning Pathway: Take self-paced, on-demand and instructor-led courses on \\ngenerative AI\\nFree LLM Course (edX): In-depth course to learn GenAI and LLMs inside and out\\nGenAI Webinar: Learn how to take control of your GenAI app performance, privacy and cost, and drive value \\nwith generative AI\\nAdditional Resources\\nBig Book of MLOps: A deep dive into the architectures and technologies behind MLOps — including LLMs  \\nand GenAI \\nMosaic AI: Product page covering the features of Mosaic AI within Databricks\\n117'),\n",
              " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-06-26T12:00:09+00:00', 'source': '/content/genai pdf.pdf', 'total_pages': 118, 'page': 117, 'page_label': '118'}, page_content='Build Production-Quality GenAI Applications — See How\\nCreate high-quality generative AI applications and ensure your output is accurate, \\ngoverned and safe. See why over 10,000 organizations worldwide rely on Databricks for \\nall their workloads from BI to AI — test-drive the full Databricks Platform free for 14 days.\\nAbout Databricks\\nDatabricks is the data and AI company. More than 10,000 organizations worldwide — \\nincluding Comcast, Condé Nast, Grammarly and over 50% of the Fortune 500 — rely on \\nthe Databricks Data Intelligence Platform to unify and democratize data, analytics and \\nAI. Databricks is headquartered in San Francisco, with offices around the globe, and was \\nfounded by the original creators of Lakehouse, Apache Spark™, Delta Lake and MLflow.  \\nTo learn more, follow Databricks on LinkedIn, X and Facebook.\\nTry Databricks free Take Generative AI Fundamentals On-Demand Training\\n© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark and the Spark \\nlogo are trademarks of the Apache Software Foundation . Privacy Policy  | Terms of Use')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYabWJKWnfPa",
        "outputId": "808dda1f-2fe8-4793-f5c4-a2f78fa7cf30"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "118"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Question_gen=\" \"\n",
        "for page in data:\n",
        "  Question_gen+=page.page_content"
      ],
      "metadata": {
        "id": "dS3JeDDynifS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Question_gen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "TEkKVfCjwezD",
        "outputId": "29c2951d-4e1f-462a-bbed-7fd45461f354"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' THE BIG BOOK OF GENERATIVE AI\\nCONTENTS\\nIntroduction  ......................................................................................................................................................................................................... 3\\nThe Path to Deploying Production-Quality GenAI Applications ........................................................................................... 5 \\nStage 0: Foundation Models  ........................................................................................................................................................................................................................................................... 5 \\n Use Case: Introducing DBRX: A New State-of-the-Art Open LLM  ................................................................................................................................................................... 5 \\nStage 1: Prompt Engineering  ......................................................................................................................................................................................................................................................... 19 \\n Use Case: Automated Analysis of Product Reviews Using Large Language Models ........................................................................................................................ 20 \\nStage 2: Retrieval Augmented Generation  ....................................................................................................................................................................................................................... 25 \\n Use Case: Improve Your RAG Application Response Quality With Real-Time Structured Data  ................................................................................................ 27 \\nStage 3: Fine-Tuning a Foundation Model  ......................................................................................................................................................................................................................... 33 \\n Use Case: Creating a Bespoke LLM for AI-Generated Documentation  ..................................................................................................................................................... 34 \\n Use Case: Efficient Fine-Tuning With LoRA: A Guide to Optimal Parameter Selection for Large Language Models  ................................................... 43 \\nStage 4: Pretraining  ........................................................................................................................................................................................................................................................................... 60 \\n Use Case: Training Stable Diffusion From Scratch for <$50K With MosaicML  ..................................................................................................................................... 62 \\n Use Case: Deep Dive: How We Trained Stable Diffusion for Less Than $50K  ........................................................................................................................................ 68 \\nStage 5: LLM Evaluation  .................................................................................................................................................................................................................................................................... 81 \\n Use Case : Best Practices for LLM Evaluation of RAG Application  ................................................................................................................................................................. 82 \\n Use Case: Offline LLM Evaluation: Step-by-Step GenAI Application Assessment on Databricks ........................................................................................... 98\\nSummary  ............................................................................................................................................................................................................. 117 \\nGenAI Training  ......................................................................................................................................................................................................................................................................................... 117 \\nAdditional Resources  ........................................................................................................................................................................................................................................................................ 117\\n2THE BIG BOOK OF GENERATIVE AI\\nAchieving Production-Quality GenAI Requires New Tools and Skills\\nGenerative AI has opened new worlds of possibilities for businesses and is being emphatically embraced \\nacross organizations. According to a recent MIT Tech Review report, all 600 CIOs surveyed stated they are \\nincreasing their investment in AI, and 71% are planning to build their own custom large language models (LLMs) \\nor other GenAI models. However, many organizations have found it challenging to deploy these applications at \\nproduction quality. To meet the standard of quality required for customer-facing applications, AI output must \\nbe accurate, governed and safe. \\nData Infrastructure Must Evolve to Support  \\nGenAI-Powered Applications\\nMaking the leap to generative AI is not just about deploying a chatbot; it requires a reshaping of the foundational \\naspects of data management. Central to this transformation is the emergence of data lakehouses as the new \\n“modern data stack.” These advanced data architectures are essential to harnessing the full potential of GenAI, \\ndriving faster, more cost-effective and wider democratization of data and AI technologies. As businesses \\nincreasingly rely on GenAI-powered tools and applications for competitive advantage, the underlying data \\ninfrastructure must evolve to support these advanced technologies effectively and securely.\\nNo Matter Where You Are on Your Path to Deploying GenAI Applications, \\nthe Quality of Your Data Matters\\nBusinesses need to achieve production quality with their GenAI applications. Developers need rich tools for \\nunderstanding the quality of their data and model outputs, along with an underlying platform that lets them \\ncombine and optimize all aspects of the GenAI process. GenAI has many components such as data preparation, \\nretrieval models, language models (either SaaS or open source), ranking and post-processing pipelines, prompt \\nengineering, and training models on custom enterprise data.\\nTo help you overcome common enterprise challenges with building GenAI, we’ve compiled a collection of \\ntechnical content and code samples. We’ll start each section with a brief overview and then provide use cases \\nand example code for reference. \\nIntroduction\\n3THE BIG BOOK OF GENERATIVE AI\\nIn this eBook, you’ll learn: \\n ■ How to plan a path from basic to advanced GenAI applications, leveraging your organization’s data\\n ■ How to use retrieval augmented generation (RAG) to make an off-the-shelf AI system smarter\\n ■ How to evaluate LLMs and where you want to invest in more powerful AI tools and systems that drive \\nmore significant operational gain\\n ■ How to build a custom LLM that may be better, faster and cheaper for your organization\\n ■ When it might be worth it to pretrain your own model — and more\\nUse cases for GenAI covered:\\n ■ How to use LLMs to gain actionable insights from product reviews\\n ■ How to use RAG for a chatbot to improve the quality of output\\n ■ How to train your own generative AI model in a cost-effective manner\\n ■ How to monitor and evaluate your deployed LLMs and GenAI applications\\n4THE BIG BOOK OF GENERATIVE AI\\nThe Path to Deploying \\nProduction-Quality \\nGenAI Applications\\nStage 0: Foundation Models\\nBefore setting off to create production-quality GenAI applications, we need to cover the base language models \\nthat serve as the foundation for layers of increasingly complex techniques. Foundation models commonly refer \\nto large language models that have been trained over extensive datasets to be generally good at some task \\n(chat, instruction following, code generation, etc.).\\nWe won’t cover many models, as it is a constantly shifting landscape, but it is important to note that while \\nunderlying architectures may differ drastically, foundation models generally fall under two categories: \\nproprietary (such as GPT-3.5 and Gemini) and open source (such as Llama2-70B and DBRX). The main difference \\nbetween the two is that while proprietary models historically have an edge on outright performance, users have \\nto send their data out to a third party and don’t have control over the underlying model as they’re often being \\nupdated and changed. \\nOpen source models, on the other hand, offer users full control over the model and the ability to run it on their \\nown terms with their own governance and data privacy. Here’s a current list of many open source GenAI models \\nacross different domains that are all free for commercial use. Databricks has also created their own state-of-\\nthe-art open source foundation model so users can build the highest-quality production GenAI applications.\\nFoundation Model Use Case\\nINTRODUCING DBRX: A NEW STATE-OF-THE-ART OPEN LLM\\nWe are excited to introduce DBRX, an open, general-purpose LLM created by Databricks. Across a range of \\nstandard benchmarks, DBRX sets a new state-of-the-art for established open LLMs. Moreover, it provides \\nthe open community and enterprises building their own LLMs with capabilities that were previously limited \\nto closed model APIs; according to our measurements, it surpasses GPT-3.5, and it is competitive with \\nGemini 1.0 Pro. It is an especially capable code model, surpassing specialized models like CodeLLaMA-70B on \\nprogramming, in addition to its strength as a general-purpose LLM.\\n5THE BIG BOOK OF GENERATIVE AI\\nThis state-of-the-art quality comes with marked improvements in training and inference performance. DBRX \\nadvances the state-of-the-art in efficiency among open models thanks to its fine-grained mixture-of-experts \\n(MoE) architecture. Inference is up to 2x faster than LLaMA2-70B, and DBRX is about 40% of the size of Grok-1 in \\nterms of both total and active parameter-counts. When hosted on Mosaic AI Model Serving, DBRX can generate \\ntext at up to 150 tok/s/user. Our customers will find that training MoEs is also about 2x more FLOP-efficient \\nthan training dense models for the same final model quality. End-to-end, our overall recipe for DBRX (including \\nthe pretraining data, model architecture, and optimization strategy) can match the quality of our previous-\\ngeneration MPT models with nearly 4x less compute.\\nFigure 1: DBRX outperforms established open source models on language understanding (MMLU), \\nProgramming (HumanEval), and Math (GSM8K).\\n6THE BIG BOOK OF GENERATIVE AI\\nThe weights of the base model (DBRX Base) and the fine-tuned model (DBRX Instruct) are available on Hugging \\nFace under an open license. Starting today, DBRX is available for Databricks customers to use via APIs, and \\nDatabricks customers can pretrain their own DBRX-class models from scratch or continue training on top of  \\none of our checkpoints using the same tools and science we used to build it. DBRX is already being integrated \\ninto our GenAI-powered products, where — in applications like SQL — early rollouts have surpassed GPT-3.5 \\nTurbo and are challenging GPT-4 Turbo. It is also a leading model among open models and GPT-3.5 Turbo on \\nRAG tasks.\\nTraining mixture-of-experts models is hard. We had to overcome a variety of scientific and performance \\nchallenges to build a pipeline robust enough to repeatedly train DBRX-class models in an efficient manner. Now \\nthat we have done so, we have a one-of-a-kind training stack that allows any enterprise to train world-class MoE \\nfoundation models from scratch. We look forward to sharing that capability with our customers and sharing our \\nlessons learned with the community.\\nDownload DBRX today from Hugging Face (DBRX Base, DBRX Instruct), or try out DBRX Instruct in our HF Space, \\nor see our model repository on github: databricks/dbrx.\\nWhat Is DBRX?\\nDBRX is a transformer-based decoder-only large language model (LLM) that was trained using next-token \\nprediction. It uses a fine-grained mixture-of-experts (MoE) architecture with 132B total parameters of which \\n36B parameters are active on any input. It was pre-trained on 12T tokens of text and code data. Compared \\nto other open MoE models like Mixtral and Grok-1, DBRX is fine-grained, meaning it uses a larger number of \\nsmaller experts. DBRX has 16 experts and chooses 4, while Mixtral and Grok-1 have 8 experts and choose 2. This \\nprovides 65x more possible combinations of experts and we found that this improves model quality. DBRX uses \\nrotary position encodings (RoPE), gated linear units (GLU), and grouped query attention (GQA). It uses the GPT-\\n4 tokenizer as provided in the tiktoken repository. We made these choices based on exhaustive evaluation and \\nscaling experiments.\\n7THE BIG BOOK OF GENERATIVE AI\\nDBRX was pretrained on 12T tokens of carefully curated data and a maximum context length of 32k tokens. We \\nestimate that this data is at least 2x better token-for-token than the data we used to pretrain the MPT family of \\nmodels. This new dataset was developed using the full suite of Databricks tools, including Apache Spark™ and \\nDatabricks notebooks for data processing, Unity Catalog for data management and governance, and MLflow for \\nexperiment tracking. We used curriculum learning for pretraining, changing the data mix during training in ways \\nwe found to substantially improve model quality.\\nQuality on Benchmarks vs. Leading Open Models\\nTable 1 shows the quality of DBRX Instruct and leading established, open models. DBRX Instruct is the leading \\nmodel on composite benchmarks, programming and mathematics benchmarks, and MMLU. It surpasses all chat \\nor instruction fine-tuned models on standard benchmarks.\\nComposite benchmarks. We evaluated DBRX Instruct and peers on two composite benchmarks: the Hugging \\nFace Open LLM Leaderboard (the average of ARC-Challenge, HellaSwag, MMLU, TruthfulQA, WinoGrande,  \\nand GSM8k) and the Databricks Model Gauntlet (a suite of over 30 tasks spanning six categories: world \\nknowledge, commonsense reasoning, language understanding, reading comprehension, symbolic problem \\nsolving, and programming).\\nAmong the models we evaluated, DBRX Instruct scores the highest on two composite benchmarks: the Hugging \\nFace Open LLM Leaderboard (74.5% vs. 72.7% for the next highest model, Mixtral Instruct) and the Databricks \\nGauntlet (66.8% vs. 60.7% for the next highest model, Mixtral Instruct).\\nProgramming and mathematics. DBRX Instruct is especially strong at programming and mathematics. It scores \\nhigher than the other open models we evaluated on HumanEval (70.1% vs. 63.2% for Grok-1, 54.8% for Mixtral \\nInstruct, and 32.2% for the best-performing LLaMA2-70B variant) and GSM8k (66.9% vs. 62.9% for Grok-1, 61.1% \\nfor Mixtral Instruct, and 54.1% for the best-performing LLaMA2-70B variant). DBRX outperforms Grok-1, the next \\nbest model on these benchmarks, despite the fact that Grok-1 has 2.4x as many parameters. On HumanEval, \\nDBRX Instruct even surpasses CodeLLaMA-70B Instruct, a model built explicitly for programming, despite the \\nfact that DBRX Instruct is designed for general-purpose use (70.1% vs. 67.8% on HumanEval as reported by Meta \\nin the CodeLLaMA blog).\\nMMLU. DBRX Instruct scores higher than all other models we consider on MMLU, reaching 73.7%.\\n8THE BIG BOOK OF GENERATIVE AI\\nMODEL DBRX  \\nINSTRUCT\\nMIXTRAL \\nINSTRUCT\\nMIXTRAL \\nBASE\\nLLAMA2-70  \\nB CHAT\\nLLAMA2-70  \\nB BASE GROK-11\\nOpen LLM Leaderboard2 \\n(Avg of next 6 rows) 74.5% 72.7% 68.4% 62.4% 67.9% —\\nARC-challenge 25-shot 68.9% 70.1% 66.4% 64.6% 67.3% —\\nHellaSwag 10-shot 89.0% 87.6% 86.5% 85.9% 87.3% —\\nMMLU 5-shot 73.7% 71.4% 71.9% 63.9% 69.8% 73.0%\\nTruthful QA 0-shot 66.9% 65.0% 46.8% 52.8% 44.9% —\\nWinoGrande 5-shot 81.8% 81.1% 81.7% 80.5% 83.7% —\\nGSM8k CoT 5-shot \\nmaj@13 66.9% 61.1% 57.6% 26.7% 54.1% 62.9% \\n(8-shot)\\nGauntlet v0.34 \\n(Avg of 30+ diverse tasks) 66.8% 60.7% 56.8% 52.8% 56.4% —\\nHumanEval5 \\n0-Shot, pass@1 \\n(Programming)\\n70.1% 54.8% 40.2% 32.2% 31.0% 63.2%\\nLLaMA2-70B Base\\nTable 1: Quality of DBRX Instruct and leading open models. See footnotes for details on how numbers were collected. \\nBolded and underlined is the highest score.\\n9THE BIG BOOK OF GENERATIVE AI\\nQuality on Benchmarks vs. Leading Closed Models\\nTable 2 shows the quality of DBRX Instruct and leading closed models. According to the scores reported by \\neach model creator, DBRX Instruct surpasses GPT-3.5 (as described in the GPT-4 paper), and it is competitive \\nwith Gemini 1.0 Pro and Mistral Medium.\\nAcross nearly all benchmarks we considered, DBRX Instruct surpasses or - at worst - matches GPT-3.5. DBRX \\nInstruct outperforms GPT-3.5 on general knowledge as measured by MMLU (73.7% vs. 70.0%) and commonsense \\nreasoning as measured by HellaSwag (89.0% vs. 85.5%) and WinoGrande (81.8% vs. 81.6%). DBRX Instruct \\nespecially shines on programming and mathematical reasoning as measured by HumanEval (70.1% vs. 48.1%) and \\nGSM8k (72.8% vs. 57.1%).\\nDBRX Instruct is competitive with Gemini 1.0 Pro and Mistral Medium. Scores for DBRX Instruct are higher than \\nGemini 1.0 Pro on Inflection Corrected MTBench, MMLU, HellaSwag, and HumanEval, while Gemini 1.0 Pro is \\nstronger on GSM8k. Scores for DBRX Instruct and Mistral Medium are similar for HellaSwag, while Mistral Medium \\nis stronger on Winogrande and MMLU and DBRX Instruct is stronger on HumanEval, GSM8k, and Inflection \\nCorrected MTBench.\\n10MODEL DBRX  \\nINSTRUCT GPT-3.57 GPT-48 CLAUDE  \\n3 HAIKU\\nCLAUDE 3 \\nSONNET\\nCLAUDE 3 \\nOPUS\\nGEMINI  \\n1.0 PRO\\nGEMINI  \\n1.5 PRO\\nMISTRAL  \\nMEDIUM\\nMISTRAL \\nLARGE\\nMT Bench  \\n(Inflection corrected , n=5) 8.39 ± 0.08 — — 8.41 ± \\n0.04 \\n8.54 ± \\n0.09\\n9.03 ± \\n0.06\\n8.23 ± 0.08 — 8.05 ± 0.12 8.90 ± \\n0.06\\nMMLU 5-shot 73.7% 70.0% 86.4% 75.2% 79.0% 86.8% 71.8% 81.9% 75.3% 81.2%\\nHellaSwag 10-shot 89.0% 85.5% 95.3% 85.9% 89.0% 95.4% 84.7% 92.5% 88.0% 89.2%\\nHumanEval 0-Shot \\npass@1 \\n(Programming)\\n70.1%  \\ntemp=0, \\nN=1\\n48.1% 67.0% 75.9% 73.0% 84.9% 67.7% 71.9% 38.4% 45.1%\\nGSM8k CoT maj@1 72.8% \\n(5-shot)\\n57.1%  \\n(5-shot)\\n92.0%  \\n(5-shot) 88.9% 92.3% 95.0%\\n86.5% \\n(maj1@32)\\n91.7%  \\n(11-shot)\\n66.7%  \\n(5-shot)\\n81.0%  \\n(5-shot)\\nWinoGrande 5-shot 81.8% 81.6% 87.5% — — — — — 88.0% 86.7%\\nTable 2: Quality of DBRX Instruct and leading closed models. Other than Inflection Corrected MTBench (which we measured ourselves on model \\nendpoints), numbers were as reported by the creators of these models in their respective whitepapers. See footnotes for additional details.\\n11\\nTHE BIG BOOK OF GENERATIVE AITHE BIG BOOK OF GENERATIVE AI\\nQuality on Long-Context Tasks and RAG\\nDBRX Instruct was trained with up to a 32K token context window. Table 3 compares its performance to that of \\nMixtral Instruct and the latest versions of the GPT-3.5 Turbo and GPT-4 Turbo APIs on a suite of long-context \\nbenchmarks (KV-Pairs from the Lost in the Middle paper and HotpotQAXL, a modified version of HotPotQA that \\nextends the task to longer sequence lengths). GPT-4 Turbo is generally the best model at these tasks. However, \\nwith one exception, DBRX Instruct performs better than GPT-3.5 Turbo at all context lengths and all parts of the \\nsequence. Overall performance for DBRX Instruct and Mixtral Instruct are similar.\\nMODEL DBRX  \\nINSTRUCT\\nMIXTRAL  \\nINSTRUCT\\nGPT-3.5 TURBO \\n(API)\\nGPT-4 TURBO \\n(API)\\nAnswer in Beginning Third of Context 45.1% 41.3% 37.3%* 49.3%\\nAnswer in Middle Third of Context 45.3% 42.7% 37.3%* 49.0%\\nAnswer in Last Third of Context 48.0% 44.4% 37.0%* 50.9%\\n2K Context 59.1% 64.6% 36.3% 69.3%\\n4K Context 65.1% 59.9% 35.9% 63.5%\\n8K Context 59.5% 55.3% 45.0% 61.5%\\n16K Context 27.0% 20.1% 31.7% 26.0%\\n32K Context 19.9% 14.0% — 28.5%\\nTable 3: The average performance of models on the KV-Pairs and HotpotQAXL benchmarks. Bold is the highest score. Underlined is the highest score \\nother than GPT-4 Turbo. GPT-3.5 Turbo supports a maximum context length of 16K, so we could not evaluate it at 32K. *Averages for the beginning, \\nmiddle, and end of the sequence for GPT-3.5 Turbo include only contexts up to 16K.\\n12THE BIG BOOK OF GENERATIVE AI\\nOne of the most popular ways to leverage a model’s context is retrieval augmented generation (RAG). \\nIn RAG, content relevant to a prompt is retrieved from a database and presented alongside the prompt to \\ngive the model more information than it would otherwise have. Table 4 shows the quality of DBRX on two RAG \\nbenchmarks — Natural Questions and HotPotQA — when the model is also provided with the top 10 passages \\nretrieved from a corpus of Wikipedia articles using the embedding model bge-large-en-v1.5. DBRX Instruct  \\nis competitive with open models like Mixtral Instruct and LLaMA2-70B Chat and the current version  \\nof GPT-3.5 Turbo.\\nMODEL DBRX  \\nINSTRUCT\\nMIXTRAL  \\nINSTRUCT\\nLLAMA2-70B \\nCHAT\\nGPT 3.5 TUR -\\nBO (API)\\nGPT 4 TURBO \\n(API)\\nNatural Questions 60.0% 59.1% 56.5% 57.7% 63.9%\\nHotPotQA 55.0% 54.2% 54.7% 53.0% 62.9%\\nTable 4: The performance of the models measured when each model is given the top 10 passages retrieved from a Wikipedia corpus \\nusing bge-large-en-v1.5. Accuracy is measured by matching within the model’s answer. Bold is the highest score. Underlined is the \\nhighest score other than GPT-4 Turbo. \\nTraining Efficiency\\nModel quality must be placed in the context of how efficient the model is to train and use. This is especially \\nso at Databricks, where we build models like DBRX to establish a process for our customers to train their own \\nfoundation models.\\nWe found training mixture-of-experts models to provide substantial improvements in compute-efficiency for \\ntraining (Table 5). For example, training a smaller member of the DBRX family called DBRX MoE-B (23.5B total \\nparameters, 6.6B active parameters) required 1.7x fewer FLOPs to reach a score of 45.5% on the Databricks LLM \\nGauntlet than LLaMA2-13B required to reach 43.8%. DBRX MoE-B also contains half as many active parameters \\nas LLaMA2-13B.\\n13THE BIG BOOK OF GENERATIVE AI\\nLooking holistically, our end-to-end LLM pretraining pipeline has become nearly 4x more compute-efficient \\nin the past ten months. On May 5, 2023, we released MPT-7B, a 7B parameter model trained on 1T tokens that \\nreached a Databricks LLM Gauntlet score of 30.9%. A member of the DBRX family called DBRX MoE-A (7.7B total \\nparameters, 2.2B active parameters) reached a Databricks Gauntlet score of 30.5% with 3.7x fewer FLOPs. This \\nefficiency is the result of a number of improvements, including using an MoE architecture, other architecture \\nchanges to the network, better optimization strategies, better tokenization, and - very importantly - better \\npretraining data.\\nIn isolation, better pretraining data made a substantial impact on model quality. We trained a 7B model on 1T \\ntokens (called DBRX Dense-A) using the DBRX pretraining data. It reached 39.0% on the Databricks Gauntlet \\ncompared to 30.9% for MPT-7B. We estimate that our new pretraining data is at least 2x better token-for-token \\nthan the data used to train MPT-7B. In other words, we estimate that half as many tokens are necessary to reach \\nthe same model quality. We determined this by training DBRX Dense-A on 500B tokens; it outperformed MPT-7B \\non the Databricks Gauntlet, reaching 32.1%. In addition to better data quality, another important contributor to \\nthis token-efficiency may be the GPT-4 tokenizer, which has a large vocabulary and is believed to be especially \\ntoken-efficient. These lessons about improving data quality translate directly into practices and tools that our \\ncustomers use to train foundation models on their own data.\\nMODEL TOTAL PARAMS ACTIVE PARAMS GAUNTLET SCORE RELATIVE FLOPS\\nDBRX MoE-A 7.7B 2.2B 30.5% 1x\\nMPT-7B (1T tokens) — 6.7B 30.9% 3.7x\\nDBRX Dense-A (1T tokens) — 6.7B 39.0% 3.7x\\nDBRX Dense-A (500B tokens) — 6.7B 32.1% 1.85x\\nDBRX MoE-B 23.5B 6.6B 45.5% 1x\\nLLaMA2-13B — 13.0B 43.8% 1.7x\\nTable 5:  Details of several test articles that we used to validate the training efficiency of the DBRX MoE architecture and end-to-end training pipeline.\\n14THE BIG BOOK OF GENERATIVE AI\\nInference Efficiency\\nFigure 2 shows the end-to-end inference efficiency of serving DBRX and similar models using NVIDIA \\nTensorRT-LLM with our optimized serving infrastructure and 16-bit precision. We aim for this benchmark \\nto reflect real-world usage as closely as possible, including multiple users simultaneously hitting the same \\ninference server. We spawn one new user per second, each user request contains an approximately 2000 \\ntoken prompt, and each response comprises 256 tokens.\\nIn general, MoE models are faster at inference than their total parameter-counts would suggest. This is due \\nto the fact that they use relatively few parameters for each input. We find that DBRX is no exception in this \\nrespect. DBRX inference throughput is 2-3x higher than a 132B non-MoE model.\\nInference efficiency and model quality are typically in tension: bigger models typically reach higher quality, but \\nsmaller models are more efficient for inference. Using an MoE architecture makes it possible to attain better \\ntradeoffs between model quality and inference efficiency than dense models typically achieve. For example, \\nDBRX is both higher quality than LLaMA2-70B and - thanks to having about half as many active parameters - \\nDBRX inference throughput is up to 2x faster (Figure 2). Mixtral is another point on the improved pareto frontier \\nattained by MoE models: it is smaller than DBRX, and it is correspondingly lower in terms of quality but reaches \\nhigher inference throughput. Users of the Databricks Foundation Model APIs can expect to see up to 150 \\ntokens per second for DBRX on our optimized model serving platform with 8-bit quantization.\\n15Figure 2:  Inference throughput for various model configurations on our optimized serving infrastructure using NVIDIA TensorRT-LLM at 16-bit \\nprecision with the best optimization flags we could find. Models are run in tensor-parallel across the entire node. The input prompt contains \\napproximately 2000 prompt tokens and we generate 256 output tokens. One new user spawns every second.\\n16\\nTHE BIG BOOK OF GENERATIVE AITHE BIG BOOK OF GENERATIVE AI\\nHow We Built DBRX\\nDBRX was trained on 3072 NVIDIA H100s connected by 3.2Tbps Infiniband. The main process of building DBRX \\n- including pretraining, post-training, evaluation, red-teaming, and refining - took place over the course of \\nthree months. It was the continuation of months of science, dataset research, and scaling experiments, not to \\nmention years of LLM development at Databricks that includes the MPT and Dolly projects and the thousands \\nof models we have built and brought to production with our customers.\\nTo build DBRX, we leveraged the same suite of Databricks tools that are available to our customers. We \\nmanaged and governed our training data using Unity Catalog. We explored this data using newly acquired  \\nLilac AI. We processed and cleaned this data using Apache Spark™ and Databricks notebooks. We trained \\nDBRX using optimized versions of our open-source training libraries: MegaBlocks, LLM Foundry, Composer, \\nand Streaming. We managed large scale model training and finetuning across thousands of GPUs using our \\nMosaic AI Training service. We logged our results using MLflow. We collected human feedback for quality and \\nsafety improvements through Mosaic AI Model Serving and Inference Tables. We manually experimented with \\nthe model using the Databricks Playground. We found the Databricks tools to be best-in-class for each of their \\npurposes, and we benefited from the fact that they were all part of a unified product experience.\\nGet Started With DBRX on Databricks\\nIf you’re looking to start working with DBRX right away, it’s easy to do so with the Databricks Mosaic AI \\nFoundation Model APIs. You can quickly get started with our pay-as-you-go pricing and query the model from \\nour AI Playground chat interface. For production applications, we offer a provisioned throughput option to \\nprovide performance guarantees, support for finetuned models, and additional security and compliance. To \\nprivately host DBRX, you can download the model from the Databricks Marketplace and deploy the model on \\nModel Serving.\\n17THE BIG BOOK OF GENERATIVE AI\\nConclusions\\nAt Databricks, we believe that every enterprise should have the ability to control its data and its destiny in the \\nemerging world of GenAI. DBRX is a central pillar of our next generation of GenAI products, and we look forward \\nto the exciting journey that awaits our customers as they leverage the capabilities of DBRX and the tools we \\nused to build it. In the past year, we have trained thousands of LLMs with our customers. DBRX is only one \\nexample of the powerful and efficient models being built at Databricks for a wide range of applications, from \\ninternal features to ambitious use-cases for our customers.\\nAs with any new model, the journey with DBRX is just the beginning, and the best work will be done by those \\nwho build on it: enterprises and the open community. This is also just the beginning of our work on DBRX, and \\nyou should expect much more to come.\\nContributions\\nThe development of DBRX was led by the Mosaic team that previously built the MPT model family, in \\ncollaboration with dozens of engineers, lawyers, procurement and finance specialists, program managers, \\nmarketers, designers, and other contributors from across Databricks. We are grateful to our colleagues, friends, \\nfamily, and the community for their patience and support over the past months.\\nIn creating DBRX, we stand on the shoulders of giants in the open and academic community. By making DBRX \\navailable openly, we intend to invest back in the community in hopes that we will build even greater technology \\ntogether in the future. With that in mind, we gratefully acknowledge the work and collaboration of Trevor Gale \\nand his MegaBlocks project (Trevor’s PhD adviser is Databricks CTO Matei Zaharia), the PyTorch team and \\nthe FSDP project, NVIDIA and the TensorRT-LLM project, the vLLM team and project, EleutherAI and their \\nLLM evaluation project, Daniel Smilkov and Nikhil Thorat at Lilac AI, and our friends at the Allen Institute for \\nArtificial Intelligence (AI2).\\n18THE BIG BOOK OF GENERATIVE AI\\nStage 1: Prompt Engineering\\nMany companies still remain in the foundational stages of adopting generative AI technology. They have  \\nno overarching AI strategy in place, no clear use cases to pursue and no access to a team of data scientists and \\nother professionals who can help guide the company’s AI adoption journey.\\nIf this is like your business, a good starting point is an off-the-shelf LLM. While these LLMs lack the domain-\\nspecific expertise of custom AI models, experimentation can help you plot your next steps. Your employees can \\ncraft specialized prompts and workflows to guide their usage. Your leaders can get a better understanding of \\nthe strengths and weaknesses of these tools as well as a clearer vision of what early success in AI might look \\nlike. Your organization can use things like the Databricks AI Playground to figure out where to invest in more \\npowerful AI tools and systems that drive more significant operational gain and even use LLMs as a judge to help \\nevaluate responses.\\nPRACTICAL APPLICATIONS OF GENAI TECHNOLOGY\\nLet’s delve into a compelling use case that illustrates the power of prompt engineering with off-the-shelf \\nLLMs. Consider the challenge many businesses face: sifting through vast amounts of product reviews  \\nto glean actionable insights. Without a dedicated team of data scientists or a clear AI strategy, this task  \\nmight seem daunting. However, leveraging the flexibility of LLMs through prompt engineering offers a \\nstraightforward solution.\\n19THE BIG BOOK OF GENERATIVE AI\\nPrompt Engineering Use Case\\nAutomated Analysis of Product Reviews Using Large Language Models\\nKeep track of customer feedback at scale\\nCheck out our LLM Solution Accelerators for Retail for more details and to download the notebooks.\\nWhile conversational AI has garnered a lot of media attention in recent months, the capabilities of large \\nlanguage models (LLMs) extend well beyond conversational interactions. It\\'s in these less prominent  \\ncapabilities such as query response, summarization, classification and search that many organizations  \\nare finding immediate opportunities to supercharge their workforce and up-level customer experiences.\\nThe potential of these applications is staggering. By one estimate, LLMs (and other generative AI \\n technologies) could, in the near future, address tasks that today occupy 60%–70% of employees’ time.  \\nThrough augmentation, numerous studies have shown that the time to complete various tasks performed  \\nby knowledge workers such as background research, data analysis and document writing can be cut in half.  \\nAnd still other studies have shown that the use of these technologies can dramatically reduce the time for  \\nnew workers to achieve full productivity.\\nBut before these benefits can be fully realized, organizations must first rethink the management of the \\nunstructured information assets on which these models depend and find ways to mitigate the issues of bias \\nand accuracy that affect their output. This is why so many organizations are currently focusing their efforts \\non focused, internal applications where a limited scope provides opportunities for better information access \\nand human oversight can serve as a check to errant results. These applications, aligned with core capabilities \\nalready residing within the organization, have the potential to deliver real and immediate value, while LLMs and \\ntheir supporting technologies continue to evolve and mature.\\n20THE BIG BOOK OF GENERATIVE AI\\nPRODUCT REVIEW SUMMARIZATION COULD USE A BOOST\\nTo illustrate the potential of a more focused approach to LLM adoption, we consider a fairly simple and common \\ntask performed within many online retail organizations: product review summarization. Today, most organizations \\nemploy a modestly-sized team of workers to read and digest user feedback for insights that may help improve a \\nproduct\\'s performance or otherwise identify issues related to customer satisfaction.\\nThe work is important but anything but sexy. A worker reads a review, takes notes, and moves on to the next. \\nIndividual reviews that require a response are flagged and a summary of the feedback from across multiple \\nreviews are compiled for review by product or category managers.\\nThis is a type of work that\\'s ripe for automation. The volume of reviews that pour into a site mean the more \\ndetailed portions of this work are often performed on a limited subset of products across variable windows \\ndepending on a products importance. In more sophisticated organizations, rules detecting course or \\ninappropriate language and models estimating user sentiment or otherwise classifying reviews for positive, \\nnegative or neutral experiences may be applied to help identify problematic content and draw a reviewer\\'s \\nattention to it. But either way, a lot is missed simply because we can\\'t throw enough bodies at the problem to \\nkeep up and those bodies tend to become bored or fatigued with the monotony of the work.\\nLARGE LANGUAGE MODELS CAN AUTOMATE PRODUCT REVIEW ANALYSIS\\nBy using an LLM, issues of scale and consistency can be easily addressed. All we need to do is bring the product \\nreviews to the model and ask:\\n ■ What are the top three points of negative feedback found across these reviews?\\n ■ What features do our customers like best about this product?\\n ■ Do customers feel they are receiving sufficient value from the product relative to what they are being \\nasked to pay?\\n ■ Are there any reviews that are especially negative or are using inappropriate language?\\n21THE BIG BOOK OF GENERATIVE AI\\nWithin seconds we can have a tidy response, allowing our product managers to focus on responding to issues \\ninstead of simply detecting them.\\nBut what about the problem of accuracy and bias? Standards for identifying inaccuracies and bias in LLM \\noutput are evolving as are techniques for better ensuring that outputs align with an organization\\'s expectations, \\nand the fine-tuning of models using approved content can go a long way to ensure models have a preference to \\ngenerate content that\\'s at least aligned with how an organization prefers to communicate.\\nThis is a long-winded way of saying there is no ideal solution to the problem as of yet. But when compared  \\nto where we are with human-driven processes and more simplistic models or rules-based approaches,  \\nthe results are expected to be better or at a minimum no worse than what we currently experience.  \\nAnd given that these review summaries are for internal consumption, the impact of an errant model can \\nbe easily managed.\\nYOU CAN BUILD A SOLUTION FOR THIS TODAY\\nTo demonstrate exactly how this work could be performed, we have built a Solution Accelerator for summarizing \\nproduct reviews. This is based heavily on a previously published blog from Sean Owen that addressed some of \\nthe core technical challenges of tuning an LLM on the Databricks platform. For the accelerator, we are using the \\nAmazon Product Reviews Dataset, which contains 51 million user-generated reviews across 2 million distinct \\nbooks as this provides access to a wide range of reviewer content and presents a scaling challenge many \\norganizations will recognize.\\nWe imagine a scenario in which a team of product managers receives customer feedback through online \\nreviews. These reviews are important for identifying issues that may need to be addressed regarding a \\nparticular item and for steering future books to be offered by the site. Without the use of technology, this team \\nstruggles to read all the feedback and summarize into a workable set notes. As a result, they limit their attention \\nto just the most critical items and are able to only process the feedback on a sporadic basis.\\n22THE BIG BOOK OF GENERATIVE AI\\nBut using Databricks, they are able to set up a pipeline to collect feedback from a wider range of products \\nand summarize these on a regular basis. Recognizing that positively rated products are likely to highlight the \\nstrengths of these books while lower rated products are likely to focus on their weaknesses, they separate  \\nthese reviews based on user-provided ratings and task an LLM to extract different sets of information from  \\neach high-level category of reviews.\\nSummary metrics are provided to allow product managers an overview of the feedback received and are \\nbacked by more detailed summaries generated by the LLM (Figure 1).\\nFigure 1: Summary metrics and bullet-point details extracted from user reviews extracted using an LLM\\n23THE BIG BOOK OF GENERATIVE AI\\nDATABRICKS BRINGS TOGETHER ALL THE COMPONENTS OF A SOLUTION\\nThe scenario demonstrated above depends on the use of an LLM. In months prior, the use of such an LLM \\nrequired access to specialized computational infrastructures, but with advances in the open source community \\nand investments in the Databricks platform, we are now able to run the LLM in our local Databricks environment.\\nIn this particular scenario, the sensitivity of the data was not a motivating factor for this choice. Instead, we \\nfound that the volume of reviews to be processed tipped the cost scales toward the use of Databricks, allowing \\nus to trim about one-third of the cost of implementing a similar solution using a third-party service.\\nIn addition, we found that by implementing our own infrastructure, we were able to scale the environment up \\nfor faster processing, tackling as many as 760,000 reviews per hour in one test without having to be concerned \\nwith constraints imposed by an external service. While most organizations will not have the need to scale quite \\nto that level, it\\'s nice to know it is there should it be.\\nBut this solution is more than just an LLM. To bring together the whole solution we needed to develop a data \\nprocessing workflow to receive incoming reviews, prepare them for submission to the model and to capture \\nmodel output for further analysis. As a unified data platform, Databricks provides us the means to address \\n both data engineering and data science requirements without data replication. And when we are done \\nprocessing the reviews, our analysts can use their tools of choice to query the output and make business \\ndecisions. Through Databricks, we have access to the full array of capabilities for us to build a solution aligned \\nwith our business’ needs.\\n24THE BIG BOOK OF GENERATIVE AI\\nStage 2: Retrieval Augmented Generation\\nRetrieval augmented generation (RAG) lets you bring in supplemental knowledge resources to make an  \\noff-the-shelf AI system smarter. RAG won’t change the underlying behavior of the model, but it will improve  \\nthe quality and accuracy of the responses.\\nHowever, at this point, your business should not be uploading its “mission-critical” data. Instead, the RAG \\nprocess typically involves smaller amounts of nonsensitive information.\\nFor example, plugging in an employee handbook can allow your workers to start asking the underlying model \\nquestions about the organization’s vacation policy. Uploading instruction manuals can help power a service \\nchatbot. With the ability to query support tickets using AI, support agents can get answers quicker; however, \\ninputting confidential financial data so employees can inquire about the company’s performance is likely a step \\ntoo far.\\nTo get started, your team should first consolidate and cleanse the data you intend to use. With RAG, it’s vital \\nthat your company stores the data in sizes that will be appropriate for the downstream models. Often, that \\nrequires users to splice it into smaller segments.\\nThen, you should seek out a tool like Databricks Vector Search, which enables users to quickly set up their own \\nvector database. And because it’s governed by Unity Catalog, granular controls can be put in place to ensure \\nemployees are only accessing the datasets for which they have credentials.\\nFinally, you can then plug that endpoint into a LLM. A tool like Databricks MLflow helps to centralize the \\nmanagement of those APIs.\\n25THE BIG BOOK OF GENERATIVE AI\\nAmong the benefits of RAG are reduced hallucinations, more up-to-date and accurate responses,  \\nand better domain-specific intelligence. RAG-assisted models are also a more cost-effective approach  \\nfor most organizations.\\nWhile RAG will help improve the results from commercial models, there are still many limitations to the use \\nof RAG. If your business is unable to get the results it wants, it’s time to move on to heavier-weight solutions, \\nbut moving beyond RAG-supported models often requires a much deeper commitment. The additional \\ncustomization costs more and requires a lot more data.\\nThat’s why it’s key that organizations first build a core understanding of how to use LLMs. By reaching the \\nperformance limitations of off-the-shelf models before moving on, you and your leadership can further hone  \\nin on where to allocate resources.\\nEnhance the Performance of Off-the-Shelf AI Models With RAG\\nLet’s explore a practical use case that demonstrates how real-time structured data can significantly improve \\nthe response quality of your RAG applications. This example will showcase how integrating dynamic information \\ncan transform the effectiveness and applicability of AI in your business operations.\\n26THE BIG BOOK OF GENERATIVE AI\\nRAG Use Case\\nImprove Your RAG Application Response Quality With Real-Time Structured Data\\nby Mani Parkhe, Aakrati Talati, Sue Ann Hong, Craig Wiley, Chenen Liang and Mingyang Ge\\nRetrieval augmented generation (RAG) is an efficient mechanism to provide relevant data as context in \\nGenAI applications. Most RAG applications typically use vector indexes to search for relevant context from \\nunstructured data such as documentation, wikis, and support tickets. Yesterday, we announced Databricks \\nVector Search Public Preview that helps with exactly that. However, GenAI response quality can be enhanced by \\naugmenting these text-based contexts with relevant and personalized structured data. Imagine a GenAI tool on \\na retail website where customers inquire, \"Where’s my recent order?\" This AI must understand that the query \\nis about a specific purchase, then gather up-to-date shipment information for line items, before using LLMs to \\ngenerate a response. Developing these scalable applications demands substantial work, integrating technologies \\nfor handling both structured and unstructured data with GenAI capabilities.\\nWe are excited to announce the public preview of Databricks Feature & Function Serving, a low latency real-\\ntime service designed to serve structured data from the Databricks Data Intelligence Platform. You can instantly \\naccess pre-computed ML features as well as perform real-time data transformations by serving any Python \\nfunction from Unity Catalog. The retrieved data can then be used in real-time rule engines, classical ML, and \\nGenAI applications.\\nUsing Feature and Function Serving (AWS)(Azure) for structured data in coordination with Databricks Vector \\nSearch (AWS)(Azure) for unstructured data significantly simplifies productionalization of GenAI applications. \\nUsers can build and deploy these applications directly in Databricks and rely on existing data pipelines, \\ngovernance, and other enterprise features. Databricks customers across various industries are using these \\ntechnologies along with open source frameworks to build powerful GenAI applications such as the ones \\ndescribed in the table below.\\n27INDUSTRY USE CASE\\nRetail  ■ Product Recommendations / Search Ranking using user preferences, search history, location . . . etc.\\n ■ Image and metadata based product search\\n ■ Inventory management and forecasting using sales data, seasonal trends and market/competitive analysis\\nEducation  ■ Personalized learning plans based on past mistakes, historical trends and  cohorts\\n ■ Automated grading, feedback, follow-ups and progress reporting\\n ■ Content filtering for issued devices\\nFinancial Services  ■ Natural language apps for analysts and investors to correlate earning calls and reports with market intelligence and historical trends\\n ■ Fraud and risk analysis\\n ■ Personalized wealth management, retirement planning, what-if analysis and next-best actions \\nTravel and Hospitality  ■ Chatbots for personalized customer interactions and tailored travel recommendations\\n ■ Dynamic route planning using weather, live traffic patterns, and historical data\\n ■ Dynamic price optimization using competitive analysis and demand-based pricing\\nHealthcare and Life Sciences  ■ Patient/member engagement and health summaries\\n ■ Support apps for personalized care, clinical decisions and care coordination\\n ■ R&D report summarization, clinical trial analysis, drug repurposing\\nInsurance  ■ Risk assessment for mortgage underwriting using text and structured data about properties and neighborhoods\\n ■ User chatbots for questions about policies, risk and what-if analysis\\n ■ Claim processing automation\\nTechnology and Manufacturing  ■ Prescriptive maintenance and diagnostics for equipment using guided instruction\\n ■ Anomaly detection on live data stream against historical statistics\\n ■ Automated analysis for daily production / shift analysis and future planning\\nMedia and Entertainment  ■ In-app content discovery and recommendations, personalized email and digital marketing\\n ■ Content localization\\n ■ Personalized gaming experiences and game review\\n28\\nTHE BIG BOOK OF GENERATIVE AITHE BIG BOOK OF GENERATIVE AI\\nSERVING STRUCTURED DATA TO RAG APPLICATIONS\\nTo demonstrate how structured data can help enhance the quality of a GenAI application, we use the following \\nexample for a travel planning chatbot. The example shows how user preferences (example: “ocean view” or \\n“family friendly”) can be paired with unstructured information sourced about hotels to search for hotel matches. \\nTypically hotel prices dynamically change based on demand and seasonality. A price calculator built into the \\nGenAI application ensures that the recommendations are within the user\\'s budget. The GenAI application that \\npowers the bot uses Databricks Vector Search and Databricks Feature and Function Serving as building blocks \\nto serve the necessary personalized user preferences and budget and hotel information using LangChain’s \\nagents API.\\nYou can find the complete notebook for this RAG Chain application as depicted above. This application can be \\nrun locally within the notebook or deployed as an endpoint accessible by a chatbot user interface.\\n29THE BIG BOOK OF GENERATIVE AI\\nACCESS YOUR DATA AND FUNCTIONS AS REAL-TIME ENDPOINTS\\nWith Feature Engineering in Unity Catalog you can already use any table with a primary key to serve features \\nfor training and serving. Databricks Model Serving supports using Python functions to compute features on-\\ndemand. Built using the same technology available under the hood for Databricks Model Serving, feature and \\nfunction endpoints can be used to access any pre-computed feature or compute them on-demand. With a \\nsimple syntax you can define a feature spec function in Unity Catalog that can encode the directed acyclic \\ngraph to compute and serve features as a REST endpoint.\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\n27\\n28\\nfrom databricks.feature_engineering import (\\n  FeatureFunction,\\n  FeatureLookup,\\n  FeatureEngineeringClient,\\n)\\nfeatures = [\\n  # Lookup columns `latitude` and `longitude` from `restaurants` table in UC using the input `restaurant_id` as key\\n  FeatureLookup(\\n    table_name=\"main.default.restaurants\",\\n    lookup_key=\"restaurant_id\",\\n    features=[\"latitude”, “longitude\"]\\n  ),\\n  # Calculate a new feature called `distance` using the restaurant and user\\'s current location\\n  FeatureFunction(\\n    udf_name=\"main.default.distance\",\\n    output_name=\"distance\",\\n    # bind the function parameter with input from other features or from request.\\n    input_bindings={\"user_latitude\": \"user_latitude\", \"user_longitude\": \"user_longitude\",\\n                    \"restaurant_latitude\": \"latitude\", \"restaurant_longitude\": \"longitude\"},\\n  ),\\n]\\nfe = FeatureEngineeringClient()\\n# Create a feature spec with the features listed above.\\n# The FeatureSpec can be accessed in UC as a Function.\\nfe.create_feature_spec(\\n  name=\"main.default.restaurant_features\",\\n  features=features,\\n)\\n30THE BIG BOOK OF GENERATIVE AI\\n \\n1\\n2\\n3\\n4\\n \\n5\\n6\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\nfrom databricks.feature_engineering.entities.feature_serving_endpoint import (\\n  ServedEntity,\\n  EndpointCoreConfig,\\n)\\nfe.create_feature_serving_endpoint(\\n  name=\"restaurant-features\",\\n    config=EndpointCoreConfig(\\n    served_entities=ServedEntity(\\n      feature_spec_name=\"main.default.restaurant_features\",\\n      workload_size=\"Small\",\\n      scale_to_zero_enabled=True\\n    )\\n  )\\n)\\nThis feature spec function can be served in real-time as a REST endpoint. All endpoints are accessible in \\nthe Serving left navigation tab including features, function, custom trained models, and foundation models. \\nProvision the endpoint using this API.\\nThe endpoint can also be created using a UI workflow as shown in the following graphic\\n31THE BIG BOOK OF GENERATIVE AI\\nNow features can be accessed in real time by querying the endpoint:\\nTo serve structured data to real-time AI applications, precomputed data needs to be deployed to operational \\ndatabases. Users can already use external online stores as a source of precomputed features — for example \\nDynamoDB and Cosmos DB are commonly used to serve features in Databricks Model Serving. Databricks \\nOnline Tables (AWS)(Azure) adds new functionality that simplifies synchronization of precomputed features to a \\ndata format optimized for low latency data lookups. You can sync any table with a primary key as an online table \\nand the system will set up an automatic pipeline to ensure data freshness.\\n \\n1\\n2\\n3\\n4\\n5\\n \\n6\\ncurl \\\\\\n  -u token:$DATABRICKS_TOKEN \\\\\\n  -X POST \\\\\\n  -H \"Content-Type: application/json\" \\\\\\n  -d \\'{\"dataframe_records\": [{\"user_latitude\": 37.9711, \"user_longitude\": -122.3940, \"restaurant_id\": 5}]}\\' \\\\ \\n  https://<databricks-instance>/serving-endpoints/restaurant-features/invocations\\n32THE BIG BOOK OF GENERATIVE AI\\nStage 3: Fine-Tuning a Foundation Model\\nMoving beyond RAG to model fine-tuning lets you start building models that are much more deeply \\npersonalized to the business. If you have already been experimenting with commercial models across your \\noperations, you are likely ready to advance to this stage. There’s a clear understanding at the executive level of \\nthe value of generative AI, as well as an understanding of the limitations of publicly available LLMs. Specific use \\ncases have been established. And now, you and your enterprise are ready to go deeper.\\nWith fine-tuning, you can take a general-purpose model and train it on your own specific data. For example, \\ndata management provider Stardog relies on the Mosaic AI tools from Databricks to fine-tune the off-the-shelf \\nLLMs they use as a foundation for their Knowledge Graph Platform. This enables Stardog’s customers to query \\ntheir own data across the different silos simply by using natural language.\\nIt’s imperative that organizations at this stage have an underlying architecture in place that will help ensure the \\ndata supporting the models is secure and accurate. Fine-tuning an AI system requires an immense amount of \\nproprietary information and, as your business advances on the AI maturity curve, the number of models running \\nwill only grow, increasing the demand for data access.\\nThat’s why you need to have the right mechanisms in place to track data from the moment it’s generated to \\nwhen it’s eventually used, and why Unity Catalog is such a popular feature among Databricks customers. With \\nUnity Catalog’s data lineage capabilities, businesses always know where data is moving and who is accessing it.\\nAny Unity Catalog table with primary keys can be used to serve features in GenAI applications using Databricks \\nOnline Tables.\\n33THE BIG BOOK OF GENERATIVE AI\\nFine-Tuning Use Cases\\nCreating a Bespoke LLM for AI-Generated Documentation \\nIt’s easier than you think: 2 engineers, 1 month and less than $1,000\\nby Matthew Hayes, Hongyi Zhang, Tao Feng, Jan van der Vegt, Zaheera Valani and Reynold Xin\\nIn this example, we share our experience from prototyping a hackathon project using off-the-shelf SaaS-based \\nLLMs to creating a bespoke LLM that is better, faster, and cheaper. The new model took 2 engineers, 1 month \\nand less than $1,000 in compute cost to develop. We hope you will find the learnings useful, as we believe \\nthey apply to a wide class of GenAI use cases. More importantly, it has allowed us to take advantage of rapid \\nadvances being made in open-source LLMs.\\nWHAT IS AI-GENERATED DOCUMENTATION?\\nAt the center of each data platform lies a (potentially enormous) collection of datasets (often in the form of \\ntables). In virtually every organization we have worked with, the vast majority of tables are not documented. The \\nabsence of documentation provides a number of challenges, including making it difficult for humans to discover \\nthe data needed for answering a business question, or more recently, for AI agents to automatically find \\ndatasets to use in response to questions (a key capability in our platform that we’re calling Data Intelligence).\\n34THE BIG BOOK OF GENERATIVE AI\\nRather than relying on humans to document these datasets, we prototyped as part of our quarterly hackathon \\na new workflow using an off-the-shelf SaaS-based LLM to automatically generate documentation for tables \\nand their columns based on their schema. This new workflow would automatically suggest descriptions for the \\ntables and columns and allow users to either individually accept, bulk accept, or modify the suggestions for \\nhigher fidelity, as shown below. When we showed this prototype to some users, their immediate question was \\nuniversally, “When can I have it?!”\\n35THE BIG BOOK OF GENERATIVE AI\\nCHALLENGES WITH LLMS\\nAs we moved toward launching this feature to all our customers, we ran into three challenges with the model:\\n1. Quality: The ultimate success of this feature depends on the quality of the generated documentation. \\nAlthough we could measure the quality (in terms of how often they are accepted), we had limited knobs \\nat our disposal to improve it, aside from basic prompting. During the private preview period, we also \\nsometimes noticed the quality of the suggestions degrading, without any change to our codebase. Our \\nspeculation is that the SaaS LLM controller rolled out updates to the model that sometimes affected \\nperformance on specific tasks.\\n2. Performance (throughput): We had limited API quota provisioned with the SaaS LLM provider. We \\nwork with tens of thousands of organizations, and it is not uncommon that a single organization would \\nhave millions of tables. It would take too long to generate documentation for all the tables based on the \\nthroughput quota.\\n3. Cost: Related to the above, it was not cost-effective unless we started charging customers for using this \\nspecific feature.\\nWe have heard similar concerns from a variety of customers as they try to move their LLM-based applications \\nfrom a proof-of-concept to production and saw this as an excellent opportunity for us to explore alternatives \\nfor an organization like ours.\\nWe experimented with different versions of the SaaS LLMs, but they all had the same challenges. This is not \\nsurprising in hindsight. The SaaS LLMs are an engineering marvel, but they are very general models that need to \\naddress all the use cases from table generation to conversing about the meaning of life. The generality means \\nit needs to have an extremely large number of parameters, which limits how fast and how cheap it can return \\nanswers. As it continues to evolve to optimize for different use cases, it might also regress the narrower use case \\nwe have.\\n36THE BIG BOOK OF GENERATIVE AI\\nBUILDING A BESPOKE MODEL\\nTo address the aforementioned challenges, we started building a bespoke model. It took a team of two \\nengineers one month to build a customized, smaller LLM that was better, faster, and cheaper:\\n ■ Quality: Based on our evaluation (see the following section), the model is significantly better than the \\ncheaper version of the SaaS model, and roughly equivalent to the more expensive version.\\n ■ Performance (throughput): Because the bespoke model is a lot smaller, it can fit in A10 GPUs, and we \\ncan increase the inference throughput with horizontal scaling. The smaller GPUs are also more available, \\nwhich enables us to generate the descriptions for all tables faster.\\n ■ Cost: Each fine-tuning run of the model only costs a few dollars, and in aggregate, it cost less than $1000 \\nto develop because we did a lot of experiments. It also resulted in a 10 fold reduction in inference cost.\\nThe first step was to treat this as an applied machine learning problem. “Applied machine learning” sounds \\ndaunting and complicated, but all it meant was that we needed to:\\n ■ Find training datasets so we can bootstrap an initial model\\n ■ Identify an evaluation mechanism so we can measure the quality, before rolling it out to production\\n ■ Train and select models\\n ■ Collect real-world usage metrics, so we can monitor how well a monitor does in production\\n ■ Iterate and roll out new models to continuously improve the three dimensions: quality, performance, cost\\n37THE BIG BOOK OF GENERATIVE AI\\nTRAINING DATA\\nWe created the initial training dataset for this fine-tuning task, using two different sources of data:\\n1. North American Industry Classification System (NAICS) codes. This is a public dataset used by Federal \\nstatistical agencies in classifying business establishments for the purpose of collecting, analyzing, and \\npublishing statistical data related to the U.S. business economy.\\n2. Databricks’ internal use case taxonomy curation datasets. This is a series of internal datasets created by \\nour solution architects to show customers best practice architectures.\\nThen we synthesized CREATE TABLE statements using the above use cases to yield a diverse set of tables and \\ngenerated sample responses including table descriptions and column comments using another LLM. In total,  \\nwe generated ~3600 training examples. \\nNotably, we didn’t use any customer data for training this powerful feature that all of our customers can  \\nbenefit from. \\nBOOTSTRAPPING MODEL EVALUATION\\nAfter the feature launch, we could measure a model’s quality through production metrics such as the rate of \\nusers accepting the suggestions. But before we made it to the launch, we needed a way to evaluate the model’s \\nquality against that of the SaaS LLM.\\nTo do that in an unbiased fashion, we set up a simple double-blind evaluation framework in which we asked \\n4 employees to rate table descriptions generated from the two models we wanted to compare using a set of \\n62 unseen tables. Our framework then generated a sheet where each row showed the input and showed both \\noutputs in a randomized order. The evaluator would vote on the better sample (or give a tie). The framework then \\nprocessed the votes from different evaluators to generate a report; it also summarizes the degree to which each \\nof the evaluators agreed.\\nBased on our experiences so far, having an evaluation dataset of tens to hundreds of data points is a sufficient \\ninitial milestone and can be generalized to other use cases as well.\\n38THE BIG BOOK OF GENERATIVE AI\\nMODEL SELECTION AND FINE-TUNING\\nWe considered the following criteria for model selection:\\n ■ Whether the license supports commercial use\\n ■ Performance (quality) of the model for text generation\\n ■ Speed of the model\\nBased on these criteria, MPT-7B and Llama2-7B were the leading candidates, as shown in our LLM guide. We \\nconsidered larger models such as MPT-30B and Llama-2-13B. In the end we chose MPT-7B, as it has the best \\ncombination of quality and inference performance:\\n ■ There was no discernable difference in the quality between the MPT-7B and Llama-2-7B fine-tuned \\nmodels for this task.\\n ■ The smaller 7B models, after fine-tuning, were already meeting the quality bar. It was significantly better \\nthan the cheaper version of the SaaS model, and roughly equivalent to the more expensive version.\\n ■ We did not yet observe a measurable benefit of using larger models for this task that would justify the \\nincreased serving costs.\\n ■ The latency for the smaller models was significantly better than the larger models while offering \\ncomparable quality so we could deliver a much snappier product experience.\\n ■ The smaller model could fit comfortably and be served using A10 GPUs, which were more readily \\navailable. Their abundance would mean higher inference throughput for the task.\\nThe total time it took to fine-tune the model on the ~3600 examples was only around 15 minutes!\\nWhile we chose MPT-7B for our model, we believe the LLM landscape is changing rapidly and the best model \\ntoday won’t be the best model tomorrow. That’s why we consider this to be an iterative and continuous process \\nand are focused on using tools that make our evaluation efficient and fast.\\n39THE BIG BOOK OF GENERATIVE AI\\nKEY ARCHITECTURAL COMPONENTS OF OUR PRODUCTION PIPELINE\\nWe were able to build this quickly by relying on the following key components of the Databricks Data \\nIntelligence Platform:\\n ■ Databricks LLM fine-tuning: It provides a very simple infrastructure for fine-tuning the models for our \\ntask. We prepared the training data in JSON format, and with a one-line CLI command, we were able to \\nfine-tune the LLMs.\\n ■ Unity Catalog: The models that we use in production are registered in Unity Catalog (UC), providing the \\ngovernance we need to not just for the data, but also the models. With its end-to-end lineage feature, UC \\nalso gives us traceability from the models back to the datasets they are trained on.\\n ■ Delta Sharing: We used Delta Sharing to distribute the model to all production regions we have around \\nthe world for faster serving.\\n ■ Databricks optimized LLM serving: Once the models are registered in UC, they can be served using the \\nnew optimized LLM serving, which provides significant performance improvement in terms of throughput \\nand latency improvement compared to traditional serving for LLM serving.\\n40THE BIG BOOK OF GENERATIVE AI\\nCOST\\nThe fine-tuning compute cost for the whole project was less than $1000 (each fine-tuning run cost only a few \\ndollars). And the final result is a more than 10-fold reduction in cost. Why is the cost-saving so significant? It is \\nnot surprising if we consider the following:\\n ■ As mentioned earlier, the SaaS LLMs need to address all the use cases, including acting as a general \\nchatbot. The generality requires an extremely large number of parameters, which incurs significant \\ncompute costs in inference.\\n ■ When we fine-tune for a more specific task, we can use a much smaller prompt. Larger, general-purpose \\nmodels require longer prompts that include detailed instructions on what the input is and what form the \\noutput should take. Fine-tuned models can bake instructions and expected structure into the model \\nitself. We found we were able to reduce the number of input tokens with no impact on performance by \\nmore than half.\\n ■ Inference costs scale with the number of input and output tokens, and costs scale linearly for SaaS \\nservices that are charged per token. With Databricks’ LLM Serving offering, we offer provisioned \\nthroughput charged per hour, which provides consistent latencies, uptime SLAs, and autoscaling. Because \\nsmaller LLMs can fit in smaller GPUs that are much cheaper and more available and because we offer a \\nhighly optimized runtime, we can aggressively drive down costs. Also, smaller LLMs scale up and down \\nfaster, meaning we can quickly scale up to meet peaks of demand and aggressively scale down when \\nusage is lighter, creating substantial cost efficiency in production.\\n41THE BIG BOOK OF GENERATIVE AI\\nCONCLUSION\\nHaving well-documented data is critical to all data users, and growing more important day-by-day to power \\nAI-based data platforms (what we’re calling Data Intelligence). We started with SaaS LLMs for prototyping this \\nnew GenAI feature but ran into challenges with quality, performance, and cost. We built a bespoke model to do \\nthe same task at better quality, and yet resulting in higher throughput with scale-out and 10x cost reduction. To \\nrecap what it took:\\n ■ 2 engineers\\n ■ 1 month\\n ■ Less than $1,000 in compute for training and experimentation\\n ■ MPT-7B fine-tuned on 3600 synthetically generated examples, in under 15 minutes\\n ■ 4 human evaluators, with 62 initial evaluation examples\\nThis experience demonstrates how easy it is to develop and deploy bespoke LLMs for specific tasks. This model \\nis now live on Databricks in Amazon Web Services and Google Cloud and is being used to power most data \\nannotations on the platform.\\n42THE BIG BOOK OF GENERATIVE AI\\nEfficient Fine-Tuning With LoRA: A Guide to Optimal Parameter Selection for Large Language Models\\nby Avinash Sooriyarachchi\\nWith the rapid advancement of neural network-based techniques and large language model (LLM) research, \\nbusinesses are increasingly interested in AI applications for value generation. They employ various machine \\nlearning approaches, both generative and non-generative, to address text-related challenges such as \\nclassification, summarization, sequence-to-sequence tasks, and controlled text generation. Organizations can \\nopt for third-party APIs, but fine-tuning models with proprietary data offers domain-specific and pertinent \\nresults, enabling cost-effective and independent solutions deployable across different environments in  \\na secure manner.\\nEnsuring efficient resource utilization and cost-effectiveness is crucial when choosing a strategy for fine-\\ntuning. This blog explores arguably the most popular and effective variant of such parameter efficient \\nmethods, Low Rank Adaptation (LoRA), with a particular emphasis on QLoRA (an even more efficient variant of \\nLoRA). The approach here will be to take an open large language model and fine-tune it to generate fictitious \\nproduct descriptions when prompted with a product name and a category. The model chosen for this \\nexercise is OpenLLaMA-3b-v2, an open large language model with a permissive license (Apache 2.0), and the \\ndataset chosen is Red Dot Design Award Product Descriptions, both of which can be downloaded from the \\nHuggingFace Hub at the links provided.\\nFINE-TUNING, LORA AND QLORA\\nIn the realm of language models, fine-tuning an existing language model to perform a specific task on specific \\ndata is a common practice. This involves adding a task-specific head, if necessary, and updating the weights of \\nthe neural network through backpropagation during the training process. It is important to note the distinction \\nbetween this fine-tuning process and training from scratch. In the latter scenario, the model\\'s weights are \\nrandomly initialized, while in fine-tuning, the weights are already optimized to a certain extent during the \\npretraining phase. The decision of which weights to optimize or update, and which ones to keep frozen, depends \\non the chosen technique.\\nFull fine-tuning involves optimizing or training all layers of the neural network. While this approach typically \\nyields the best results, it is also the most resource-intensive and time-consuming.\\n43THE BIG BOOK OF GENERATIVE AI\\nFortunately, there exist parameter-efficient approaches for fine-tuning that have proven to be effective. \\nAlthough most such approaches have yielded less performance, Low Rank Adaptation (LoRA) has bucked \\nthis trend by even outperforming full fine-tuning in some cases, as a consequence of avoiding catastrophic \\nforgetting (a phenomenon which occurs when the knowledge of the pretrained model is lost during the  \\nfine-tuning process).\\nLoRA is an improved fine-tuning method where instead of fine-tuning all the weights that constitute the  \\nweight matrix of the pretrained large language model, two smaller matrices that approximate this larger matrix \\nare fine-tuned. These matrices constitute the LoRA adapter. This fine-tuned adapter is then loaded to the \\npretrained model and used for inference.\\nQLoRA is an even more memory efficient version of LoRA where the pretrained model is loaded to GPU  \\nmemory as quantized 4-bit weights (compared to 8-bits in the case of LoRA), while preserving similar \\neffectiveness to LoRA. Probing this method, comparing the two methods when necessary, and figuring out the \\nbest combination of QLoRA hyperparameters to achieve optimal performance with the quickest training time \\nwill be the focus here.\\nLoRA is implemented in the Hugging Face Parameter Efficient Fine-Tuning (PEFT) library, offering ease  \\nof use and QLoRA can be leveraged by using bitsandbytes and PEFT together. HuggingFace Transformer \\nReinforcement Learning (TRL) library offers a convenient trainer for supervised fine-tuning with seamless \\nintegration for LoRA. These three libraries will provide the necessary tools to fine-tune the chosen pretrained \\nmodel to generate coherent and convincing product descriptions once prompted with an instruction  \\nindicating the desired attributes.\\n44THE BIG BOOK OF GENERATIVE AI\\nPREPPING THE DATA FOR SUPERVISED FINE-TUNING\\nTo probe the effectiveness of QLoRA for fine-tuning a model for instruction following, it is essential to transform \\nthe data to a format suited for supervised fine-tuning. Supervised fine-tuning in essence, further trains a \\npretrained model to generate text conditioned on a provided prompt. It is supervised in that the model is fine-\\ntuned on a dataset that has prompt-response pairs formatted in a consistent manner.\\nAn example observation from our chosen dataset from the Hugging Face hub looks as follows:\\nAs useful as this dataset is, this is not well formatted for fine-tuning of a language model for instruction following \\nin the manner described.\\nThe following code snippet loads the dataset from the Hugging Face hub into memory, transforms the \\nnecessary fields into a consistently formatted string representing the prompt, and inserts the response (i.e., \\nthe description), immediately afterward. This format is known as the ‘Alpaca format’ in large language model \\nresearch circles as it was the format used to fine-tune the original LlaMA model from Meta to result in the \\nAlpaca model, one of the first widely distributed instruction-following large language models (although not \\nlicensed for commercial use).\\nPRODUCT CATEGORY DESCRIPTION TEXT\\n“Biamp Rack Products” “Digital Audio Processors\" “High recognition value, uniform \\naesthetics and practical \\nscalability — this has been \\nimpressively achieved with the \\nBiamp brand language . . . “\\n“Product Name: Biamp Rack Products; \\nProduct Category: Digital Audio \\nProcessors; Product Description: High \\nrecognition value, uniform aesthetics \\nand practical scalability — this has been \\nimpressively achieved with the Biamp \\nbrand language . . . “\\n45THE BIG BOOK OF GENERATIVE AI\\n \\n1\\n2\\n3\\n4 \\n5\\n6\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\nimport pandas as pd\\nfrom datasets import load_dataset\\nfrom datasets import Dataset\\n#Load the dataset from the HuggingFace Hub\\nrd_ds = load_dataset(\"xiyuez/red-dot-design-award-product-description\")\\n#Convert to pandas dataframe for convenient processing\\nrd_df = pd.DataFrame(rd_ds[\\'train\\'])\\n#Combine the two attributes into an instruction string\\nrd_df[\\'instruction\\'] = \\'Create a detailed description for the following product: \\'+ rd_df[\\'product\\']+\\', belonging to \\ncategory: \\'+ rd_df[\\'category\\']\\nrd_df = rd_df[[\\'instruction\\', \\'description\\']]\\n#Get a 5000 sample subset for fine-tuning purposes\\nrd_df_sample = rd_df.sample(n=5000, random_state=42)\\n#Define template and format data into the template for supervised fine-tuning\\ntemplate = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the \\nrequest.\\n### Instruction:\\n{}\\n### Response:\\\\n\"\"\"\\nrd_df_sample[\\'prompt\\'] = rd_df_sample[\"instruction\"].apply(lambda x: template.format(x))\\nrd_df_sample.rename(columns={\\'description\\': \\'response\\'}, inplace=True)\\nrd_df_sample[\\'response\\'] = rd_df_sample[\\'response\\'] + \"\\\\n### End\"\\nrd_df_sample = rd_df_sample[[\\'prompt\\', \\'response\\']]\\nrd_df[\\'text\\'] = rd_df[\"prompt\"] + rd_df[\"response\"]\\nrd_df.drop(columns=[\\'prompt\\', \\'response\\'], inplace=True)\\nThe resulting prompts are then loaded into a hugging face dataset for supervised fine-tuning. Each such prompt \\nhas the following format.\\n46THE BIG BOOK OF GENERATIVE AI\\n \\n1\\n2\\n3\\n4 \\n5\\n6\\n8\\n9\\n10\\n11\\n12\\n13\\n```\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n### Instruction:\\nCreate a detailed description for the following product: Beseye Pro, belonging to category: Cloud-Based Home Security \\nCamera\\n### Response:\\nBeseye Pro combines intelligent home monitoring with decorative art. The camera, whose form is reminiscent of a water \\ndrop, is secured in the mounting with a neodymium magnet and can be rotated by 360 degrees. This allows it to be \\neasily positioned in the desired direction. The camera also houses modern technologies, such as infrared LEDs, cloud-\\nbased intelligent video analyses and SSL encryption.\\n### End\\n```\\nTo facilitate quick experimentation, each fine-tuning exercise will be done on a 5000 observation subset  \\nof this data.\\nTESTING MODEL PERFORMANCE BEFORE FINE-TUNING\\nBefore any fine-tuning, it’s a good idea to check how the model performs without any fine-tuning to get a \\nbaseline for pretrained model performance.\\nThe model can be loaded in 8-bit as follows and prompted with the format specified in the model card on \\nHugging Face.\\n47THE BIG BOOK OF GENERATIVE AI\\n \\n1\\n2\\n3\\n4 \\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\nimport torch\\nfrom transformers import LlamaTokenizer, LlamaForCausalLM\\nmodel_path = \\'openlm-research/open_llama_3b_v2\\'\\ntokenizer = LlamaTokenizer.from_pretrained(model_path)\\nmodel = LlamaForCausalLM.from_pretrained(\\nmodel_path, load_in_8bit=True, device_map=\\'auto\\',\\n)\\n#Pass in a prompt and infer with the model\\nprompt = \\'Q: Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: \\nOptical Mouse\\\\nA:\\'\\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\\ngeneration_output = model.generate(\\ninput_ids=input_ids, max_new_tokens=128\\n)\\nprint(tokenizer.decode(generation_output[0]))\\n \\n1\\n2\\n3\\n4 \\n5\\n6\\nQ: Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical \\nMouse A: The Corelogic Smooth Mouse is a wireless optical mouse that has a 1000 dpi resolution. It has a 2.4 GHz \\nwireless connection and a 12-month warranty. Q: What is the price of the Corelogic Smooth Mouse? A: The Corelogic \\nSmooth Mouse is priced at $29.99. Q: What is the weight of the Corelogic Smooth Mouse? A: The Corelogic Smooth Mouse \\nweighs 0.1 pounds. Q: What is the dimensions of the Corelogic Smooth Mouse? A: The Corelogic Smooth Mouse has a \\ndimension\\nThe output obtained is not quite what we want.\\nThe first part of the result is actually satisfactory, but the rest of it is more of a rambling mess.\\n48THE BIG BOOK OF GENERATIVE AI\\n \\n1\\n2\\n3\\n4 \\n5\\n6\\n7\\n8\\n9\\nprompt= \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n### Instruction:\\nCreate a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical Mouse\\n### Response:\"\"\"\\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\\ngeneration_output = model.generate(\\ninput_ids=input_ids, max_new_tokens=128\\n)\\nprint(tokenizer.decode(generation_output[0]))\\n \\n1\\n2\\n3\\n4 \\n5\\n6\\nCorelogic Smooth Mouse is a mouse that is designed to be used by people with disabilities. It is a wireless mouse \\nthat is designed to be used by people with disabilities. It is a wireless mouse that is designed to be used by people \\nwith disabilities. It is a wireless mouse that is designed to be used by people with disabilities. It is a wireless \\nmouse that is designed to be used by people with disabilities. It is a wireless mouse that is designed to be used by \\npeople with disabilities. It is a wireless mouse that is designed to be used by people with disabilities. It is a \\nwireless mouse that is designed to be used by\\nSimilarly, if the model is prompted with the input text in the ‘Alpaca format’ as discussed before, the output is \\nexpected to be just as suboptimal:\\nAnd sure enough, it is:\\nThe model performs what it was trained to do, predicts the next most probable token. The point of supervised \\nfine-tuning in this context is to generate the desired text in a controllable manner. Please note that in the \\nsubsequent experiments, while QLoRA leverages a model loaded in 4-bit with the weights frozen, the  \\ninference process to examine output quality is done once the model has been loaded in 8-bit as shown  \\nabove for consistency.\\n49THE BIG BOOK OF GENERATIVE AI\\nTHE TURNABLE KNOBS\\nWhen using PEFT to train a model with LoRA or QLoRA (note that, as mentioned before, the primary difference \\nbetween the two is that in the latter, the pretrained models are frozen in 4-bit during the fine-tuning process), \\nthe hyperparameters of the low rank adaptation process can be defined in a LoRA config as shown below:\\nTwo of these hyperparameters, r and target_modules are empirically shown to affect adaptation quality \\nsignificantly and will be the focus of the tests that follow. The other hyperparameters are kept constant at the \\nvalues indicated above for simplicity.\\nr represents the rank of the low rank matrices learned during the fine-tuning process. As this value is increased, \\nthe number of parameters needed to be updated during the low-rank adaptation increases. Intuitively, a lower \\nr may lead to a quicker, less computationally intensive training process, but may affect the quality of the model \\nthus produced. However, increasing r beyond a certain value may not yield any discernible increase in quality of \\nmodel output. How the value of r affects adaptation (fine-tuning) quality will be put to the test shortly.\\n \\n1\\n2\\n3\\n4 \\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\nfrom peft import LoraConfig\\n...\\n...\\n#If only targeting attention blocks of the model\\ntarget_modules = [\"q_proj\", \"v_proj\"]\\n#If targeting all linear layers\\ntarget_modules = [\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'o_proj\\',\\'gate_proj\\',\\'down_proj\\',\\'up_proj\\',\\'lm_head\\']\\nlora_config = LoraConfig(\\nr=16,\\ntarget_modules = target_modules,\\nlora_alpha=8,\\nlora_dropout=0.05,\\nbias=\"none\",\\ntask_type=\"CAUSAL_LM\",}\\n50THE BIG BOOK OF GENERATIVE AI\\nWhen fine-tuning with LoRA, it is possible to target specific modules in the model architecture. The adaptation \\nprocess will target these modules and apply the update matrices to them. Similar to the situation with \"r,\" \\ntargeting more modules during LoRA adaptation results in increased training time and greater demand for \\ncompute resources. Thus, it is a common practice to only target the attention blocks of the transformer. \\nHowever, recent work as shown in the QLoRA paper by Dettmers et al. suggests that targeting all linear layers \\nresults in better adaptation quality. This will be explored here as well.\\nNames of the linear layers of the model can be conveniently appended to a list with the following code snippet:\\nTUNING THE FINE-TUNING WITH LORA\\nThe developer experience of fine-tuning large language models in general have improved dramatically over the \\npast year or so. The latest high level abstraction from Hugging Face is the SFTTrainer class in the TRL library. To \\nperform QLoRA, all that is needed is the following:\\n1. Load the model to GPU memory in 4-bit (bitsandbytes enables this process)\\n2. Define the LoRA configuration as discussed previously\\n3. Define the train and test splits of the prepped instruction following data into Hugging Face  \\nDataset objects\\n4. Define training arguments: These include the number of epochs, batch size and other training \\nhyperparameters which will be kept constant during this exercise\\n5. Pass these arguments into an instance of SFTTrainer\\nThese steps are clearly indicated in the source file in the repository associated with this blog.\\n \\n1\\n2\\n3\\n4 \\n5\\n6\\n7\\n8\\n9\\nimport re\\nmodel_modules = str(model.modules)\\npattern = r\\'\\\\((\\\\w+)\\\\): Linear\\'\\nlinear_layer_names = re.findall(pattern, model_modules)\\nnames = []\\n# Print the names of the Linear layers\\nfor name in linear_layer_names:\\n    names.append(name)\\ntarget_modules = list(set(names))\\n51THE BIG BOOK OF GENERATIVE AI\\nThe actual training logic is abstracted away nicely as follows:\\nIf MLflow autologging is enabled in the Databricks workspace, which is highly recommended, all the training \\nparameters and metrics are automatically tracked and logged with the MLflow tracking server. This functionality \\nis invaluable in monitoring long-running training tasks. Needless to say, the fine-tuning process is performed \\nusing a compute cluster (in this case, a single node with a single A100 GPU) created using the latest Databricks \\nMachine Runtime with GPU support.\\n \\n1\\n2\\n3\\n4 \\n5\\n6\\n7\\n8\\n9\\n10\\n11\\ntrainer = SFTTrainer(\\nmodel,\\ntrain_dataset=dataset[\\'train\\'],\\neval_dataset = dataset[\\'test\\'],\\ndataset_text_field=\"text\",\\nmax_seq_length=256,\\nargs=training_args,\\n)\\n# Initiate the training process\\nwith mlflow.start_run(run_name= ‘run_name_of_choice’):\\ntrainer.train()\\n52THE BIG BOOK OF GENERATIVE AI\\nHYPERPARAMETER COMBINATION #1: QLoRA with r=8 and targeting “q_proj”, “v_proj”\\nThe first combination of QLoRA hyperparameters attempted is r=8 and targets only the attention blocks, namely \\n“q_proj” and “v_proj” for adaptation.\\nThe following code snippets gives the number of trainable parameters:\\nThese choices result in 2,662,400 parameters being updated during the fine-tuning process (~2.6 million) from a \\ntotal of ~3.2 billion parameters the model consists of. This is less than 0.1% of the model parameters. The entire \\nfine-tuning process on a single Nvidia A100 with 80 GBs of GPU for 3 epochs only takes roughly 12 minutes. The \\nGPU utilization metrics can be conveniently viewed at the metrics tab of the cluster configurations.\\n \\n1\\n2\\nmodel = get_peft_model(model, lora_config)\\nmodel.print_trainable_parameters()\\n53THE BIG BOOK OF GENERATIVE AI\\nAt the end of the training process, the fine-tuned model is obtained by loading the adapter weights to the \\npretrained model as follows:\\nThis model can now be used for inference as any other model.\\nQualitative Evaluation \\nA couple of example prompt-response pairs are listed below\\nPrompt (passed to the model in the Alpaca format, not shown for conciseness here): \\nCreate a detailed description for the following product: Corelogic Smooth Mouse, belonging to category:  \\nOptical Mouse\\nResponse:\\nPrompt: \\nCreate a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless \\nVacuum Cleaner\\nResponse:\\nThe model has clearly been adapted for generating more consistent descriptions. However the response to the \\nfirst prompt about the optical mouse is quite short and the following phrase “The vacuum cleaner is equipped \\nwith a dust container that can be emptied via a dust container” is logically flawed.\\n \\n1 peft_model = PeftModel.from_pretrained(model, adapter_location)\\n \\n1\\n2\\nThe Corelogic Smooth Mouse is a wireless optical mouse with a smooth surface. The mouse is equipped with a 1000 DPI \\nsensor and a 1000 Hz polling rate. The mouse is available in black and white.\\n \\n1\\n2\\n3\\n4\\nThe Hoover Lightspeed is a cordless vacuum cleaner that is equipped with a lithium-ion battery. The battery is \\ncharged via a USB cable. The vacuum cleaner is equipped with a 2-in-1 brush and a turbo brush. The brush is suitable \\nfor cleaning carpets and hard floors. The turbo brush is suitable for cleaning carpets and hard floors. The vacuum \\ncleaner is equipped with a dust container that can be emptied via a dust container.\\n54THE BIG BOOK OF GENERATIVE AI\\nHYPERPARAMETER COMBINATION #2: QLoRA with r=16 and targeting all linear layers\\nSurely, things can be improved here. It is worth exploring increasing the rank of low rank matrices learned during \\nadaptation to 16, i.e., double the value of r to 16 and keep all else  the same. This doubles the number of trainable \\nparameters to 5,324,800 (~5.3 million)\\nQualitative Evaluation \\nThe quality of output, however, remains unchanged for the same exact prompts.\\nPrompt: \\nCreate a detailed description for the following product: Corelogic Smooth Mouse, belonging to category:  \\nOptical Mouse\\nResponse:\\n \\n1\\n2\\nThe Corelogic Smooth Mouse is a high-quality optical mouse with a smooth surface. The mouse is equipped with a 1000 \\nDPI sensor and a 1000 Hz polling rate. The mouse is available in black and white.\\n \\n1\\n2\\n3\\n4\\n5\\nThe Hoover Lightspeed is a cordless vacuum cleaner that is equipped with a lithium-ion battery. The battery is \\ncharged via a USB cable. The vacuum cleaner is equipped with a 2-in-1 brush and a motorized brush. The brush is used \\nto clean hard floors and the motorized brush is used to clean carpets. The vacuum cleaner is equipped with a dust \\ncontainer that can be emptied via a dust container. The vacuum cleaner is equipped with a LED display that shows the \\nremaining battery capacity.\\nPrompt: \\nCreate a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless \\nVacuum Cleaner\\nResponse:\\nThe same lack of detail and logical flaws in detail where details are available persists. If this fine tuned model is \\nused for product description generation in a real-world scenario, this is not acceptable output.\\n55THE BIG BOOK OF GENERATIVE AI\\nHYPERPARAMETER COMBINATION #3: QLoRA with r=8 and targeting all linear layers\\nGiven that doubling r does not seemingly result in any perceivable increase in output quality, it is worth \\nchanging the other important knob. i.e., targeting all linear layers instead of just the attention blocks. Here, the \\nLoRA hyperparameters are r=8 and target_layers are  \\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'o_proj\\',\\'gate_proj\\',\\'down_proj\\',\\'up_\\nproj\\' and \\'lm_head\\'. This increases the number of parameters updated to 12,994,560 and increases the training \\ntime to roughly 15.5 minutes.\\nQualitative Evaluation \\nPrompting the model with the same prompts yield the following:\\nPrompt: \\nCreate a detailed description for the following product: Corelogic Smooth Mouse, belonging to category:  \\nOptical Mouse\\nResponse:\\nPrompt: \\nCreate a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless \\nVacuum Cleaner\\nResponse:\\nNow it is possible to see a somewhat longer coherent description of the fictitious optical mouse and there are \\nno logical flaws in the description of the vacuum cleaner. The product descriptions are not only logical, but \\nrelevant. Just as a reminder, these relatively high-quality results are obtained by fine-tuning less than a 1% of the \\nmodel’s weights with a total dataset of 5000 such prompt-description pairs formatted in a consistent manner.\\n \\n1\\n2\\n3\\nThe Corelogic Smooth Mouse is a wireless optical mouse with a smooth surface. The mouse is equipped with a 1000 DPI \\nsensor and a 1000 Hz polling rate. The mouse is equipped with a 2.4 GHz wireless connection and a USB receiver. The \\nmouse is available in black and white.\\n \\n1\\n2\\n3\\n4\\nThe Hoover Lightspeed cordless vacuum cleaner is a high-performance device that is easy to use. The device is \\nequipped with a lithium-ion battery that provides up to 40 minutes of runtime. The device is equipped with a \\nhigh-performance motor that provides a suction power of up to 200 watts. The device is equipped with a 2-in-1 dusting \\nbrush and a crevice tool. The device is equipped with a LED display that shows the remaining battery charge.\\n56THE BIG BOOK OF GENERATIVE AI\\nHYPERPARAMETER COMBINATION #4: LoRA with  r=8 and targeting all linear transformer layers\\nIt is also worth exploring whether the quality of output from the model improves if the pretrained model is \\nfrozen in 8-bit instead of 4-bit. In other words, replicating the exact fine-tuning process using LoRA instead of \\nQLoRA. Here, the LoRA hyperparameters are kept the same as before, in the new-found optimal configuration, \\ni.e., r=8 and targeting all linear transformer layers during the adaptation process.\\nQualitative Evaluation \\nThe results for the two prompts used throughout the article are as given below:\\nPrompt: \\nCreate a detailed description for the following product: Corelogic Smooth Mouse, belonging to category:  \\nOptical Mouse\\nResponse:\\nPrompt: \\nCreate a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless \\nVacuum Cleaner\\nResponse:\\nAgain, there isn’t much of an improvement in the quality of the output text. \\n \\n1\\n2\\n3\\nThe Corelogic Smooth Mouse is a high-quality optical mouse with a smooth surface. The mouse is equipped with a 1000 \\nDPI sensor and a 1000 Hz polling rate. The mouse is available in three colours: black, white and red. The mouse is \\nequipped with a USB receiver and is compatible with Windows and Mac.\\n \\n1\\n2\\n3 \\n4\\nThe Hoover Lightspeed cordless vacuum cleaner is a compact and lightweight device that is easy to use. The device is \\nequipped with a lithium-ion battery that provides up to 40 minutes of cleaning time. The vacuum cleaner is equipped \\nwith a high-performance filter that ensures that the air is cleaned of dust and allergens. The device is equipped \\nwith a 2-in-1 dusting brush and a crevice tool that can be used to clean hard-to-reach areas.\\n57THE BIG BOOK OF GENERATIVE AI\\nKEY OBSERVATIONS\\nBased on the above set of trials, and further evidence detailed in the excellent publication presenting QLoRA, \\nit can be deduced that the value of r (the rank of matrices updated during adaptation) does not improve \\nadaptation quality beyond a certain point. The biggest improvement is observed in targeting all linear layers \\nin the adaptation process, as opposed to just the attention blocks, as commonly documented in technical \\nliterature detailing LoRA and QLoRA. The trials executed above and other empirical evidence suggest that \\nQLoRA does not indeed suffer from any discernible reduction in quality of text generated, compared to LoRA.\\nFURTHER CONSIDERATIONS FOR USING LORA ADAPTERS IN DEPLOYMENT\\nIt\\'s important to optimize the usage of adapters and understand the limitations of the technique. The size of the \\nLoRA adapter obtained through fine-tuning is typically just a few megabytes, while the pretrained base model \\ncan be several gigabytes in memory and on disk. During inference, both the adapter and the pretrained LLM \\nneed to be loaded, so the memory requirement remains similar.\\nFurthermore, if the weights of the pre-trained LLM and the adapter aren’t merged, there will be a slight increase \\nin inference latency. Fortunately, with the PEFT library, the process of merging the weights with the adapter can \\nbe done with a single line of code as shown here:\\nThe figure below outlines the process from fine-tuning an adapter to model deployment.\\n \\n1 merged_model = peft_model.merge_and_unload()\\n58THE BIG BOOK OF GENERATIVE AI\\nWhile the adapter pattern offers significant benefits, merging adapters is not a universal solution. One \\nadvantage of the adapter pattern is the ability to deploy a single large pretrained model with task-specific \\nadapters. This allows for efficient inference by utilizing the pretrained model as a backbone for different \\ntasks. However, merging weights makes this approach impossible. The decision to merge weights depends on \\nthe specific use case and acceptable inference latency. Nonetheless, LoRA/ QLoRA continues to be a highly \\neffective method for parameter efficient fine-tuning and is widely used.\\nCONCLUSION\\nLow Rank Adaptation is a powerful fine-tuning technique that can yield great results if used with the right \\nconfiguration. Choosing the correct value of rank and the layers of the neural network architecture to target \\nduring adaptation could decide the quality of the output from the fine-tuned model. QLoRA results in further \\nmemory savings while preserving the adaptation quality. Even when the fine-tuning is performed,  there are \\nseveral important engineering considerations to ensure the adapted model is deployed in the correct manner.\\nIn summary, a concise table indicating the different combinations of LoRA parameters attempted, text quality \\noutput and number of parameters updated when fine-tuning OpenLLaMA-3b-v2 for 3 epochs on 5000 \\nobservations on a single A100 is shown below.\\nTry this on Databricks! Clone the GitHub repository associated with the blog into a Databricks Repo to get \\nstarted. More thoroughly documented examples to fine-tune models on Databricks are available here.\\nR TARGET_MODULES BASE MODEL \\nWEIGHTS QUALITY OF OUTPUT NUMBER OF PARAMETERS UPDATED  \\n(IN MILLIONS)\\n8 Attention blocks 4 low 2.662\\n16 Attention blocks 4 low 5.324\\n8 All linear layers 4 high 12.995\\n8 All linear layers 8 high 12.995\\n59THE BIG BOOK OF GENERATIVE AI\\nStage 4: Pretraining\\nPretraining a model from scratch refers to the process of training a language model on a large corpus of data \\n(e.g., text, code) without using any prior knowledge or weights from an existing model. This is in contrast to fine-\\ntuning, where an already pretrained model is further adapted to a specific task or dataset. The output of full \\npretraining is a base model that can be directly used or further fine-tuned for downstream tasks.\\nWHEN TO USE PRETRAINING\\nChoosing to pretrain an LLM from scratch is a significant commitment, both in terms of data and computational \\nresources. Here are some scenarios where it makes sense:\\n1. Unique data sources: If you possess a unique and extensive corpus of data that is distinct from what \\navailable pretrained LLMs have seen, it might be worth pretraining a model to capture this uniqueness\\n2. Domain specificity: Organizations might want a base model tailored to their specific domain (e.g., \\nmedical, legal, code) to ensure even the foundational knowledge of the model is domain-specific\\n3. Full control over training data: Pretraining from scratch offers transparency and control over the data \\nthe model is trained on. This may be essential for ensuring data security, privacy and custom tailoring of \\nthe model’s foundational knowledge.\\n4. Avoiding third-party biases: Pretraining ensures that your LLM application does not inherit biases or \\nlimitations from third-party pretrained models.\\n60THE BIG BOOK OF GENERATIVE AI\\nPRETRAINING IN PRACTICE\\nGiven the resource-intensive nature of pretraining, careful planning and sophisticated tooling are required. \\nLibraries like PyTorch FSDP and Deepspeed, mentioned in the fine-tuning section, are similarly required for \\ntheir distributed training capabilities when pretraining an LLM from scratch. The following only scratches the \\nsurface on some of the considerations one must take into account when pretraining an LLM: \\n ■ Large-scale data preprocessing: A pretrained model is only as good as the data it is trained on. Thus, \\nit becomes vitally important to ensure robust data preprocessing is conducted prior to model training. \\nGiven the scale of the training data involved, this preprocessing typically requires distributed frameworks \\nlike Apache Spark™. Consideration must be given to factors such as dataset mix and deduplication \\ntechniques to ensure the model is exposed to a wide variety of unique data points.\\n ■ Hyperparameter selection and tuning: Before executing full-scale training of an LLM, determining \\nthe set of optimal hyperparameters is crucial. Given the high computational cost associated with LLM \\ntraining, extensive hyperparameter sweeps are not always feasible. Instead, informed decisions based \\non smaller-scale searches or prior research are employed. Once a promising set is identified, these \\nhyperparameters are used for the full training run. Tooling like MLflow is essential to manage and track \\nthese experiments.\\n ■ Maximizing resource utilization: Given the high costs associated with long-running distributed GPU \\ntraining jobs, it is hugely important to maximize resource utilization. MosaicML’s composer is an example \\nof a library that uses PyTorch FSDP with additional optimizations to maximize Model FLOPs Utilization \\n(MFU) and Hardware FLOPs Utilization (HFU) during training.\\n ■ Handling GPU failures: Training large models can run for days or even weeks. During such large-scale \\ntraining for this length of time, hardware failures, especially GPU failures, can (and typically do) occur.  \\nIt is essential to have mechanisms in place to handle such failures gracefully. \\n ■ Monitoring and evaluation: Close monitoring of the training process is essential. Saving model \\ncheckpoints regularly and evaluating validation sets not only act as safeguards but also provide insights \\ninto model performance and convergence trends.\\nIn cases where pretraining an LLM from scratch is required, Mosaic AI Training provides a platform to conduct \\ntraining of multibillion-parameter models in a highly optimized and automated manner. Automatically handling \\nGPU failures and resuming training without human intervention and leveraging Mosaic AI Streaming for efficient \\nstreaming of data into the training process are just some of the capabilities provided out of the box.\\n61THE BIG BOOK OF GENERATIVE AI\\nThe Value of Training Models From Scratch on Databricks \\nAfter diving into the details of starting a model’s training from scratch, why you might do it and the advanced \\ntools needed, let’s look at a real-world example to show that training top-notch language models isn’t as \\ncomplex or expensive as it might seem. This shift highlights that even organizations watching their budget can \\nstart training their own models, with Databricks providing the necessary support and infrastructure. Databricks \\nstands out as uniquely capable to help customers train their own models from scratch, enabling them to fully \\nown their AI assets.\\nPretraining Use Cases\\nTraining Stable Diffusion From Scratch for <$50K With MosaicML\\nby Mihir Patel, Cory Stephenson, Landan Seguin, Austin Jacobson and Erica Ji Yuen\\nWe’ve replicated Stable Diffusion 2 for less than $50K, and we’ve open sourced the training code so you can \\ntoo! This is a 3x cost reduction from our last blog post and an 8x reduction from the original Stable Diffusion 2, \\nmaking training large-scale diffusion models from scratch more accessible than ever before.\\nToday, we are excited to show the results of our own training run: under $50K to train Stable Diffusion 2 base1 \\nfrom scratch in 7.45 days using the MosaicML platform.\\n62THE BIG BOOK OF GENERATIVE AI\\nFigure 1: Imagining mycelium couture. Integrating image generation into the design process pushes creative boundaries. All images in this mood board \\nwere created with our internal diffusion model trained from scratch on the MosaicML Platform.\\nTraining your own image generation model on your own data is now easy and accessible. By training your own \\ndiffusion models, you can:\\n ■ Use your proprietary data\\n ■ Tune the representations for certain art or photography styles\\n ■ Avoid violating intellectual property laws so your models can be used commercially\\nWe’ve open sourced our code and methods to train a diffusion model from scratch so that you can train your \\nown; check it out here! If you\\'re interested in training your own models, contact us for a demo, and read on to \\nlearn more about our engineering setup!\\n63THE BIG BOOK OF GENERATIVE AI\\nSETUP\\nModel: Our diffusion model is a ComposerModel \\ncomposed of a Variational Autoencoder (VAE), a \\nCLIP model, a U-Net, and a diffusion noise scheduler, \\nall from the HuggingFace\\'s Diffusers library. All of \\nthe model configurations were based on stabilityai/\\nstable-diffusion-2-base.\\nFigure 2: Getting creative and embracing serendipity. A variety of subjects, art, and photography styles are generated by our diffusion model.\\nFigure 3: Simplified diagram of the diffusion model.\\n64THE BIG BOOK OF GENERATIVE AI\\nData: We trained on a subset of LAION-5B that includes samples with English-only captions and an aesthetic \\nscore of 4.5+. Similar to Stable Diffusion 2 base, we did two phases of training based on the image resolution of \\nthe training data. For the first phase of training, we used all images with resolution >=256x256, amounting to 790 \\nmillion image-caption samples. For the second phase of training, we only used images with resolution >=512x512, \\namounting to 300 million image-caption samples.\\nCompute: Both phases of training ran on 128 NVIDIA A100 GPUs. The first training phase was run for 550k \\niterations in 1.6 days while the second phase was run for 850k iterations in 4.9 days, for a total of 20,051 A100 \\nhours for training. In addition to the training time, we pre-computed the latents for the VAE and CLIP model \\nto reduce training time and cost when making multiple passes over the dataset. Pre-computing the latents \\nrequired an additional 3,784 A100 hours, resulting in 23,835 A100 hours in total. Assuming a cost of $2 / A100 \\nhour, the total price tag is $47.7k.\\nTech Stack: We used Composer for our training framework, StreamingDataset to load our 100TB of data, and \\nthe MosaicML platform for overcoming infrastructure challenges when training and evaluating on 128 GPUs.\\nFigure 4: Loss curve for our training run. Our platform caught two hardware failures and automatically restarted the run with no human intervention. \\nThe loss discontinuity is because phase 2 increases the resolution from 256x256 to 512x512.\\n65THE BIG BOOK OF GENERATIVE AI\\nCHALLENGES AND SOLUTIONS\\nWhether for diffusion models or large language models, training at scale has significant challenges. We trained \\nour diffusion model using the MosaicML platform, which addresses these challenges automatically so you can \\nfocus on training the best possible model. Below are three main challenges with large-scale training and how our \\nplatform solves them.\\nINFRASTRUCTURE\\nTraining large models on large datasets requires significant compute. The MosaicML platform effortlessly \\norchestrates hundreds of GPUs on any cloud provider. For example, our primary training run took place on \\na cluster of 128 A100 GPUs. To ensure evaluating the model didn\\'t slow training, we automatically kicked off \\nevaluation runs at every checkpoint on different clusters using different cloud providers, seamlessly scaling up \\nto 64 GPUs and back down to 8 GPUs depending on availability.\\nEven after training is underway, software or hardware failures can halt training, leaving GPUs idle until someone \\nnotices or requiring someone on-call 24/7 to babysit the run. Thankfully, the Node Doctor and Watchdog \\nfeatures of the MosaicML platform automatically detect failed nodes and resume jobs as needed. With auto-\\nresumption, we recover from failures and continue training with zero human intervention, avoiding expensive \\ndowntime and human babysitting. Just launch and train!\\nEFFICIENT SOFTWARE\\nSoftware is difficult to configure optimally. Our PyTorch-based Composer library maximizes training efficiency \\nat scale. As shown in our previous blog post, Composer demonstrated excellent throughput scaling as the \\nnumber of GPUs increased. For this update, we added further optimizations (Low Precision GroupNorm and Low \\nPrecision LayerNorm, Fully Sharded Data Parallel) to achieve near-perfect strong scaling up to 128 GPUs, bringing \\nthe cost down to $50k. We also used Composer\\'s native Exponential Moving Average (EMA) algorithm, which \\nallowed us to start EMA close to the end of training (iteration 800k of the final phase) to gain all the benefits of \\nEMA while saving on memory and compute for the majority of training.\\n66THE BIG BOOK OF GENERATIVE AI\\nMANAGING 100TB OF DATA\\nWe trained with a subset of LAION-5B that contained 790 million samples, amounting to >100TB of data. The \\nsheer size of the dataset makes it difficult to manage, especially when working with multiple clusters with \\nseparate local storage. The MosaicML StreamingDataset library makes working with massive datasets much \\nsimpler and faster. There were three key features of the StreamingDataset library that were especially useful for \\nthis training run:\\n1. Mixing datasets stored in different locations. We bucketed samples based on image resolution into \\ndifferent datasets. At training time, we used the MosaicML StreamingDataset library to train on a mixture \\nof resolutions from these datasets.\\n2. Instant mid-epoch resumption. We were able to instantly resume training in the middle of an epoch. This \\nsaved hours by avoiding the need to iterate over the entire dataset to get back to where we left off.\\n3. Elastic determinism. The MosaicML StreamingDataset library deterministically shuffles data, even when \\nchanging the number of GPUs used for training. This made it possible for us to exactly reproduce training \\nruns, dramatically simplifying debugging.\\nHUMAN EVALUATION RESUL TS\\nEvaluating image generation models is difficult, and there is no substitute for human evaluation. In a blind human \\nevaluation, we measured user preferences in image quality and prompt alignment between Stable Diffusion 2 \\nand our diffusion model. Based on user preferences, we concluded that the two models were comparable in \\nquality (see Figure 5) All images were generated based on prompts from the Drawbench benchmark proposed \\nin the Imagen paper. For more details, see our follow-up blog post coming soon.\\n67THE BIG BOOK OF GENERATIVE AI\\nFigure 5: Results from our human evaluation of image quality (left) and prompt alignment (right). Error bars show 95% confidence intervals. In both ex-\\nperiments, the difference in user preference rates between the two models was comparable to the uncertainty in the measurement, so we conclude \\nthat the two models are of comparable overall quality.\\nDeep Dive: How We Trained Stable Diffusion for Less Than $50K \\nby Mihir Patel, Erica Ji Yuen, Cory Stephenson and Landan Seguin\\nIn our previous example, we showed how we used the MosaicML platform, Streaming datasets, and the \\nComposer library to train a Stable Diffusion model from scratch for less than $50,000. Now, we do a deep dive \\ninto the technical details behind this speedup, demonstrating how we were able to replicate the Stable Diffusion \\n2 base model in just 6.8 days.\\nTry out our code here!\\nMany organizations require high-performing large AI models tailored to their specific use cases. However, \\ntraining such models is often prohibitively time-consuming and expensive, requiring vast amounts of \\ncomputation and expertise. This is where MosaicML comes in: we provide a comprehensive solution that \\nsimplifies and accelerates the process of training these models.\\n68THE BIG BOOK OF GENERATIVE AI\\nIn our previous blog post, we announced that we have trained a diffusion model comparable to Stable Diffusion \\n2 from scratch for $47.7K. In this post, we dive into the technical details to highlight how we achieved an 8x \\nspeedup/cost reduction from the number reported by StabilityAI and a 3x cost reduction over our own \\nbaseline. All our code is open source and easy to modify for custom use cases. If you\\'re interested in learning \\nmore about our stack, please contact us for a demo.\\nACCELERATING TRAINING\\nWe’ve introduced a variety of techniques, from fusions to sharding strategies, that dramatically speed up \\ntraining and lower costs by almost 3x.\\nFigure 1: Stable Diffusion 2 model architecture. For training, the VAE image encoder, CLIP text encoder and U-Net are used. For inference,  \\nthe CLIP Text Encoder, U-Net, and VAE image decoder are used. Only the U-Net weights are updated during training; CLIP and VAE are fixed.\\n69THE BIG BOOK OF GENERATIVE AI\\nXFORMERS FLASHATTENTION\\nFigure 2: xFormers accelerates cross attention blocks in the U-Net.\\n70THE BIG BOOK OF GENERATIVE AI\\nThe attention layers in the Stable Diffusion architecture can be slow with a naive implementation, so most \\ncodebases use faster implementations that rely on fused kernels. In our stack, we leverage xFormers \\nFlashAttention.\\nWhile this was enabled in our original blog post, we found an issue with the usage that resulted in extra memory \\nbeing consumed on rank 0. After fixing this bug, we were able to increase our device microbatch size1 from 4 to \\n8. This yielded a sizable speedup, since A100s are more efficient at larger matrix sizes.\\nPRECOMPUTING LATENTS\\nFigure 3: Two phase training with precomputed latents. \\nFirst, all VAE and CLIP latents are precomputed and stored. \\nThen, the U-Net diffusion model is trained using these \\nprecomputed latents.\\n71THE BIG BOOK OF GENERATIVE AI\\nStable Diffusion is a combination of three models: a variational autoencoder (VAE), a text encoder (CLIP), and a \\nU-Net. During diffusion training, only the U-Net is trained, and the other two models are used to compute the \\nlatent encodings of the image and text inputs. Standard training involves computing the VAE and CLIP latents for \\nevery batch, but this does a lot of duplicate work when training for multiple epochs: latents are re-computed for \\neach image every time it is used. Instead, we precompute the latents once before training. Empirically, we have 2 \\nepochs at 256 resolution and 5 epochs at 512 resolution, so we avoid 6 extra VAE and CLIP calls per image-text \\npair in the dataset.\\nAdditionally, when pre-computing the latents, we can lower the precision of the VAE and CLIP models to \\nfp16. This could lead to numerical instability if we were training the VAE and CLIP and used this precision for \\nthe backward pass. However, since we\\'re only using them for inference, we can safely lower the precision, \\nwhich increases speed. The extra memory savings also let us use far larger batch sizes and improve hardware \\nutilization during the latent precomputation.\\n72THE BIG BOOK OF GENERATIVE AI\\nLOW PRECISION LAYERNORM AND GROUPNORM\\nFigure 4: Low Precision LayerNorm and Low Precision GroupNorm. Low precision gives faster training and lower memory usage, enabling larger \\nmicrobatches.\\n73THE BIG BOOK OF GENERATIVE AI\\nDiffusion training is done in automatic mixed precision by default. This uses half precision (fp16) in most \\nlayers, but fp32 in a few numerically unstable layers like normalization and softmax. The Stable Diffusion U-Net \\narchitecture uses several LayerNorm and GroupNorm layers, which by default are run in fp32.\\nMotivated by our finding that half precision LayerNorms are safe to use in language models, we decided to \\ntry out half precision LayerNorm and GroupNorm layers. This change resulted in identical loss curves and no \\ninstability in our experiments.\\nWhile we did observe some throughput improvement, the real benefit was decreased memory usage. Now, \\nalong with removing the VAE and CLIP memory by precomputing latents, we have enough space on our 40GB \\nA100 to increase our microbatch size from 8 to 16, 4x larger than what we started with!\\n74THE BIG BOOK OF GENERATIVE AI\\nFULLY SHARDED DATA PARALLELISM\\nFigure 5: Fully Sharded Data Parallel with SHARD_GRAD_OP speeds up the gradient update step and enables linear scaling.\\n75THE BIG BOOK OF GENERATIVE AI\\nMosaicML Composer, our go-to training library, includes support for PyTorch Fully Sharded Data Parallelism \\n(FSDP). We primarily use this to shard large scale models like 10B+ parameter LLMs that don\\'t fit in a single \\ndevice across hundreds of GPUs for incredibly fast training. Stable Diffusion doesn\\'t require sharding since it  \\nfits in a single GPU. However, some of the distributed features in FSDP are still useful for speeding up training  \\non a large number of GPUs.\\nWhen batches don’t fit into memory, we do several forward and backward passes on smaller microbatches, \\nfollowed by a single gradient update. If we use a small number of GPUs to train, we have far more forward and \\nbackward passes per gradient update, so the time spent on the gradient update doesn\\'t matter. However, at \\n128+ GPUs with a microbatch size of 16, we\\'re only doing one forward and one backward pass for each gradient \\nupdate. At this scale, the gradient update step starts to become a significant bottleneck.\\nTo tackle this problem, we use FSDP\\'s SHARD_GRAD_OP mode. In normal training, each GPU communicates all \\nits gradients to every other GPU, and then each GPU updates its local copy of the model. With this FSDP variant, \\neach GPU only gets the gradients and updates the weights for a small part of the model before sending the \\nupdated weights for that part of the model to all of the other GPUs. By dividing the update step across all the \\nGPUs, we can ensure the amount of work per GPU decreases as we increase the number of GPUs, helping us \\nachieve linear scaling.\\n76THE BIG BOOK OF GENERATIVE AI\\nSCHEDULED EMA\\nFigure 6: Loss curve of our training run with the scheduled exponential moving average (EMA) period highlighted.\\n77THE BIG BOOK OF GENERATIVE AI\\nStable Diffusion 2 uses Exponential Moving Averaging (EMA), which maintains an exponential moving average \\nof the weights. At every time step, the EMA model is updated by taking 0.9999 times the current EMA model \\nplus 0.0001 times the new weights after the latest forward and backward pass. By default, the EMA algorithm is \\napplied after every gradient update for the entire training period. However, this can be slow due to the memory \\noperations required to read and write all the weights at every step.\\nTo avoid this costly procedure, we start with a key observation: since the old weights are decayed by a factor of \\n0.9999 at every batch, the early iterations of training only contribute minimally to the final average. This means \\nwe only need to take the exponential moving average of the final few steps. Concretely, we train for 1,400,000 \\nbatches and only apply EMA for the final 50,000 steps, which is about 3.5% of the training period. The weights \\nfrom the first 1,350,000 iterations decay away by (0.9999)^50000, so their aggregate contribution would have \\na weight of less than 1% in the final model. Using this technique, we can avoid adding overhead for 96.5% of \\ntraining and still achieve a nearly equivalent EMA model.\\n78THE BIG BOOK OF GENERATIVE AI\\nFINAL TIME AND COST ESTIMATES\\nFigure 7: Throughput at 512x512 images on 128 GPUs as each speedup optimization is enabled. We achieve a total cumulative speedup of 2.71x over \\nthe baseline.\\nWe’ve shown how we obtained nearly a 3x reduction in time and cost to train Stable Diffusion compared to our \\noriginal results. With xFormers, precomputed latents, low precision LayerNorm, low precision GroupNorm, FSDP, \\nand scheduled EMA, Table 1 shows it\\'s possible to train Stable Diffusion in just 6.79 days using 21,000 A100-\\nhours for a total cost of less than $42,000. We estimated these times and costs by measuring throughput for \\ntraining 1.1 billion 256x256 images and 1.7 billion 512x512 images with a max tokenized length of 77 at a global \\nbatch size of 2048, as detailed in the Stable Diffusion 2 base model card. This is slightly cheaper than our \\npreviously reported run with a cost of $47.7k as it does not account for any time spent on evaluation or restarts \\ndue to hardware failures.\\n79THE BIG BOOK OF GENERATIVE AI\\nNUMBER  \\nOF A100S\\nTHROUGHPUT  \\nFOR U-NET \\n@ 256X256 \\n(IMAGES / \\nSECOND)\\nTHROUGHPUT \\nFOR U-NET \\n@ 512X512 \\n(IMAGES / \\nSECOND)\\nTHROUGHPUT \\nFOR U-NET @ \\n512X512 WITH \\nEMA (IMAGES /  \\nSECOND)\\nDAYS TO TRAIN \\nON MOSAICML \\nCLOUD\\nAPPROX. COST \\nON MOSAICML \\nCLOUD\\n8 1100 290 290 101.04 $38,800\\n16 2180 585 580 50.29 $38,630\\n32 4080 1195 1160 25.01 $38,420\\n64 8530 2340 2220 12.63 $38,800\\n128 11600 4590 3927 6.79 $41,710\\nTable 1: Estimated time and cost to train a Stable Diffusion model on 1.1 billion images at 256x256 resolution, followed by 1.7 billion images at 512x512 \\nresolution. Different rows show different numbers of NVIDIA 40GB A100 GPUs at a global batch size of 2048.\\nThese optimizations show that training image generation models from scratch is within reach for everyone. For \\nupdates on our latest work, join our Community Slack or follow us on Twitter. If your organization wants to start \\ntraining diffusion models today, please schedule a demo online or email us at demo@mosaicml.com.\\n1 When training large models with big batches that don\\'t fit in memory in a single pass, each batch is divided into smaller microbatches. On each \\ndevice, we can do a forward and backward pass for each microbatch and sum the gradients at the end to compute a gradient update equivalent to \\na single forward and backward pass with the entire batch all at once.\\n80THE BIG BOOK OF GENERATIVE AI\\nStage 5: LLM Evaluation\\nConstant evaluation and monitoring of deployed large language models (LLMs) and generative AI applications \\nare crucial due to the dynamic nature of both the data they interact with and the environments in which they \\noperate. These systems learn from vast datasets and can evolve over time, potentially leading to shifts in \\nperformance, accuracy or even the emergence of biases. Continuous monitoring ensures that any deviation \\nfrom expected behavior can be detected and corrected promptly, maintaining the integrity and reliability \\nof the AI application. As user needs and societal norms change, ongoing evaluation allows these models to \\nadapt, ensuring their outputs remain relevant, appropriate and effective. This vigilance not only mitigates risks \\nassociated with AI deployments, such as ethical concerns and regulatory compliance, but also maximizes the \\nvalue and utility these technologies bring to organizations and end users. \\nEvaluating LLMs is a challenging and evolving domain, primarily because LLMs often demonstrate uneven \\ncapabilities across different tasks. An LLM might excel in one benchmark, but slight variations in the prompt \\nor problem can drastically affect its performance. The dynamic nature of LLMs and their vast potential \\napplications only amplify the challenge of establishing comprehensive evaluation standards.\\n81THE BIG BOOK OF GENERATIVE AI\\nPresent challenges involved with evaluating LLM-powered applications include the following:\\n ■ Variable performance: LLMs can be sensitive to prompt variations, demonstrating high proficiency in \\none task but faltering with slight deviations in prompts.\\n ■ Lack of ground truth: Since most LLMs output natural language, it is very difficult to evaluate the outputs \\nvia traditional NLP metrics (BLEU, ROUGE, etc.). For example, suppose an LLM were used to summarize \\na news article. Two equally good summaries might have almost completely different words and word \\norders, so even defining a “ground truth” label becomes difficult or impossible.\\n ■ Domain-specific evaluation: For domain-specific fine-tuned LLMs, popular generic benchmarks \\nmay not capture their nuanced capabilities. Such models are tailored for specialized tasks, making \\ntraditional metrics less relevant. This divergence often necessitates the development of domain-specific \\nbenchmarks and evaluation criteria. See the example of Replit’s code generation LLM. \\n ■ Reliance on human judgment: It is often the case that LLM performance is being evaluated in domains \\nwhere text is scarce or there is a reliance on subject matter expert knowledge. In such scenarios, \\nevaluating LLM output can be costly and time-consuming.\\nTo help give examples of how this can be accomplished, here are two great examples of how you can monitor \\nand evaluate your deployed LLMs and generative AI applications using Databricks.\\nLLM Evaluation Examples\\nBest Practices for LLM Evaluation of RAG Applications  \\nA Case Study on the Databricks Documentation Bot\\nby Quinn Leng, Kasey Uhlenhuth and Alkis Polyzotis\\nChatbots are the most widely adopted use case for leveraging the powerful chat and reasoning capabilities \\nof large language models (LLM). The retrieval augmented generation (RAG) architecture is quickly becoming \\nthe industry standard for developing chatbots because it combines the benefits of a knowledge base (via a \\nvector store) and generative models (e.g., GPT-3.5 and GPT-4) to reduce hallucinations, maintain up-to-date \\ninformation, and leverage domain-specific knowledge. However, evaluating the quality of chatbot responses \\nremains an unsolved problem today. With no industry standards defined, organizations resort to human grading \\n(labeling) –which is time-consuming and hard to scale.\\n82THE BIG BOOK OF GENERATIVE AI\\nWe applied theory to practice to help form best practices for LLM automated evaluation so you can deploy RAG \\napplications to production quickly and with confidence. This blog represents the first in a series of investigations \\nwe’re running at Databricks to provide learnings on LLM evaluation. All research in this post was conducted by \\nQuinn Leng, Senior Software Engineer at Databricks and creator of the Databricks Documentation AI Assistant. \\nCHALLENGES WITH AUTO-EVALUATION IN PRACTICE\\nRecently, the LLM community has been exploring the use of “LLMs as a judge” for automated evaluation with \\nmany using powerful LLMs such as GPT-4 to do the evaluation for their LLM outputs. The lmsys group’s research \\npaper explores the feasibility and pros/cons of using various LLMs (GPT-4, ClaudeV1, GPT-3.5) as the judge for \\ntasks in writing, math, and world knowledge.\\nDespite all this great research, there are still many unanswered questions about how to apply LLM judges  \\nin practice:\\n ■ Alignment With Human Grading: Specifically for a document-Q&A chatbot, how well does an \\nLLM judge’s grading reflect the actual human preference in terms of correctness, readability and \\ncomprehensiveness of the answers? \\n ■ Accuracy Through Examples: What’s the effectiveness of providing a few grading examples to the LLM \\njudge and how much does it increase the reliability and reusability of the LLM judge on different metrics?\\n ■ Appropriate Grade Scales: What grading scale is recommended because different grading scales are \\nused by different frameworks (e.g., AzureML uses 0 to 100 whereas langchain uses binary scales)?\\n ■ Applicability Across Use Cases: With the same evaluation metric (e.g. correctness), to what extent can \\nthe evaluation metric be reused across different use cases (e.g. casual chat, content summarization, \\nretrieval augmented generation)?\\n83THE BIG BOOK OF GENERATIVE AI\\nAPPLYING EFFECTIVE AUTO-EVALUATION FOR RAG APPLICATIONS\\nWe explored the possible options for the questions outlined above in the context of our own chatbot \\napplication at Databricks. We believe that our findings generalize and can thus help your team effectively \\nevaluate RAG-based chatbots at a lower cost and faster speed:\\n ■ LLM-as-a-judge agrees with human grading on over 80% of judgments. Using LLMs-as-a-judge for our \\ndocument-based chatbot evaluation was as effective as human judges, matching the exact score in over \\n80% of judgments and being within a 1-score distance (using a scale of 0-3) in over 95% of judgments.\\n ■ Save costs by using GPT-3.5 with examples. GPT-3.5 can be used as an LLM judge if you provide \\nexamples for each grading score. Because of the context size limit it’s only practical to use a low-\\nprecision grading scale. Using GPT-3.5 with examples instead of GPT-4 drives down the cost of LLM judge \\nby 10x and improves the speed by more than 3x.\\n ■ Use low-precision grading scales for easier interpretation. We found lower-precision grading scores like 0, \\n1, 2, 3 or even binary (0, 1) can largely retain precision compared to higher precision scales like 0 to 10.0 or \\n0 to 100.0, while making it considerably easier to provide grading rubrics to both human annotators and \\nLLM judges. Using a lower precision scale also allows consistency of grading scales among different LLM \\njudges (e.g., between GPT-4 and claude2).\\n ■ RAG applications require their own benchmarks. A model might have good performance on a published \\nspecialized benchmark (e.g. casual chat, math, or creative writing) but that doesn’t guarantee good \\nperformance on other tasks (e.g. answering questions from a given context). Benchmarks should only be \\nused if the use case matches, i.e., a RAG application should only be evaluated with a RAG benchmark.\\nBased on our research, we recommend the following procedure when using an LLM judge: \\n ■ Use a 1-5 grading scale\\n ■ Use GPT-4 as an LLM judge with no examples to understand grading rules\\n ■ Switch your LLM judge to GPT-3.5 with one example per score\\n84THE BIG BOOK OF GENERATIVE AI\\nOUR METHODOLOGY FOR ESTABLISHING BEST PRACTICES\\nThe remainder of this post will walk through the series of experiments we conducted to form these  \\nbest practices. \\nEXPERIMENT SETUP\\n85THE BIG BOOK OF GENERATIVE AI\\n The experiment had three steps: \\n1. Generate evaluation dataset: We created a dataset from 100 questions and context from Databricks \\ndocuments. The context represents (chunks of) documents that are relevant to the question. \\n2. Generate answer sheets: Using the evaluation dataset, we prompted different language models to \\ngenerate answers and stored the question-context-answer pairs in a dataset called “answer sheets”. In \\nthis investigation, we used GPT-4, GPT-3.5, Claude-v1, Llama2-70b-chat, Vicuna-33b, and mpt-30b-chat.\\n3. Generate grades: Given the answer sheets, we used various LLMs to generate grades and reasoning \\nfor the grades. The grades are a composite score of Correctness (weighted: 60%), Comprehensiveness \\n(weighted: 20%) and Readability (weighted: 20%). We chose this weighting scheme to reflect our \\npreference for Correctness in the generated answers. Other applications may tune these weights \\ndifferently but we expect Correctness to remain a dominant factor.\\n86THE BIG BOOK OF GENERATIVE AI\\nAdditionally, the following techniques were used to avoid positional bias and improve reliability:\\n ■ Low temperature (temperature 0.1) to ensure reproducibility\\n ■ Single-answer grading instead of pairwise comparison\\n ■ Chain of thoughts to let the LLM reason about the grading process before giving the final score\\n ■ Few-shots generation where the LLM is provided with several examples in the grading rubric for each \\nscore value on each factor (Correctness, Comprehensiveness, Readability)\\nEXPERIMENT 1: ALIGNMENT WITH HUMAN GRADING\\nTo confirm the level of agreement between human annotators and LLM judges, we sent answer sheets  \\n(grading scale 0-3) from gpt-3.5-turbo and vicuna-33b to a labeling company to collect human labels,  \\nand then compared the result with GPT-4’s grading output. Below are the findings:\\nHuman and GPT-4 judges can reach above 80% agreement on the correctness and readability score.  \\nAnd if we lower the requirement to be smaller or equal than 1 score difference, the agreement level can  \\nreach above 95%. The Comprehensiveness metric has less alignment, which matches what we’ve heard  \\nfrom business stakeholders who shared that “comprehensive” seems more subjective than metrics like \\nCorrectness or Readability.\\n87THE BIG BOOK OF GENERATIVE AI\\nEXPERIMENT 2: ACCURACY THROUGH EXAMPLES\\nThe lmsys paper uses this prompt to instruct the LLM judge to evaluate based on the helpfulness, relevance, \\naccuracy, depth, creativity, and level of detail of the response. However, the paper doesn’t share specifics on the \\ngrading rubric. From our research, we found many factors can significantly affect the final score, for example:\\n ■ The importance of different factors: Helpfulness, Relevance, Accuracy, Depth, Creativity\\n ■ The interpretation of factors like Helpfulness is ambiguous \\n ■ If different factors conflict with each other, where an answer is helpful but is not accurate \\nWe developed a rubric for instructing an LLM judge for a given grading scale, by trying the following:\\n1. Original Prompt: Here is the original prompt used in the lmsys paper:\\nWe adapted the original lmsys paper prompt to emit our metrics about correctness, comprehensiveness and \\nreadability, and also prompt the judge to provide one line justification before giving each score (to benefit from \\nchain-of-thought reasoning). Below are the zero-shot version of the prompt which doesn’t provide any example, \\nand the few-shot version of the prompt which provides one example for each score. Then we used the same \\nanswer sheets as input and compared the graded results from the two prompt types.\\n2. Zero Shot Learning: Require the LLM judge to emit our metrics about correctness, comprehensiveness \\nand readability, and also prompt the judge to provide one line justification for each score.\\n \\nPlease act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question \\ndisplayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and \\nlevel of detail of the response. Begin your evaluation by providing a short explanation. Be as objective as possible. After \\nproviding your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format\\n \\nPlease act as an impartial judge and evaluate the quality of the provided answer which attempts to answer the provided \\nquestion based on a provided context. \\n  You\\'ll be given a function grading_function which you\\'ll call for each provided context, question and answer to submit your \\nreasoning and score for the correctness, comprehensiveness and readability of the answer\\n88THE BIG BOOK OF GENERATIVE AI\\n3. Few Shots Learning: We adapted the zero shot prompt to provide explicit examples for each score in the \\nscale. The new prompt:\\n \\nPlease act as an impartial judge and evaluate the quality of the provided answer which attempts to answer the provided \\nquestion based on a provided context.\\n  You\\'ll be given a function grading_function which you\\'ll call for each provided context, question and answer to submit your \\nreasoning and score for the correctness, comprehensiveness and readability of the answer. \\n  \\n  Below is your grading rubric: \\n- Correctness: If the answer correctly answer the question, below are the details for different scores:\\n  - Score 0: the answer is completely incorrect, doesn’t mention anything about the question or is completely contrary to the \\ncorrect answer.\\n      - For example, when asked “How to terminate a databricks cluster”, the answer is empty string, or content that’s \\ncompletely irrelevant, or sorry I don’t know the answer.\\n  - Score 1: the answer provides some relevance to the question and answers one aspect of the question correctly.\\n      - Example:\\n          - Question: How to terminate a databricks cluster\\n          - Answer: Databricks cluster is a cloud-based computing environment that allows users to process big data and run \\ndistributed data processing tasks efficiently.\\n          - Or answer:  In the Databricks workspace, navigate to the \"Clusters\" tab. And then this is a hard question that I \\nneed to think more about it\\n  - Score 2: the answer mostly answer the question but is missing or hallucinating on one critical aspect.\\n      - Example:\\n          - Question: How to terminate a databricks cluster”\\n          - Answer: “In the Databricks workspace, navigate to the \"Clusters\" tab.\\n          Find the cluster you want to terminate from the list of active clusters.\\n          And then you’ll find a button to terminate all clusters at once”\\n  - Score 3: the answer correctly answer the question and not missing any major aspect\\n      - Example:\\n          - Question: How to terminate a databricks cluster\\n          - Answer: In the Databricks workspace, navigate to the \"Clusters\" tab.\\n          Find the cluster you want to terminate from the list of active clusters.\\n          Click on the down-arrow next to the cluster name to open the cluster details.\\n          Click on the \"Terminate\" button. A confirmation dialog will appear. Click \"Terminate\" again to confirm the action.”\\n- Comprehensiveness: How comprehensive is the answer, does it fully answer all aspects of the question and provide \\ncomprehensive explanation and other necessary information. Below are the details for different scores:\\n  - Score 0: typically if the answer is completely incorrect, then the comprehensiveness is also zero score.\\n  - Score 1: if the answer is correct but too short to fully answer the question, then we can give score 1 for \\ncomprehensiveness.\\n      - Example:\\n          - Question: How to use databricks API to create a cluster?\\n          - Answer: First, you will need a Databricks access token with the appropriate permissions. You can generate this \\ntoken through the Databricks UI under the \\'User Settings\\' option. And then (the rest is missing)\\n  - Score 2: the answer is correct and roughly answer the main aspects of the question, but it’s missing description about \\ndetails. Or is completely missing details about one minor aspect.  \\n89THE BIG BOOK OF GENERATIVE AI\\n      - Example:\\n          - Question: How to use databricks API to create a cluster?\\n          - Answer: You will need a Databricks access token with the appropriate permissions. Then you’ll need to set up the \\nrequest URL, then you can make the HTTP Request. Then you can handle the request response.\\n      - Example:\\n          - Question: How to use databricks API to create a cluster?\\n          - Answer: You will need a Databricks access token with the appropriate permissions. Then you’ll need to set up the \\nrequest URL, then you can make the HTTP Request. Then you can handle the request response.\\n  - Score 3: the answer is correct, and covers all the main aspects of the question\\n- Readability: How readable is the answer, does it have redundant information or incomplete information that hurts the \\nreadability of the answer.\\n  - Score 0: the answer is completely unreadable, e.g. fully of symbols that’s hard to read; e.g. keeps repeating the words \\nthat it’s very hard to understand the meaning of the paragraph. No meaningful information can be extracted from the answer.\\n  - Score 1: the answer is slightly readable, there are irrelevant symbols or repeated words, but it can roughly form a \\nmeaningful sentence that cover some aspects of the answer.\\n      - Example:\\n          - Question: How to use databricks API to create a cluster?\\n          - Answer: You you  you  you  you  you  will need a Databricks access token with the appropriate permissions. And \\nthen then you’ll need to set up the request URL, then you can make the HTTP Request. Then Then Then Then Then Then Then Then \\nThen\\n  - Score 2: the answer is correct and mostly readable, but there is one obvious piece that’s affecting the readability \\n(mentioning of irrelevant pieces, repeated words)\\n      - Example:\\n          - Question: How to terminate a databricks cluster\\n          - Answer: In the Databricks workspace, navigate to the \"Clusters\" tab.\\n          Find the cluster you want to terminate from the list of active clusters.\\n          Click on the down-arrow next to the cluster name to open the cluster details.\\n          Click on the \"Terminate\" button…………………………………..\\n          A confirmation dialog will appear. Click \"Terminate\" again to confirm the action.\\n  - Score 3: the answer is correct and reader friendly, no obvious piece that affect readability.\\n- Then final rating:\\n    - Ratio: 60% correctness + 20% comprehensiveness + 20% readability\\nFrom this experiment, we learned several things:\\n ■ Using the Few Shots prompt with GPT-4 didn’t make an obvious difference in the consistency of results. \\nWhen we included the detailed grading rubric with examples we didn’t see a noticeable improvement in \\nGPT-4’s grading results across different LLM models. Interestingly, it caused a slight variance in the range \\nof the scores. \\n90THE BIG BOOK OF GENERATIVE AI\\n91THE BIG BOOK OF GENERATIVE AI\\n ■ Including few examples for GPT-3.5-turbo-16k significantly improves the consistency of the scores,  \\nand makes the result usable. Including detailed grading rubric/examples has very obvious improvement \\non the grading result from GPT-3.5. Though the actual average score value is slightly different between \\nGPT-4 and GPT-3.5 (score 3.0 vs score 2.6), the ranking and precision remains fairly consistent\\n ■ On the contrary, using GPT-3.5 without a grading rubric gets very inconsistent results and is  \\ncompletely unusable\\n ■ Note that we are using GPT-3.5-turbo-16k instead of GPT-3.5-turbo since the prompt can be larger than \\n4k tokens. \\n92THE BIG BOOK OF GENERATIVE AI\\nEXPERIMENT 3: APPROPRIATE GRADE SCALES\\nThe LLM-as-judge paper uses a non-integer 0~10 scale (i.e., float) for the grading scale; in other words, it uses \\na high precision rubric for the final score. We found these high-precision scales cause issues downstream with \\nthe following:\\n ■ Consistency: Evaluators–both human and LLM–struggled to hold the same standard for the same score \\nwhen grading on high precision. As a result, we found that output scores are less consistent across judges \\nif you move from low-precision to high-precision scales. \\n ■ Explainability: Additionally, if we want to cross-validate the LLM-judged results with human-judged \\nresults we must provide instructions on how to grade answers. It is very difficult to provide accurate \\ninstructions for each “score” in a high-precision grading scale–for example, what’s a good example for an \\nanswer that’s scored at 5.1 as compared to 5.6? \\n93THE BIG BOOK OF GENERATIVE AI\\nWe experimented with various low-precision grading scales to provide guidance on the “best” one to use, \\nultimately we recommend an integer scale of 0-3 or 0-4 (if you want to stick to the Likert scale). We tried  \\n0-10, 1-5, 0-3, and 0-1 and learned:\\n ■ Binary grading works for simple metrics like “usability” or “good/bad”.\\n ■ Scales like 0-10 are difficult to come up with distinguishing criteria between all scores.\\n94THE BIG BOOK OF GENERATIVE AI\\nAs shown in these plots, both GPT-4 and GPT-3.5 can retain consistent ranking of results using different  \\nlow-precision grading scales, thus using a lower grading scale like 0~3 or 1~5 can balance the precision  \\nwith explainability).\\nThus, we recommend 0-3 or 1-5 as a grading scale to make it easier to align with human labels, reason about \\nscoring criteria, and provide examples for each score in the range. \\n95THE BIG BOOK OF GENERATIVE AI\\nEXPERIMENT 4: APPLICABILITY ACROSS USE CASES\\nThe LLM-as-judge paper shows that both LLM and human judgment ranks the Vicuna-13B model as a close \\ncompetitor to GPT-3.5:\\nHowever, when we benchmarked the set of models for our document Q&A use cases, we found that even the \\nmuch larger Vicuna-33B model has a noticeably worse performance than GPT-3.5 when answering questions \\nbased on context. These findings are also verified by GPT-4, GPT-3.5 and human judges (as mentioned in \\nExperiment 1) which all agree that Vicuna-33B is performing worse than GPT-3.5.\\nFigure 4: Average win rate of nine models under different judges on Chatbot Arena.\\n96THE BIG BOOK OF GENERATIVE AI\\nWe looked closer at the benchmark dataset proposed by the paper and found that the 3 categories of tasks \\n(writing, math, knowledge) don’t directly reflect or contribute to the model’s ability to synthesize an answer \\nbased on a context. Instead, intuitively, document Q&A use cases need benchmarks on reading comprehension \\nand instruction following. Thus evaluation results can’t be transferred between use cases and we need to build \\nuse-case-specific benchmarks in order to properly evaluate how good a model can meet customer needs.\\n97THE BIG BOOK OF GENERATIVE AI\\nUSE MLFLOW TO LEVERAGE OUR BEST PRACTICES\\nWith the experiments above, we explored how different factors can significantly affect the evaluation of a \\nchatbot and confirmed that LLM as a judge can largely reflect human preferences for the document Q&A use \\ncase. At Databricks, we are evolving the MLflow Evaluation API to help your team effectively evaluate your LLM \\napplications based on these findings. MLflow 2.4 introduced the Evaluation API for LLMs to compare various \\nmodels’ text output side-by-side, MLflow 2.6 introduced LLM-based metrics for evaluation like toxicity and \\nperplexity, and we’re working to support LLM-as-a-judge in the near future!\\nIn the meantime, we compiled the list of resources we referenced in our research below:\\n ■ Doc_qa repository\\n ■ The code and data we used to conduct the experiments\\n ■ LLM-as-Judge Research paper from lmsys group \\n ■ The paper is the first research for using LLM as judge for the casual chat use cases, it extensively \\nexplored the feasibility and pros and cons of using LLM (GPT-4, ClaudeV1, GPT-3.5) as the judge for \\ntasks in writing, math, world knowledge\\nOffline LLM Evaluation: Step-by-Step GenAI Application Assessment on Databricks\\nby Abe Omorogbe, Liang Zhang, Sunish Sheth, Corey Zumar, Maheswaran Venkatachalam, Emil Lysgaard  \\nand Mathias Christiansen\\nBACKGROUND\\nIn an era where retrieval augmented generation (RAG) is revolutionizing the way we interact with AI-driven \\napplications, ensuring the efficiency and effectiveness of these systems has never been more essential. \\nDatabricks and MLflow are at the forefront of this innovation, offering streamlined solutions for the critical \\nevaluation of GenAI applications. \\nThis blog post guides you through the simple and effective process of leveraging the Databricks Data \\nIntelligence Platform to enhance and evaluate the quality of the three core components of your GenAI \\napplications: Prompts, Retrieval System, and Foundation LLM, ensuring that your GenAI applications continue  \\nto generate accurate results.\\n98THE BIG BOOK OF GENERATIVE AI\\nUSE CASE\\nWe are going to be creating a QA chatbot that will answer questions from the MLflow documentation and then \\nevaluate the results.\\n99THE BIG BOOK OF GENERATIVE AI\\nSET UP EXTERNAL MODELS IN DATABRICKS\\nDatabricks Model Serving feature can be used to manage, govern, and access external models from various \\nlarge language model (LLM) providers, such as Azure OpenAI GPT, Anthropic Claude, or AWS Bedrock, within \\nan organization. It offers a high-level interface that simplifies the interaction with these services by providing a \\nunified endpoint to handle specific LLM related requests.\\nMajor advantages of using Model Serving:\\n ■ Query Models Through a Unified Interface:  Simplifies the interface to call multiple LLMs in your \\norganization. Query models through a unified OpenAI-compatible API and SDK and manage all models \\nthrough a single UI.\\n ■ Govern and Manage Models: Centralizes endpoint management of multiple LLMs in your organization.  \\nThis includes the ability to manage permissions and track usage limits.\\n ■ Central Key Management: Centralizes API key management in a secure location, which enhances \\norganizational security by minimizing key exposure in the system and code, and reduces the burden  \\non end-users.\\n100THE BIG BOOK OF GENERATIVE AI\\nCREATE A SERVING ENDPOINT WITH AN EXTERNAL MODEL IN DATABRICKS\\n \\n1\\n2\\n3 \\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n23\\n25\\n26\\nimport mlflow\\nimport mlflow.deployments\\nclient = mlflow.deployments.get_deploy_client(\"databricks\")\\nendpoint_name = f\"test-endpoint-{uuid.uuid4()}\"\\nclient.create_endpoint(\\nname=endpoint_name,\\nconfig={\\n        \"served_entities\": [\\n            {\\n                \"name\": \"test\",\\n                \"external_model\": {\\n                    \"name\": \"gpt-3.5-turbo-instruct\",\\n                    \"provider\": \"openai\",\\n                    \"task\": \"llm/v1/completions\",\\n                    \"openai_config\": {\\n                        \"openai_api_type\": \"azure\",\\n                        \"openai_api_key\": \"{{secrets/<your-scope-name>/<your-key-name>}}\", ## Use Databricks Secrets. \\n                        \"openai_api_base\": \"https://<your-endpoint>.openai.azure.com/\",\\n                        \"openai_deployment_name\": \"<your-deployment-name>\",\\n                        \"openai_api_version\": \"2023-05-15\",\\n                    },\\n                },\\n            }\\n        ],\\n     },\\n)\\n101THE BIG BOOK OF GENERATIVE AI\\nEXPLORE PROMPTS WITH THE DATABRICKS AI PLAYGROUND\\nIn this section, we will understand: How well do different prompts perform with the chosen LLM?\\nWe recently introduced the Databricks AI Playground, which provides a best-in-class experience for crafting the \\nperfect prompt. With no code required, you can try out multiple LLMs served as Endpoints in Databricks, and \\ntest different parameters and prompts.\\nMajor advantages of the Databricks AI Playground are:\\n ■ Quick Testing: Quickly test deployed models directly in Databricks.\\n ■ Easy Comparison: Central location to compare multiple models on different prompts and parameters for \\ncomparison and selection.\\nUSING DATABRICKS AI PLAYGROUND\\nWe delve into testing relevant prompts with OpenAI GPT 3.5 Turbo, leveraging the Databricks AI Playground. \\n102THE BIG BOOK OF GENERATIVE AI\\nCOMPARING DIFFERENT PROMPTS AND PARAMETERS\\nIn the Playground, you are able to compare the output of multiple prompts to see which gives better results. \\nDirectly in the Playground, you can try several prompts,  models, and parameters to figure out which \\ncombination provides the best results. The model and parameters combo can then be added to the GenAI  \\napp and used for answer generation with the right context.\\n103THE BIG BOOK OF GENERATIVE AI\\nADDING MODEL AND PARAMETERS TO YOUR GENAI APPLICATION\\nAfter playing with a few prompts and parameters, you can use the same settings and model in your  \\nGenAI application.\\n104THE BIG BOOK OF GENERATIVE AI\\nExample of how to import the same external model in LangChain. We will cover how we turn this into a GenAI \\nPOC in the next section.\\nCREATE GENAI POC WITH LANGCHAIN AND LOG WITH MLFLOW\\nNow that we have found a good model and prompt parameters for your use case, we are going to create a \\nsample GenAI app that is a QA chatbot that will answer questions from the MLflow documentation using a \\nvector database, embedding model with the Databricks Foundation Model API and Azure OpenAI GPT 3.5 as \\nthe generation model.\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\nfrom langchain.llms import Databricks\\nllm = Databricks(\\n    endpoint_name=\"<endpoint-name>\",\\n    extra_params={\"temperature\": 0.1,\\n                 \"top_p\": 0.1,\\n                 \"max_tokens\": 500,\\n                 } #parameters used in AI Playground\\n)\\n105THE BIG BOOK OF GENERATIVE AI\\nCREATE A SAMPLE GENAI APP WITH LANGCHAIN USING DOCS FROM THE MLFLOW WEBSITE\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\n27\\n28\\n29\\n30\\n31\\n32\\n33\\n34\\n35\\n36\\n37\\nimport os\\nimport pandas as pd\\nimport mlflow\\nimport chromadb\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.document_loaders import WebBaseLoader\\nfrom langchain.llms import Databricks\\nfrom langchain.embeddings.databricks import DatabricksEmbeddings\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\\nloader = WebBaseLoader(\\n    [ \\n     \"https://mlflow.org/docs/latest/index.html\",\\n     \"https://mlflow.org/docs/latest/tracking/autolog.html\", \\n     \"https://mlflow.org/docs/latest/getting-started/tracking-server-overview/index.html\",\\n     \"https://mlflow.org/docs/latest/python_api/mlflow.deployments.html\" ])\\ndocuments = loader.load()\\nCHUNK_SIZE = 1000\\ntext_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=0)\\ntexts = text_splitter.split_documents(documents)\\nllm = Databricks(\\n    endpoint_name=\"<endpoint-name>\",\\n    extra_params={\"temperature\": 0.1,\\n                 \"top_p\": 0.1,\\n                 \"max_tokens\": 500,\\n                 } #parameters used in AI Playground\\n)\\n# create the embedding function using Databricks Foundation Model APIs\\nembedding_function = DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\")\\ndocsearch = Chroma.from_documents(texts, embedding_function)\\nqa = RetrievalQA.from_chain_type(\\n    llm=llm,\\n    chain_type=\"stuff\",\\n    retriever=docsearch.as_retriever(fetch_k=3),\\n    return_source_documents=True,\\n)\\n106THE BIG BOOK OF GENERATIVE AI\\nFor customers wanting to scale the retriever used in their GenAI application, we advise using Databricks Vector \\nSearch, a serverless similarity search engine that allows you to store a vector representation of your data, \\nincluding metadata, in a vector database.\\nEVALUATION OF RETRIEVAL SYSTEM WITH MLFLOW\\nIn this section, we will understand: How well does the retriever work with a given query?\\nIn MLflow 2.9.1, Evaluation for retrievers was introduced and provides a way for you to assess the efficiency \\nof their retriever with the MLflow evaluate API. You can use this API to evaluate the effectiveness of your \\nembedding model, the top K threshold choice, or the chunking strategy.\\nCREATING A GROUND TRUTH DATASET\\nCurating a ground truth dataset for evaluating your GenAI often involves the meticulous task of manually \\nannotating test sets, a process that demands both time and domain expertise. In this blog, we’re taking a \\ndifferent route. We’re leveraging the power of an LLM to generate synthetic data for testing, offering a quick-\\nstart approach to get a sense of your GenAI app’s retrieval capability, and a warm-up for all the in-depth \\nevaluation work that may follow. To our readers and customers, we emphasize the importance of crafting a \\ndataset that mirrors the expected inputs and outputs of your GenAI application. It’s a journey worth taking for \\nthe incredible insights you’ll gain!\\nYou can explore with the full dataset but let\\'s demo with a subset of the generated data. The question column \\ncontains all the questions that will be evaluated and the source column is the expected source for the answer \\nfor the questions as an ordered list of strings.\\n107THE BIG BOOK OF GENERATIVE AI\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\neval_data = pd.DataFrame(\\n    {\\n        \"question\": [\\n            \"What is MLflow?\",\\n            \"What is Databricks?\",\\n            \"How to serve a model on Databricks?\",\\n            \"How to enable MLflow Autologging for my workspace by default?\",\\n        ],\\n        \"source\": [\\n            [\"https://mlflow.org/docs/latest/index.html\"],\\n            [\"https://mlflow.org/docs/latest/getting-started/tracking-server-overview/index.html\"],\\n            [\"https://mlflow.org/docs/latest/python_api/mlflow.deployments.html\"],\\n            [\"https://mlflow.org/docs/latest/tracking/autolog.html\"],\\n        ],\\n    }\\n)\\nEVALUATE THE EMBEDDING MODEL WITH MLFLOW\\nThe quality of your embedding model is pivotal for accurate retrieval. In MLflow 2.9.0, we introduced three built-\\nin metrics mlflow.metrics.precision_at_k(k), mlflow.metrics.recall_at_k(k) and mlflow.metrics.ndcg_at_k(k) \\nto help determine how effective your retriever is at predicting the most relevant results for you. For example; \\nSuppose the vector database returns 10 results (k=10), and out of these 10 results, 4 are relevant to your query. \\nThe precision_at_10 would be 4/10 or 40%. \\n108THE BIG BOOK OF GENERATIVE AI\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\ndef evaluate_embedding(embedding_function):\\n    CHUNK_SIZE = 1000\\n    list_of_documents = loader.load()\\n    text_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=0)\\n    docs = text_splitter.split_documents(list_of_documents)\\n    retriever = Chroma.from_documents(docs, embedding_function).as_retriever()\\n    def retrieve_doc_ids(question: str) -> List[str]:\\n        docs = retriever.get_relevant_documents(question)\\n        doc_ids = [doc.metadata[\"source\"] for doc in docs]\\n        return doc_ids\\n    def retriever_model_function(question_df: pd.DataFrame) -> pd.Series:\\n        return question_df[\"question\"].apply(retrieve_doc_ids)\\n    with mlflow.start_run() as run:\\n        evaluate_results = mlflow.evaluate(\\n                model=retriever_model_function,\\n                data=eval_data,\\n                model_type=\"retriever\",\\n                targets=\"source\",\\n                evaluators=\"default\",\\n            )\\n    return evaluate_results\\nresult1 = evaluate_embedding(DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\"))result2 = evaluate_embed-\\nding(<another-embedding-function>)\\neval_results_of_retriever_df_bge = result1.tables[\"eval_results_table\"]\\ndisplay(eval_results_of_retriever_df_bge)\\n109THE BIG BOOK OF GENERATIVE AI\\nThe evaluation will return a table with the results of your evaluation for each question. i.e., for this test, we can \\nsee that the retriever seems to performing great for the questions \"How to enable MLflow Autologging for my \\nworkspace by default?” with a Precision @ K score is 1, and is not retrieving any of the right documentation for \\nthe questions \"What is MLflow?” since the precision @ K score is 0. With this insight, we can debug the retriever \\nand improve the retriever for questions like “What is MLflow?”\\nEvaluation results when using databricks-bge-large-en embedding model\\nEVALUATE RETRIEVER WITH DIFFERENT TOP K VALUES WITH MLFLOW\\nYou can quickly calculate the metrics for different Ks by specifying the extra_metrics argument.\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\nwith mlflow.start_run() as run:\\n        evaluate_results = mlflow.evaluate(\\n        data=eval_results_of_retriever_df_bge,\\n        targets=\"source\",\\n        predictions=\"outputs\",\\n        evaluators=\"default\",\\n        extra_metrics=[\\n            mlflow.metrics.precision_at_k(1),\\n            mlflow.metrics.precision_at_k(2),\\n            mlflow.metrics.precision_at_k(3),\\n            mlflow.metrics.recall_at_k(1),\\n            mlflow.metrics.recall_at_k(2),\\n            mlflow.metrics.recall_at_k(3),\\n            mlflow.metrics.ndcg_at_k(1),\\n            mlflow.metrics.ndcg_at_k(2),\\n            mlflow.metrics.ndcg_at_k(3),\\n        ],\\n    )\\ndisplay(evaluate_results.tables[\"eval_results_table\"])\\n110THE BIG BOOK OF GENERATIVE AI\\nThe evaluation will return a table with the results of your evaluation for each question, and you can better \\nunderstand which K value to use when retrieving documents. i.e., for this test we can see changing the top K \\nvalue can positively affect the precision of the retriever for questions like “What is Databricks?”\\nEvaluation result with all precision at K values\\n111THE BIG BOOK OF GENERATIVE AI\\nEVALUATE THE CHUNKING STRATEGY WITH MLFLOW\\nThe effectiveness of your chunking strategy is critical. We explore how MLflow can assist in this evaluation, \\nfocusing on the retrieval model type and its impact on overall performance.\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\ndef evaluate_chunk_size(chunk_size):\\n  list_of_documents = loader.load()\\n  text_splitter = CharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=0)\\n  docs = text_splitter.split_documents(list_of_documents)\\n  embedding_function = DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\")\\n  retriever = Chroma.from_documents(docs, embedding_function).as_retriever()\\n  \\n  def retrieve_doc_ids(question: str) -> List[str]:\\n    docs = retriever.get_relevant_documents(question)\\n    doc_ids = [doc.metadata[\"source\"] for doc in docs]\\n    return doc_ids\\n   \\n  def retriever_model_function(question_df: pd.DataFrame) -> pd.Series:\\n    return question_df[\"question\"].apply(retrieve_doc_ids)\\n  with mlflow.start_run() as run:\\n      evaluate_results = mlflow.evaluate(\\n          model=retriever_model_function,\\n          data=eval_data,\\n          model_type=\"retriever\",\\n          targets=\"source\",\\n          evaluators=\"default\",\\n      )\\n  return evaluate_results\\nresult1 = evaluate_chunk_size(500)\\nresult2 = evaluate_chunk_size(2000)\\ndisplay(result1.tables[\"eval_results_table\"])\\ndisplay(result2.tables[\"eval_results_table\"])\\n112THE BIG BOOK OF GENERATIVE AI\\nThe evaluation will return 2 tables with the results of your evaluation for each question using 2 different chunk \\nsizes, and you can better understand which chunk size to use when retrieving documents (i.e., for this example, \\nit seems like changing the chunk size did not affect any metric).\\nEvaluation result with Chunk size of 1000\\nEvaluation result with Chunk size of 2000\\nCheck out the in-depth notebook on retrieval evaluation\\n113THE BIG BOOK OF GENERATIVE AI\\nEVALUATION OF GENAI RESUL TS WITH MLFLOW\\nIn this section, we will understand: How good is the response of the GenAI app with a given prompt and context?\\nAssessing the quality of generated responses is key. We will augment the manual process of evaluating with \\nquestions and answers by leveraging MLflow\\'s QA metrics, and comparing them against a GPT-4 model as a \\nbenchmark to understand the effectiveness of the generated answers. \\nUsing an LLM like GPT-4 as a judge to assist in evaluation can offer several benefits, here are some key benefits:\\n ■ Rapid and Scalable Experimentation:  In many situations, we think LLM judges represent a sweet-spot: \\nthey can evaluate unstructured outputs (like a response from a chat-bot) automatically, rapidly, and  \\nat low-cost.  \\n ■ Cost-Effective: By automating some evaluations with LLMs, we consider it a worthy companion to human \\nevaluation, which is slower and more expensive but represents the gold standard of model evaluation.\\nUSE MLFLOW EVALUATE AND LLM AS A JUDGE\\nWe take some sample questions and use the LLM as a judge, and inspect the results with MLflow, providing a \\ncomprehensive analysis of the outcome with built-in metrics. We are going to judge the GenAI app on relevance \\n(how relevant is the output with respect to both the input and the context).\\nCreate a simple function that runs each input through the chain\\n \\n1\\n2\\ndef model(input_df):\\n    return input_df[\"questions\"].map(qa).tolist()\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\neval_df = pd.DataFrame(\\n    {\\n        \"questions\": [\\n            \"What is MLflow?\",\\n            \"What is Databricks?\",\\n            \"How to serve a model on Databricks?\",\\n            \"How to enable MLflow Autologging for my workspace by default?\",\\n        ],\\n    }\\n)\\n114THE BIG BOOK OF GENERATIVE AI\\nUse relevance metric to determine the relevance of the answer and context. There are other metrics you can \\nuse too.\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\nfrom mlflow.deployments import set_deployments_target\\nfrom  mlflow.metrics.genai.metric_definitions import relevance\\nset_deployments_target(\"databricks\") #To retrieve all endpoint in your Databricks Workspace\\nrelevance_metric = relevance(model=f\"endpoints:/{endpoint_name}\") #You can also use any model you have hosted on Da-\\ntabricks, models from the Marketplace or models in the Foundation model API\\nwith mlflow.start_run():\\n    results =  mlflow.evaluate(\\n        model,\\n        eval_df,\\n        model_type=\"question-answering\",\\n        evaluators=\"default\",\\n        predictions=\"result\",\\n        extra_metrics=[relevance_metric, mlflow.metrics.latency()],\\n        evaluator_config={\\n            \"col_mapping\": {\\n                \"inputs\": \"questions\",\\n                \"context\": \"source_documents\",\\n            }\\n        }\\n    )\\n    print(results.metrics)\\n115THE BIG BOOK OF GENERATIVE AI\\nIn your Databricks Workspace, you can compare and evaluate all your inputs and outputs, as well as the source \\ndocuments, relevance and any other metrics you added to your evaluation function.\\nCheck out more in depth notebooks on LLM evaluation\\n116THE BIG BOOK OF GENERATIVE AI\\nSummary Whether you’re looking to disrupt traditional industries, enhance creative endeavors or solve complex problems \\nin novel ways, the potential applications of generative AI are limited only by your imagination and willingness to \\nexperiment. Remember, every significant advancement in this field began with a simple idea and the courage to \\nexplore it further.\\nFor those seeking more knowledge or simply curious about the latest developments in the realm of generative \\nAI, we’ve provided some resources on training, demos and product information. \\nGenAI Training\\nGenerative AI Engineer Learning Pathway: Take self-paced, on-demand and instructor-led courses on \\ngenerative AI\\nFree LLM Course (edX): In-depth course to learn GenAI and LLMs inside and out\\nGenAI Webinar: Learn how to take control of your GenAI app performance, privacy and cost, and drive value \\nwith generative AI\\nAdditional Resources\\nBig Book of MLOps: A deep dive into the architectures and technologies behind MLOps — including LLMs  \\nand GenAI \\nMosaic AI: Product page covering the features of Mosaic AI within Databricks\\n117Build Production-Quality GenAI Applications — See How\\nCreate high-quality generative AI applications and ensure your output is accurate, \\ngoverned and safe. See why over 10,000 organizations worldwide rely on Databricks for \\nall their workloads from BI to AI — test-drive the full Databricks Platform free for 14 days.\\nAbout Databricks\\nDatabricks is the data and AI company. More than 10,000 organizations worldwide — \\nincluding Comcast, Condé Nast, Grammarly and over 50% of the Fortune 500 — rely on \\nthe Databricks Data Intelligence Platform to unify and democratize data, analytics and \\nAI. Databricks is headquartered in San Francisco, with offices around the globe, and was \\nfounded by the original creators of Lakehouse, Apache Spark™, Delta Lake and MLflow.  \\nTo learn more, follow Databricks on LinkedIn, X and Facebook.\\nTry Databricks free Take Generative AI Fundamentals On-Demand Training\\n© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark and the Spark \\nlogo are trademarks of the Apache Software Foundation . Privacy Policy  | Terms of Use'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(Question_gen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jxr5l1BwpJJ",
        "outputId": "0923b18f-ba8f-49a7-c130-6db26cb21950"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "164153"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import TokenTextSplitter"
      ],
      "metadata": {
        "id": "BA1Ag0jvyPQ5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitter_ques_gen = TokenTextSplitter(\n",
        "    encoding_name= \"cl100k_base\",\n",
        "    chunk_size= 10000,\n",
        "    chunk_overlap = 200\n",
        ")"
      ],
      "metadata": {
        "id": "icxmxzt82Igs"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chuck_ques_gen=splitter_ques_gen.split_text(Question_gen)"
      ],
      "metadata": {
        "id": "RZekVUMT-C0P"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chuck_ques_gen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kc60OKI-KZT",
        "outputId": "230ba979-a495-413c-ddd4-972dcdbc08f4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(chuck_ques_gen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bIclx7j-gZ2",
        "outputId": "e9a47a25-0ac0-4fc8-b554-9fb31ae60e21"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "splitter_que = CharacterTextSplitter(\n",
        "    chunk_size=1000,   # number of characters, not tokens\n",
        "    chunk_overlap=200\n",
        ")\n",
        "\n",
        "chunks = splitter_que.split_text(Question_gen)\n"
      ],
      "metadata": {
        "id": "GoDRVgin2MNa"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUdFki_s9CpU",
        "outputId": "273cbe2e-872d-40a5-f0b4-fe0a6a424d7a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(chunks[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_Omt6vX9tCo",
        "outputId": "237c8bac-3479-410a-9ed2-c8ace2ff9818"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document"
      ],
      "metadata": {
        "id": "XhSKOqYF-bPS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_ques_gen=[Document(page_content=t)for t in chunks]"
      ],
      "metadata": {
        "id": "XIH8pSGx-pl5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_ques_gen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLHsDS8O-76k",
        "outputId": "9fbac3a7-8ffc-441f-cb91-bdc700beca23"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={}, page_content='THE BIG BOOK OF GENERATIVE AI\\nCONTENTS\\nIntroduction  ......................................................................................................................................................................................................... 3\\nThe Path to Deploying Production-Quality GenAI Applications ........................................................................................... 5 \\nStage 0: Foundation Models  ........................................................................................................................................................................................................................................................... 5 \\n Use Case: Introducing DBRX: A New State-of-the-Art Open LLM  ................................................................................................................................................................... 5 \\nStage 1: Prompt Engineering  ......................................................................................................................................................................................................................................................... 19 \\n Use Case: Automated Analysis of Product Reviews Using Large Language Models ........................................................................................................................ 20 \\nStage 2: Retrieval Augmented Generation  ....................................................................................................................................................................................................................... 25 \\n Use Case: Improve Your RAG Application Response Quality With Real-Time Structured Data  ................................................................................................ 27 \\nStage 3: Fine-Tuning a Foundation Model  ......................................................................................................................................................................................................................... 33 \\n Use Case: Creating a Bespoke LLM for AI-Generated Documentation  ..................................................................................................................................................... 34 \\n Use Case: Efficient Fine-Tuning With LoRA: A Guide to Optimal Parameter Selection for Large Language Models  ................................................... 43 \\nStage 4: Pretraining  ........................................................................................................................................................................................................................................................................... 60 \\n Use Case: Training Stable Diffusion From Scratch for <$50K With MosaicML  ..................................................................................................................................... 62 \\n Use Case: Deep Dive: How We Trained Stable Diffusion for Less Than $50K  ........................................................................................................................................ 68 \\nStage 5: LLM Evaluation  .................................................................................................................................................................................................................................................................... 81 \\n Use Case : Best Practices for LLM Evaluation of RAG Application  ................................................................................................................................................................. 82 \\n Use Case: Offline LLM Evaluation: Step-by-Step GenAI Application Assessment on Databricks ........................................................................................... 98\\nSummary  ............................................................................................................................................................................................................. 117 \\nGenAI Training  ......................................................................................................................................................................................................................................................................................... 117 \\nAdditional Resources  ........................................................................................................................................................................................................................................................................ 117\\n2THE BIG BOOK OF GENERATIVE AI\\nAchieving Production-Quality GenAI Requires New Tools and Skills\\nGenerative AI has opened new worlds of possibilities for businesses and is being emphatically embraced \\nacross organizations. According to a recent MIT Tech Review report, all 600 CIOs surveyed stated they are \\nincreasing their investment in AI, and 71% are planning to build their own custom large language models (LLMs) \\nor other GenAI models. However, many organizations have found it challenging to deploy these applications at \\nproduction quality. To meet the standard of quality required for customer-facing applications, AI output must \\nbe accurate, governed and safe. \\nData Infrastructure Must Evolve to Support  \\nGenAI-Powered Applications\\nMaking the leap to generative AI is not just about deploying a chatbot; it requires a reshaping of the foundational \\naspects of data management. Central to this transformation is the emergence of data lakehouses as the new \\n“modern data stack.” These advanced data architectures are essential to harnessing the full potential of GenAI, \\ndriving faster, more cost-effective and wider democratization of data and AI technologies. As businesses \\nincreasingly rely on GenAI-powered tools and applications for competitive advantage, the underlying data \\ninfrastructure must evolve to support these advanced technologies effectively and securely.\\nNo Matter Where You Are on Your Path to Deploying GenAI Applications, \\nthe Quality of Your Data Matters\\nBusinesses need to achieve production quality with their GenAI applications. Developers need rich tools for \\nunderstanding the quality of their data and model outputs, along with an underlying platform that lets them \\ncombine and optimize all aspects of the GenAI process. GenAI has many components such as data preparation, \\nretrieval models, language models (either SaaS or open source), ranking and post-processing pipelines, prompt \\nengineering, and training models on custom enterprise data.\\nTo help you overcome common enterprise challenges with building GenAI, we’ve compiled a collection of \\ntechnical content and code samples. We’ll start each section with a brief overview and then provide use cases \\nand example code for reference. \\nIntroduction\\n3THE BIG BOOK OF GENERATIVE AI\\nIn this eBook, you’ll learn: \\n ■ How to plan a path from basic to advanced GenAI applications, leveraging your organization’s data\\n ■ How to use retrieval augmented generation (RAG) to make an off-the-shelf AI system smarter\\n ■ How to evaluate LLMs and where you want to invest in more powerful AI tools and systems that drive \\nmore significant operational gain\\n ■ How to build a custom LLM that may be better, faster and cheaper for your organization\\n ■ When it might be worth it to pretrain your own model — and more\\nUse cases for GenAI covered:\\n ■ How to use LLMs to gain actionable insights from product reviews\\n ■ How to use RAG for a chatbot to improve the quality of output\\n ■ How to train your own generative AI model in a cost-effective manner\\n ■ How to monitor and evaluate your deployed LLMs and GenAI applications\\n4THE BIG BOOK OF GENERATIVE AI\\nThe Path to Deploying \\nProduction-Quality \\nGenAI Applications\\nStage 0: Foundation Models\\nBefore setting off to create production-quality GenAI applications, we need to cover the base language models \\nthat serve as the foundation for layers of increasingly complex techniques. Foundation models commonly refer \\nto large language models that have been trained over extensive datasets to be generally good at some task \\n(chat, instruction following, code generation, etc.).\\nWe won’t cover many models, as it is a constantly shifting landscape, but it is important to note that while \\nunderlying architectures may differ drastically, foundation models generally fall under two categories: \\nproprietary (such as GPT-3.5 and Gemini) and open source (such as Llama2-70B and DBRX). The main difference \\nbetween the two is that while proprietary models historically have an edge on outright performance, users have \\nto send their data out to a third party and don’t have control over the underlying model as they’re often being \\nupdated and changed. \\nOpen source models, on the other hand, offer users full control over the model and the ability to run it on their \\nown terms with their own governance and data privacy. Here’s a current list of many open source GenAI models \\nacross different domains that are all free for commercial use. Databricks has also created their own state-of-\\nthe-art open source foundation model so users can build the highest-quality production GenAI applications.\\nFoundation Model Use Case\\nINTRODUCING DBRX: A NEW STATE-OF-THE-ART OPEN LLM\\nWe are excited to introduce DBRX, an open, general-purpose LLM created by Databricks. Across a range of \\nstandard benchmarks, DBRX sets a new state-of-the-art for established open LLMs. Moreover, it provides \\nthe open community and enterprises building their own LLMs with capabilities that were previously limited \\nto closed model APIs; according to our measurements, it surpasses GPT-3.5, and it is competitive with \\nGemini 1.0 Pro. It is an especially capable code model, surpassing specialized models like CodeLLaMA-70B on \\nprogramming, in addition to its strength as a general-purpose LLM.\\n5THE BIG BOOK OF GENERATIVE AI\\nThis state-of-the-art quality comes with marked improvements in training and inference performance. DBRX \\nadvances the state-of-the-art in efficiency among open models thanks to its fine-grained mixture-of-experts \\n(MoE) architecture. Inference is up to 2x faster than LLaMA2-70B, and DBRX is about 40% of the size of Grok-1 in \\nterms of both total and active parameter-counts. When hosted on Mosaic AI Model Serving, DBRX can generate \\ntext at up to 150 tok/s/user. Our customers will find that training MoEs is also about 2x more FLOP-efficient \\nthan training dense models for the same final model quality. End-to-end, our overall recipe for DBRX (including \\nthe pretraining data, model architecture, and optimization strategy) can match the quality of our previous-\\ngeneration MPT models with nearly 4x less compute.\\nFigure 1: DBRX outperforms established open source models on language understanding (MMLU), \\nProgramming (HumanEval), and Math (GSM8K).\\n6THE BIG BOOK OF GENERATIVE AI\\nThe weights of the base model (DBRX Base) and the fine-tuned model (DBRX Instruct) are available on Hugging \\nFace under an open license. Starting today, DBRX is available for Databricks customers to use via APIs, and \\nDatabricks customers can pretrain their own DBRX-class models from scratch or continue training on top of  \\none of our checkpoints using the same tools and science we used to build it. DBRX is already being integrated \\ninto our GenAI-powered products, where — in applications like SQL — early rollouts have surpassed GPT-3.5 \\nTurbo and are challenging GPT-4 Turbo. It is also a leading model among open models and GPT-3.5 Turbo on \\nRAG tasks.\\nTraining mixture-of-experts models is hard. We had to overcome a variety of scientific and performance \\nchallenges to build a pipeline robust enough to repeatedly train DBRX-class models in an efficient manner. Now \\nthat we have done so, we have a one-of-a-kind training stack that allows any enterprise to train world-class MoE \\nfoundation models from scratch. We look forward to sharing that capability with our customers and sharing our \\nlessons learned with the community.\\nDownload DBRX today from Hugging Face (DBRX Base, DBRX Instruct), or try out DBRX Instruct in our HF Space, \\nor see our model repository on github: databricks/dbrx.\\nWhat Is DBRX?\\nDBRX is a transformer-based decoder-only large language model (LLM) that was trained using next-token \\nprediction. It uses a fine-grained mixture-of-experts (MoE) architecture with 132B total parameters of which \\n36B parameters are active on any input. It was pre-trained on 12T tokens of text and code data. Compared \\nto other open MoE models like Mixtral and Grok-1, DBRX is fine-grained, meaning it uses a larger number of \\nsmaller experts. DBRX has 16 experts and chooses 4, while Mixtral and Grok-1 have 8 experts and choose 2. This \\nprovides 65x more possible combinations of experts and we found that this improves model quality. DBRX uses \\nrotary position encodings (RoPE), gated linear units (GLU), and grouped query attention (GQA). It uses the GPT-\\n4 tokenizer as provided in the tiktoken repository. We made these choices based on exhaustive evaluation and \\nscaling experiments.\\n7THE BIG BOOK OF GENERATIVE AI\\nDBRX was pretrained on 12T tokens of carefully curated data and a maximum context length of 32k tokens. We \\nestimate that this data is at least 2x better token-for-token than the data we used to pretrain the MPT family of \\nmodels. This new dataset was developed using the full suite of Databricks tools, including Apache Spark™ and \\nDatabricks notebooks for data processing, Unity Catalog for data management and governance, and MLflow for \\nexperiment tracking. We used curriculum learning for pretraining, changing the data mix during training in ways \\nwe found to substantially improve model quality.\\nQuality on Benchmarks vs. Leading Open Models\\nTable 1 shows the quality of DBRX Instruct and leading established, open models. DBRX Instruct is the leading \\nmodel on composite benchmarks, programming and mathematics benchmarks, and MMLU. It surpasses all chat \\nor instruction fine-tuned models on standard benchmarks.\\nComposite benchmarks. We evaluated DBRX Instruct and peers on two composite benchmarks: the Hugging \\nFace Open LLM Leaderboard (the average of ARC-Challenge, HellaSwag, MMLU, TruthfulQA, WinoGrande,  \\nand GSM8k) and the Databricks Model Gauntlet (a suite of over 30 tasks spanning six categories: world \\nknowledge, commonsense reasoning, language understanding, reading comprehension, symbolic problem \\nsolving, and programming).\\nAmong the models we evaluated, DBRX Instruct scores the highest on two composite benchmarks: the Hugging \\nFace Open LLM Leaderboard (74.5% vs. 72.7% for the next highest model, Mixtral Instruct) and the Databricks \\nGauntlet (66.8% vs. 60.7% for the next highest model, Mixtral Instruct).\\nProgramming and mathematics. DBRX Instruct is especially strong at programming and mathematics. It scores \\nhigher than the other open models we evaluated on HumanEval (70.1% vs. 63.2% for Grok-1, 54.8% for Mixtral \\nInstruct, and 32.2% for the best-performing LLaMA2-70B variant) and GSM8k (66.9% vs. 62.9% for Grok-1, 61.1% \\nfor Mixtral Instruct, and 54.1% for the best-performing LLaMA2-70B variant). DBRX outperforms Grok-1, the next \\nbest model on these benchmarks, despite the fact that Grok-1 has 2.4x as many parameters. On HumanEval, \\nDBRX Instruct even surpasses CodeLLaMA-70B Instruct, a model built explicitly for programming, despite the \\nfact that DBRX Instruct is designed for general-purpose use (70.1% vs. 67.8% on HumanEval as reported by Meta \\nin the CodeLLaMA blog).\\nMMLU. DBRX Instruct scores higher than all other models we consider on MMLU, reaching 73.7%.\\n8THE BIG BOOK OF GENERATIVE AI\\nMODEL DBRX  \\nINSTRUCT\\nMIXTRAL \\nINSTRUCT\\nMIXTRAL \\nBASE\\nLLAMA2-70  \\nB CHAT\\nLLAMA2-70  \\nB BASE GROK-11\\nOpen LLM Leaderboard2 \\n(Avg of next 6 rows) 74.5% 72.7% 68.4% 62.4% 67.9% —\\nARC-challenge 25-shot 68.9% 70.1% 66.4% 64.6% 67.3% —\\nHellaSwag 10-shot 89.0% 87.6% 86.5% 85.9% 87.3% —\\nMMLU 5-shot 73.7% 71.4% 71.9% 63.9% 69.8% 73.0%\\nTruthful QA 0-shot 66.9% 65.0% 46.8% 52.8% 44.9% —\\nWinoGrande 5-shot 81.8% 81.1% 81.7% 80.5% 83.7% —\\nGSM8k CoT 5-shot \\nmaj@13 66.9% 61.1% 57.6% 26.7% 54.1% 62.9% \\n(8-shot)\\nGauntlet v0.34 \\n(Avg of 30+ diverse tasks) 66.8% 60.7% 56.8% 52.8% 56.4% —\\nHumanEval5 \\n0-Shot, pass@1 \\n(Programming)\\n70.1% 54.8% 40.2% 32.2% 31.0% 63.2%\\nLLaMA2-70B Base\\nTable 1: Quality of DBRX Instruct and leading open models. See footnotes for details on how numbers were collected. \\nBolded and underlined is the highest score.\\n9THE BIG BOOK OF GENERATIVE AI\\nQuality on Benchmarks vs. Leading Closed Models\\nTable 2 shows the quality of DBRX Instruct and leading closed models. According to the scores reported by \\neach model creator, DBRX Instruct surpasses GPT-3.5 (as described in the GPT-4 paper), and it is competitive \\nwith Gemini 1.0 Pro and Mistral Medium.\\nAcross nearly all benchmarks we considered, DBRX Instruct surpasses or - at worst - matches GPT-3.5. DBRX \\nInstruct outperforms GPT-3.5 on general knowledge as measured by MMLU (73.7% vs. 70.0%) and commonsense \\nreasoning as measured by HellaSwag (89.0% vs. 85.5%) and WinoGrande (81.8% vs. 81.6%). DBRX Instruct \\nespecially shines on programming and mathematical reasoning as measured by HumanEval (70.1% vs. 48.1%) and \\nGSM8k (72.8% vs. 57.1%).\\nDBRX Instruct is competitive with Gemini 1.0 Pro and Mistral Medium. Scores for DBRX Instruct are higher than \\nGemini 1.0 Pro on Inflection Corrected MTBench, MMLU, HellaSwag, and HumanEval, while Gemini 1.0 Pro is \\nstronger on GSM8k. Scores for DBRX Instruct and Mistral Medium are similar for HellaSwag, while Mistral Medium \\nis stronger on Winogrande and MMLU and DBRX Instruct is stronger on HumanEval, GSM8k, and Inflection \\nCorrected MTBench.\\n10MODEL DBRX  \\nINSTRUCT GPT-3.57 GPT-48 CLAUDE  \\n3 HAIKU\\nCLAUDE 3 \\nSONNET\\nCLAUDE 3 \\nOPUS\\nGEMINI  \\n1.0 PRO\\nGEMINI  \\n1.5 PRO\\nMISTRAL  \\nMEDIUM\\nMISTRAL \\nLARGE\\nMT Bench  \\n(Inflection corrected , n=5) 8.39 ± 0.08 — — 8.41 ± \\n0.04 \\n8.54 ± \\n0.09\\n9.03 ± \\n0.06\\n8.23 ± 0.08 — 8.05 ± 0.12 8.90 ± \\n0.06\\nMMLU 5-shot 73.7% 70.0% 86.4% 75.2% 79.0% 86.8% 71.8% 81.9% 75.3% 81.2%\\nHellaSwag 10-shot 89.0% 85.5% 95.3% 85.9% 89.0% 95.4% 84.7% 92.5% 88.0% 89.2%\\nHumanEval 0-Shot \\npass@1 \\n(Programming)\\n70.1%  \\ntemp=0, \\nN=1\\n48.1% 67.0% 75.9% 73.0% 84.9% 67.7% 71.9% 38.4% 45.1%\\nGSM8k CoT maj@1 72.8% \\n(5-shot)\\n57.1%  \\n(5-shot)\\n92.0%  \\n(5-shot) 88.9% 92.3% 95.0%\\n86.5% \\n(maj1@32)\\n91.7%  \\n(11-shot)\\n66.7%  \\n(5-shot)\\n81.0%  \\n(5-shot)\\nWinoGrande 5-shot 81.8% 81.6% 87.5% — — — — — 88.0% 86.7%\\nTable 2: Quality of DBRX Instruct and leading closed models. Other than Inflection Corrected MTBench (which we measured ourselves on model \\nendpoints), numbers were as reported by the creators of these models in their respective whitepapers. See footnotes for additional details.\\n11\\nTHE BIG BOOK OF GENERATIVE AITHE BIG BOOK OF GENERATIVE AI\\nQuality on Long-Context Tasks and RAG\\nDBRX Instruct was trained with up to a 32K token context window. Table 3 compares its performance to that of \\nMixtral Instruct and the latest versions of the GPT-3.5 Turbo and GPT-4 Turbo APIs on a suite of long-context \\nbenchmarks (KV-Pairs from the Lost in the Middle paper and HotpotQAXL, a modified version of HotPotQA that \\nextends the task to longer sequence lengths). GPT-4 Turbo is generally the best model at these tasks. However, \\nwith one exception, DBRX Instruct performs better than GPT-3.5 Turbo at all context lengths and all parts of the \\nsequence. Overall performance for DBRX Instruct and Mixtral Instruct are similar.\\nMODEL DBRX  \\nINSTRUCT\\nMIXTRAL  \\nINSTRUCT\\nGPT-3.5 TURBO \\n(API)\\nGPT-4 TURBO \\n(API)\\nAnswer in Beginning Third of Context 45.1% 41.3% 37.3%* 49.3%\\nAnswer in Middle Third of Context 45.3% 42.7% 37.3%* 49.0%\\nAnswer in Last Third of Context 48.0% 44.4% 37.0%* 50.9%\\n2K Context 59.1% 64.6% 36.3% 69.3%\\n4K Context 65.1% 59.9% 35.9% 63.5%\\n8K Context 59.5% 55.3% 45.0% 61.5%\\n16K Context 27.0% 20.1% 31.7% 26.0%\\n32K Context 19.9% 14.0% — 28.5%\\nTable 3: The average performance of models on the KV-Pairs and HotpotQAXL benchmarks. Bold is the highest score. Underlined is the highest score \\nother than GPT-4 Turbo. GPT-3.5 Turbo supports a maximum context length of 16K, so we could not evaluate it at 32K. *Averages for the beginning, \\nmiddle, and end of the sequence for GPT-3.5 Turbo include only contexts up to 16K.\\n12THE BIG BOOK OF GENERATIVE AI\\nOne of the most popular ways to leverage a model’s context is retrieval augmented generation (RAG). \\nIn RAG, content relevant to a prompt is retrieved from a database and presented alongside the prompt to \\ngive the model more information than it would otherwise have. Table 4 shows the quality of DBRX on two RAG \\nbenchmarks — Natural Questions and HotPotQA — when the model is also provided with the top 10 passages \\nretrieved from a corpus of Wikipedia articles using the embedding model bge-large-en-v1.5. DBRX Instruct  \\nis competitive with open models like Mixtral Instruct and LLaMA2-70B Chat and the current version  \\nof GPT-3.5 Turbo.\\nMODEL DBRX  \\nINSTRUCT\\nMIXTRAL  \\nINSTRUCT\\nLLAMA2-70B \\nCHAT\\nGPT 3.5 TUR -\\nBO (API)\\nGPT 4 TURBO \\n(API)\\nNatural Questions 60.0% 59.1% 56.5% 57.7% 63.9%\\nHotPotQA 55.0% 54.2% 54.7% 53.0% 62.9%\\nTable 4: The performance of the models measured when each model is given the top 10 passages retrieved from a Wikipedia corpus \\nusing bge-large-en-v1.5. Accuracy is measured by matching within the model’s answer. Bold is the highest score. Underlined is the \\nhighest score other than GPT-4 Turbo. \\nTraining Efficiency\\nModel quality must be placed in the context of how efficient the model is to train and use. This is especially \\nso at Databricks, where we build models like DBRX to establish a process for our customers to train their own \\nfoundation models.\\nWe found training mixture-of-experts models to provide substantial improvements in compute-efficiency for \\ntraining (Table 5). For example, training a smaller member of the DBRX family called DBRX MoE-B (23.5B total \\nparameters, 6.6B active parameters) required 1.7x fewer FLOPs to reach a score of 45.5% on the Databricks LLM \\nGauntlet than LLaMA2-13B required to reach 43.8%. DBRX MoE-B also contains half as many active parameters \\nas LLaMA2-13B.\\n13THE BIG BOOK OF GENERATIVE AI\\nLooking holistically, our end-to-end LLM pretraining pipeline has become nearly 4x more compute-efficient \\nin the past ten months. On May 5, 2023, we released MPT-7B, a 7B parameter model trained on 1T tokens that \\nreached a Databricks LLM Gauntlet score of 30.9%. A member of the DBRX family called DBRX MoE-A (7.7B total \\nparameters, 2.2B active parameters) reached a Databricks Gauntlet score of 30.5% with 3.7x fewer FLOPs. This \\nefficiency is the result of a number of improvements, including using an MoE architecture, other architecture \\nchanges to the network, better optimization strategies, better tokenization, and - very importantly - better \\npretraining data.\\nIn isolation, better pretraining data made a substantial impact on model quality. We trained a 7B model on 1T \\ntokens (called DBRX Dense-A) using the DBRX pretraining data. It reached 39.0% on the Databricks Gauntlet \\ncompared to 30.9% for MPT-7B. We estimate that our new pretraining data is at least 2x better token-for-token \\nthan the data used to train MPT-7B. In other words, we estimate that half as many tokens are necessary to reach \\nthe same model quality. We determined this by training DBRX Dense-A on 500B tokens; it outperformed MPT-7B \\non the Databricks Gauntlet, reaching 32.1%. In addition to better data quality, another important contributor to \\nthis token-efficiency may be the GPT-4 tokenizer, which has a large vocabulary and is believed to be especially \\ntoken-efficient. These lessons about improving data quality translate directly into practices and tools that our \\ncustomers use to train foundation models on their own data.\\nMODEL TOTAL PARAMS ACTIVE PARAMS GAUNTLET SCORE RELATIVE FLOPS\\nDBRX MoE-A 7.7B 2.2B 30.5% 1x\\nMPT-7B (1T tokens) — 6.7B 30.9% 3.7x\\nDBRX Dense-A (1T tokens) — 6.7B 39.0% 3.7x\\nDBRX Dense-A (500B tokens) — 6.7B 32.1% 1.85x\\nDBRX MoE-B 23.5B 6.6B 45.5% 1x\\nLLaMA2-13B — 13.0B 43.8% 1.7x\\nTable 5:  Details of several test articles that we used to validate the training efficiency of the DBRX MoE architecture and end-to-end training pipeline.\\n14THE BIG BOOK OF GENERATIVE AI\\nInference Efficiency\\nFigure 2 shows the end-to-end inference efficiency of serving DBRX and similar models using NVIDIA \\nTensorRT-LLM with our optimized serving infrastructure and 16-bit precision. We aim for this benchmark \\nto reflect real-world usage as closely as possible, including multiple users simultaneously hitting the same \\ninference server. We spawn one new user per second, each user request contains an approximately 2000 \\ntoken prompt, and each response comprises 256 tokens.\\nIn general, MoE models are faster at inference than their total parameter-counts would suggest. This is due \\nto the fact that they use relatively few parameters for each input. We find that DBRX is no exception in this \\nrespect. DBRX inference throughput is 2-3x higher than a 132B non-MoE model.\\nInference efficiency and model quality are typically in tension: bigger models typically reach higher quality, but \\nsmaller models are more efficient for inference. Using an MoE architecture makes it possible to attain better \\ntradeoffs between model quality and inference efficiency than dense models typically achieve. For example, \\nDBRX is both higher quality than LLaMA2-70B and - thanks to having about half as many active parameters - \\nDBRX inference throughput is up to 2x faster (Figure 2). Mixtral is another point on the improved pareto frontier \\nattained by MoE models: it is smaller than DBRX, and it is correspondingly lower in terms of quality but reaches \\nhigher inference throughput. Users of the Databricks Foundation Model APIs can expect to see up to 150 \\ntokens per second for DBRX on our optimized model serving platform with 8-bit quantization.\\n15Figure 2:  Inference throughput for various model configurations on our optimized serving infrastructure using NVIDIA TensorRT-LLM at 16-bit \\nprecision with the best optimization flags we could find. Models are run in tensor-parallel across the entire node. The input prompt contains \\napproximately 2000 prompt tokens and we generate 256 output tokens. One new user spawns every second.\\n16\\nTHE BIG BOOK OF GENERATIVE AITHE BIG BOOK OF GENERATIVE AI\\nHow We Built DBRX\\nDBRX was trained on 3072 NVIDIA H100s connected by 3.2Tbps Infiniband. The main process of building DBRX \\n- including pretraining, post-training, evaluation, red-teaming, and refining - took place over the course of \\nthree months. It was the continuation of months of science, dataset research, and scaling experiments, not to \\nmention years of LLM development at Databricks that includes the MPT and Dolly projects and the thousands \\nof models we have built and brought to production with our customers.\\nTo build DBRX, we leveraged the same suite of Databricks tools that are available to our customers. We \\nmanaged and governed our training data using Unity Catalog. We explored this data using newly acquired  \\nLilac AI. We processed and cleaned this data using Apache Spark™ and Databricks notebooks. We trained \\nDBRX using optimized versions of our open-source training libraries: MegaBlocks, LLM Foundry, Composer, \\nand Streaming. We managed large scale model training and finetuning across thousands of GPUs using our \\nMosaic AI Training service. We logged our results using MLflow. We collected human feedback for quality and \\nsafety improvements through Mosaic AI Model Serving and Inference Tables. We manually experimented with \\nthe model using the Databricks Playground. We found the Databricks tools to be best-in-class for each of their \\npurposes, and we benefited from the fact that they were all part of a unified product experience.\\nGet Started With DBRX on Databricks\\nIf you’re looking to start working with DBRX right away, it’s easy to do so with the Databricks Mosaic AI \\nFoundation Model APIs. You can quickly get started with our pay-as-you-go pricing and query the model from \\nour AI Playground chat interface. For production applications, we offer a provisioned throughput option to \\nprovide performance guarantees, support for finetuned models, and additional security and compliance. To \\nprivately host DBRX, you can download the model from the Databricks Marketplace and deploy the model on \\nModel Serving.\\n17THE BIG BOOK OF GENERATIVE AI\\nConclusions\\nAt Databricks, we believe that every enterprise should have the ability to control its data and its destiny in the \\nemerging world of GenAI. DBRX is a central pillar of our next generation of GenAI products, and we look forward \\nto the exciting journey that awaits our customers as they leverage the capabilities of DBRX and the tools we \\nused to build it. In the past year, we have trained thousands of LLMs with our customers. DBRX is only one \\nexample of the powerful and efficient models being built at Databricks for a wide range of applications, from \\ninternal features to ambitious use-cases for our customers.\\nAs with any new model, the journey with DBRX is just the beginning, and the best work will be done by those \\nwho build on it: enterprises and the open community. This is also just the beginning of our work on DBRX, and \\nyou should expect much more to come.\\nContributions\\nThe development of DBRX was led by the Mosaic team that previously built the MPT model family, in \\ncollaboration with dozens of engineers, lawyers, procurement and finance specialists, program managers, \\nmarketers, designers, and other contributors from across Databricks. We are grateful to our colleagues, friends, \\nfamily, and the community for their patience and support over the past months.\\nIn creating DBRX, we stand on the shoulders of giants in the open and academic community. By making DBRX \\navailable openly, we intend to invest back in the community in hopes that we will build even greater technology \\ntogether in the future. With that in mind, we gratefully acknowledge the work and collaboration of Trevor Gale \\nand his MegaBlocks project (Trevor’s PhD adviser is Databricks CTO Matei Zaharia), the PyTorch team and \\nthe FSDP project, NVIDIA and the TensorRT-LLM project, the vLLM team and project, EleutherAI and their \\nLLM evaluation project, Daniel Smilkov and Nikhil Thorat at Lilac AI, and our friends at the Allen Institute for \\nArtificial Intelligence (AI2).\\n18THE BIG BOOK OF GENERATIVE AI\\nStage 1: Prompt Engineering\\nMany companies still remain in the foundational stages of adopting generative AI technology. They have  \\nno overarching AI strategy in place, no clear use cases to pursue and no access to a team of data scientists and \\nother professionals who can help guide the company’s AI adoption journey.\\nIf this is like your business, a good starting point is an off-the-shelf LLM. While these LLMs lack the domain-\\nspecific expertise of custom AI models, experimentation can help you plot your next steps. Your employees can \\ncraft specialized prompts and workflows to guide their usage. Your leaders can get a better understanding of \\nthe strengths and weaknesses of these tools as well as a clearer vision of what early success in AI might look \\nlike. Your organization can use things like the Databricks AI Playground to figure out where to invest in more \\npowerful AI tools and systems that drive more significant operational gain and even use LLMs as a judge to help \\nevaluate responses.\\nPRACTICAL APPLICATIONS OF GENAI TECHNOLOGY\\nLet’s delve into a compelling use case that illustrates the power of prompt engineering with off-the-shelf \\nLLMs. Consider the challenge many businesses face: sifting through vast amounts of product reviews  \\nto glean actionable insights. Without a dedicated team of data scientists or a clear AI strategy, this task  \\nmight seem daunting. However, leveraging the flexibility of LLMs through prompt engineering offers a \\nstraightforward solution.\\n19THE BIG BOOK OF GENERATIVE AI\\nPrompt Engineering Use Case\\nAutomated Analysis of Product Reviews Using Large Language Models\\nKeep track of customer feedback at scale\\nCheck out our LLM Solution Accelerators for Retail for more details and to download the notebooks.\\nWhile conversational AI has garnered a lot of media attention in recent months, the capabilities of large \\nlanguage models (LLMs) extend well beyond conversational interactions. It\\'s in these less prominent  \\ncapabilities such as query response, summarization, classification and search that many organizations  \\nare finding immediate opportunities to supercharge their workforce and up-level customer experiences.\\nThe potential of these applications is staggering. By one estimate, LLMs (and other generative AI \\n technologies) could, in the near future, address tasks that today occupy 60%–70% of employees’ time.  \\nThrough augmentation, numerous studies have shown that the time to complete various tasks performed  \\nby knowledge workers such as background research, data analysis and document writing can be cut in half.  \\nAnd still other studies have shown that the use of these technologies can dramatically reduce the time for  \\nnew workers to achieve full productivity.\\nBut before these benefits can be fully realized, organizations must first rethink the management of the \\nunstructured information assets on which these models depend and find ways to mitigate the issues of bias \\nand accuracy that affect their output. This is why so many organizations are currently focusing their efforts \\non focused, internal applications where a limited scope provides opportunities for better information access \\nand human oversight can serve as a check to errant results. These applications, aligned with core capabilities \\nalready residing within the organization, have the potential to deliver real and immediate value, while LLMs and \\ntheir supporting technologies continue to evolve and mature.\\n20THE BIG BOOK OF GENERATIVE AI\\nPRODUCT REVIEW SUMMARIZATION COULD USE A BOOST\\nTo illustrate the potential of a more focused approach to LLM adoption, we consider a fairly simple and common \\ntask performed within many online retail organizations: product review summarization. Today, most organizations \\nemploy a modestly-sized team of workers to read and digest user feedback for insights that may help improve a \\nproduct\\'s performance or otherwise identify issues related to customer satisfaction.\\nThe work is important but anything but sexy. A worker reads a review, takes notes, and moves on to the next. \\nIndividual reviews that require a response are flagged and a summary of the feedback from across multiple \\nreviews are compiled for review by product or category managers.\\nThis is a type of work that\\'s ripe for automation. The volume of reviews that pour into a site mean the more \\ndetailed portions of this work are often performed on a limited subset of products across variable windows \\ndepending on a products importance. In more sophisticated organizations, rules detecting course or \\ninappropriate language and models estimating user sentiment or otherwise classifying reviews for positive, \\nnegative or neutral experiences may be applied to help identify problematic content and draw a reviewer\\'s \\nattention to it. But either way, a lot is missed simply because we can\\'t throw enough bodies at the problem to \\nkeep up and those bodies tend to become bored or fatigued with the monotony of the work.\\nLARGE LANGUAGE MODELS CAN AUTOMATE PRODUCT REVIEW ANALYSIS\\nBy using an LLM, issues of scale and consistency can be easily addressed. All we need to do is bring the product \\nreviews to the model and ask:\\n ■ What are the top three points of negative feedback found across these reviews?\\n ■ What features do our customers like best about this product?\\n ■ Do customers feel they are receiving sufficient value from the product relative to what they are being \\nasked to pay?\\n ■ Are there any reviews that are especially negative or are using inappropriate language?\\n21THE BIG BOOK OF GENERATIVE AI\\nWithin seconds we can have a tidy response, allowing our product managers to focus on responding to issues \\ninstead of simply detecting them.\\nBut what about the problem of accuracy and bias? Standards for identifying inaccuracies and bias in LLM \\noutput are evolving as are techniques for better ensuring that outputs align with an organization\\'s expectations, \\nand the fine-tuning of models using approved content can go a long way to ensure models have a preference to \\ngenerate content that\\'s at least aligned with how an organization prefers to communicate.\\nThis is a long-winded way of saying there is no ideal solution to the problem as of yet. But when compared  \\nto where we are with human-driven processes and more simplistic models or rules-based approaches,  \\nthe results are expected to be better or at a minimum no worse than what we currently experience.  \\nAnd given that these review summaries are for internal consumption, the impact of an errant model can \\nbe easily managed.\\nYOU CAN BUILD A SOLUTION FOR THIS TODAY\\nTo demonstrate exactly how this work could be performed, we have built a Solution Accelerator for summarizing \\nproduct reviews. This is based heavily on a previously published blog from Sean Owen that addressed some of \\nthe core technical challenges of tuning an LLM on the Databricks platform. For the accelerator, we are using the \\nAmazon Product Reviews Dataset, which contains 51 million user-generated reviews across 2 million distinct \\nbooks as this provides access to a wide range of reviewer content and presents a scaling challenge many \\norganizations will recognize.\\nWe imagine a scenario in which a team of product managers receives customer feedback through online \\nreviews. These reviews are important for identifying issues that may need to be addressed regarding a \\nparticular item and for steering future books to be offered by the site. Without the use of technology, this team \\nstruggles to read all the feedback and summarize into a workable set notes. As a result, they limit their attention \\nto just the most critical items and are able to only process the feedback on a sporadic basis.\\n22THE BIG BOOK OF GENERATIVE AI\\nBut using Databricks, they are able to set up a pipeline to collect feedback from a wider range of products \\nand summarize these on a regular basis. Recognizing that positively rated products are likely to highlight the \\nstrengths of these books while lower rated products are likely to focus on their weaknesses, they separate  \\nthese reviews based on user-provided ratings and task an LLM to extract different sets of information from  \\neach high-level category of reviews.\\nSummary metrics are provided to allow product managers an overview of the feedback received and are \\nbacked by more detailed summaries generated by the LLM (Figure 1).\\nFigure 1: Summary metrics and bullet-point details extracted from user reviews extracted using an LLM\\n23THE BIG BOOK OF GENERATIVE AI\\nDATABRICKS BRINGS TOGETHER ALL THE COMPONENTS OF A SOLUTION\\nThe scenario demonstrated above depends on the use of an LLM. In months prior, the use of such an LLM \\nrequired access to specialized computational infrastructures, but with advances in the open source community \\nand investments in the Databricks platform, we are now able to run the LLM in our local Databricks environment.\\nIn this particular scenario, the sensitivity of the data was not a motivating factor for this choice. Instead, we \\nfound that the volume of reviews to be processed tipped the cost scales toward the use of Databricks, allowing \\nus to trim about one-third of the cost of implementing a similar solution using a third-party service.\\nIn addition, we found that by implementing our own infrastructure, we were able to scale the environment up \\nfor faster processing, tackling as many as 760,000 reviews per hour in one test without having to be concerned \\nwith constraints imposed by an external service. While most organizations will not have the need to scale quite \\nto that level, it\\'s nice to know it is there should it be.\\nBut this solution is more than just an LLM. To bring together the whole solution we needed to develop a data \\nprocessing workflow to receive incoming reviews, prepare them for submission to the model and to capture \\nmodel output for further analysis. As a unified data platform, Databricks provides us the means to address \\n both data engineering and data science requirements without data replication. And when we are done \\nprocessing the reviews, our analysts can use their tools of choice to query the output and make business \\ndecisions. Through Databricks, we have access to the full array of capabilities for us to build a solution aligned \\nwith our business’ needs.\\n24THE BIG BOOK OF GENERATIVE AI\\nStage 2: Retrieval Augmented Generation\\nRetrieval augmented generation (RAG) lets you bring in supplemental knowledge resources to make an  \\noff-the-shelf AI system smarter. RAG won’t change the underlying behavior of the model, but it will improve  \\nthe quality and accuracy of the responses.\\nHowever, at this point, your business should not be uploading its “mission-critical” data. Instead, the RAG \\nprocess typically involves smaller amounts of nonsensitive information.\\nFor example, plugging in an employee handbook can allow your workers to start asking the underlying model \\nquestions about the organization’s vacation policy. Uploading instruction manuals can help power a service \\nchatbot. With the ability to query support tickets using AI, support agents can get answers quicker; however, \\ninputting confidential financial data so employees can inquire about the company’s performance is likely a step \\ntoo far.\\nTo get started, your team should first consolidate and cleanse the data you intend to use. With RAG, it’s vital \\nthat your company stores the data in sizes that will be appropriate for the downstream models. Often, that \\nrequires users to splice it into smaller segments.\\nThen, you should seek out a tool like Databricks Vector Search, which enables users to quickly set up their own \\nvector database. And because it’s governed by Unity Catalog, granular controls can be put in place to ensure \\nemployees are only accessing the datasets for which they have credentials.\\nFinally, you can then plug that endpoint into a LLM. A tool like Databricks MLflow helps to centralize the \\nmanagement of those APIs.\\n25THE BIG BOOK OF GENERATIVE AI\\nAmong the benefits of RAG are reduced hallucinations, more up-to-date and accurate responses,  \\nand better domain-specific intelligence. RAG-assisted models are also a more cost-effective approach  \\nfor most organizations.\\nWhile RAG will help improve the results from commercial models, there are still many limitations to the use \\nof RAG. If your business is unable to get the results it wants, it’s time to move on to heavier-weight solutions, \\nbut moving beyond RAG-supported models often requires a much deeper commitment. The additional \\ncustomization costs more and requires a lot more data.\\nThat’s why it’s key that organizations first build a core understanding of how to use LLMs. By reaching the \\nperformance limitations of off-the-shelf models before moving on, you and your leadership can further hone  \\nin on where to allocate resources.\\nEnhance the Performance of Off-the-Shelf AI Models With RAG\\nLet’s explore a practical use case that demonstrates how real-time structured data can significantly improve \\nthe response quality of your RAG applications. This example will showcase how integrating dynamic information \\ncan transform the effectiveness and applicability of AI in your business operations.\\n26THE BIG BOOK OF GENERATIVE AI\\nRAG Use Case\\nImprove Your RAG Application Response Quality With Real-Time Structured Data\\nby Mani Parkhe, Aakrati Talati, Sue Ann Hong, Craig Wiley, Chenen Liang and Mingyang Ge\\nRetrieval augmented generation (RAG) is an efficient mechanism to provide relevant data as context in \\nGenAI applications. Most RAG applications typically use vector indexes to search for relevant context from \\nunstructured data such as documentation, wikis, and support tickets. Yesterday, we announced Databricks \\nVector Search Public Preview that helps with exactly that. However, GenAI response quality can be enhanced by \\naugmenting these text-based contexts with relevant and personalized structured data. Imagine a GenAI tool on \\na retail website where customers inquire, \"Where’s my recent order?\" This AI must understand that the query \\nis about a specific purchase, then gather up-to-date shipment information for line items, before using LLMs to \\ngenerate a response. Developing these scalable applications demands substantial work, integrating technologies \\nfor handling both structured and unstructured data with GenAI capabilities.\\nWe are excited to announce the public preview of Databricks Feature & Function Serving, a low latency real-\\ntime service designed to serve structured data from the Databricks Data Intelligence Platform. You can instantly \\naccess pre-computed ML features as well as perform real-time data transformations by serving any Python \\nfunction from Unity Catalog. The retrieved data can then be used in real-time rule engines, classical ML, and \\nGenAI applications.\\nUsing Feature and Function Serving (AWS)(Azure) for structured data in coordination with Databricks Vector \\nSearch (AWS)(Azure) for unstructured data significantly simplifies productionalization of GenAI applications. \\nUsers can build and deploy these applications directly in Databricks and rely on existing data pipelines, \\ngovernance, and other enterprise features. Databricks customers across various industries are using these \\ntechnologies along with open source frameworks to build powerful GenAI applications such as the ones \\ndescribed in the table below.\\n27INDUSTRY USE CASE\\nRetail  ■ Product Recommendations / Search Ranking using user preferences, search history, location . . . etc.\\n ■ Image and metadata based product search\\n ■ Inventory management and forecasting using sales data, seasonal trends and market/competitive analysis\\nEducation  ■ Personalized learning plans based on past mistakes, historical trends and  cohorts\\n ■ Automated grading, feedback, follow-ups and progress reporting\\n ■ Content filtering for issued devices\\nFinancial Services  ■ Natural language apps for analysts and investors to correlate earning calls and reports with market intelligence and historical trends\\n ■ Fraud and risk analysis\\n ■ Personalized wealth management, retirement planning, what-if analysis and next-best actions \\nTravel and Hospitality  ■ Chatbots for personalized customer interactions and tailored travel recommendations\\n ■ Dynamic route planning using weather, live traffic patterns, and historical data\\n ■ Dynamic price optimization using competitive analysis and demand-based pricing\\nHealthcare and Life Sciences  ■ Patient/member engagement and health summaries\\n ■ Support apps for personalized care, clinical decisions and care coordination\\n ■ R&D report summarization, clinical trial analysis, drug repurposing\\nInsurance  ■ Risk assessment for mortgage underwriting using text and structured data about properties and neighborhoods\\n ■ User chatbots for questions about policies, risk and what-if analysis\\n ■ Claim processing automation\\nTechnology and Manufacturing  ■ Prescriptive maintenance and diagnostics for equipment using guided instruction\\n ■ Anomaly detection on live data stream against historical statistics\\n ■ Automated analysis for daily production / shift analysis and future planning\\nMedia and Entertainment  ■ In-app content discovery and recommendations, personalized email and digital marketing\\n ■ Content localization\\n ■ Personalized gaming experiences and game review\\n28\\nTHE BIG BOOK OF GENERATIVE AITHE BIG BOOK OF GENERATIVE AI\\nSERVING STRUCTURED DATA TO RAG APPLICATIONS\\nTo demonstrate how structured data can help enhance the quality of a GenAI application, we use the following \\nexample for a travel planning chatbot. The example shows how user preferences (example: “ocean view” or \\n“family friendly”) can be paired with unstructured information sourced about hotels to search for hotel matches. \\nTypically hotel prices dynamically change based on demand and seasonality. A price calculator built into the \\nGenAI application ensures that the recommendations are within the user\\'s budget. The GenAI application that \\npowers the bot uses Databricks Vector Search and Databricks Feature and Function Serving as building blocks \\nto serve the necessary personalized user preferences and budget and hotel information using LangChain’s \\nagents API.\\nYou can find the complete notebook for this RAG Chain application as depicted above. This application can be \\nrun locally within the notebook or deployed as an endpoint accessible by a chatbot user interface.\\n29THE BIG BOOK OF GENERATIVE AI\\nACCESS YOUR DATA AND FUNCTIONS AS REAL-TIME ENDPOINTS\\nWith Feature Engineering in Unity Catalog you can already use any table with a primary key to serve features \\nfor training and serving. Databricks Model Serving supports using Python functions to compute features on-\\ndemand. Built using the same technology available under the hood for Databricks Model Serving, feature and \\nfunction endpoints can be used to access any pre-computed feature or compute them on-demand. With a \\nsimple syntax you can define a feature spec function in Unity Catalog that can encode the directed acyclic \\ngraph to compute and serve features as a REST endpoint.\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\n27\\n28\\nfrom databricks.feature_engineering import (\\n  FeatureFunction,\\n  FeatureLookup,\\n  FeatureEngineeringClient,\\n)\\nfeatures = [\\n  # Lookup columns `latitude` and `longitude` from `restaurants` table in UC using the input `restaurant_id` as key\\n  FeatureLookup(\\n    table_name=\"main.default.restaurants\",\\n    lookup_key=\"restaurant_id\",\\n    features=[\"latitude”, “longitude\"]\\n  ),\\n  # Calculate a new feature called `distance` using the restaurant and user\\'s current location\\n  FeatureFunction(\\n    udf_name=\"main.default.distance\",\\n    output_name=\"distance\",\\n    # bind the function parameter with input from other features or from request.\\n    input_bindings={\"user_latitude\": \"user_latitude\", \"user_longitude\": \"user_longitude\",\\n                    \"restaurant_latitude\": \"latitude\", \"restaurant_longitude\": \"longitude\"},\\n  ),\\n]\\nfe = FeatureEngineeringClient()\\n# Create a feature spec with the features listed above.\\n# The FeatureSpec can be accessed in UC as a Function.\\nfe.create_feature_spec(\\n  name=\"main.default.restaurant_features\",\\n  features=features,\\n)\\n30THE BIG BOOK OF GENERATIVE AI\\n \\n1\\n2\\n3\\n4\\n \\n5\\n6\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\nfrom databricks.feature_engineering.entities.feature_serving_endpoint import (\\n  ServedEntity,\\n  EndpointCoreConfig,\\n)\\nfe.create_feature_serving_endpoint(\\n  name=\"restaurant-features\",\\n    config=EndpointCoreConfig(\\n    served_entities=ServedEntity(\\n      feature_spec_name=\"main.default.restaurant_features\",\\n      workload_size=\"Small\",\\n      scale_to_zero_enabled=True\\n    )\\n  )\\n)\\nThis feature spec function can be served in real-time as a REST endpoint. All endpoints are accessible in \\nthe Serving left navigation tab including features, function, custom trained models, and foundation models. \\nProvision the endpoint using this API.\\nThe endpoint can also be created using a UI workflow as shown in the following graphic\\n31THE BIG BOOK OF GENERATIVE AI\\nNow features can be accessed in real time by querying the endpoint:\\nTo serve structured data to real-time AI applications, precomputed data needs to be deployed to operational \\ndatabases. Users can already use external online stores as a source of precomputed features — for example \\nDynamoDB and Cosmos DB are commonly used to serve features in Databricks Model Serving. Databricks \\nOnline Tables (AWS)(Azure) adds new functionality that simplifies synchronization of precomputed features to a \\ndata format optimized for low latency data lookups. You can sync any table with a primary key as an online table \\nand the system will set up an automatic pipeline to ensure data freshness.\\n \\n1\\n2\\n3\\n4\\n5\\n \\n6\\ncurl \\\\\\n  -u token:$DATABRICKS_TOKEN \\\\\\n  -X POST \\\\\\n  -H \"Content-Type: application/json\" \\\\\\n  -d \\'{\"dataframe_records\": [{\"user_latitude\": 37.9711, \"user_longitude\": -122.3940, \"restaurant_id\": 5}]}\\' \\\\ \\n  https://<databricks-instance>/serving-endpoints/restaurant-features/invocations\\n32THE BIG BOOK OF GENERATIVE AI\\nStage 3: Fine-Tuning a Foundation Model\\nMoving beyond RAG to model fine-tuning lets you start building models that are much more deeply \\npersonalized to the business. If you have already been experimenting with commercial models across your \\noperations, you are likely ready to advance to this stage. There’s a clear understanding at the executive level of \\nthe value of generative AI, as well as an understanding of the limitations of publicly available LLMs. Specific use \\ncases have been established. And now, you and your enterprise are ready to go deeper.\\nWith fine-tuning, you can take a general-purpose model and train it on your own specific data. For example, \\ndata management provider Stardog relies on the Mosaic AI tools from Databricks to fine-tune the off-the-shelf \\nLLMs they use as a foundation for their Knowledge Graph Platform. This enables Stardog’s customers to query \\ntheir own data across the different silos simply by using natural language.\\nIt’s imperative that organizations at this stage have an underlying architecture in place that will help ensure the \\ndata supporting the models is secure and accurate. Fine-tuning an AI system requires an immense amount of \\nproprietary information and, as your business advances on the AI maturity curve, the number of models running \\nwill only grow, increasing the demand for data access.\\nThat’s why you need to have the right mechanisms in place to track data from the moment it’s generated to \\nwhen it’s eventually used, and why Unity Catalog is such a popular feature among Databricks customers. With \\nUnity Catalog’s data lineage capabilities, businesses always know where data is moving and who is accessing it.\\nAny Unity Catalog table with primary keys can be used to serve features in GenAI applications using Databricks \\nOnline Tables.\\n33THE BIG BOOK OF GENERATIVE AI\\nFine-Tuning Use Cases\\nCreating a Bespoke LLM for AI-Generated Documentation \\nIt’s easier than you think: 2 engineers, 1 month and less than $1,000\\nby Matthew Hayes, Hongyi Zhang, Tao Feng, Jan van der Vegt, Zaheera Valani and Reynold Xin\\nIn this example, we share our experience from prototyping a hackathon project using off-the-shelf SaaS-based \\nLLMs to creating a bespoke LLM that is better, faster, and cheaper. The new model took 2 engineers, 1 month \\nand less than $1,000 in compute cost to develop. We hope you will find the learnings useful, as we believe \\nthey apply to a wide class of GenAI use cases. More importantly, it has allowed us to take advantage of rapid \\nadvances being made in open-source LLMs.\\nWHAT IS AI-GENERATED DOCUMENTATION?\\nAt the center of each data platform lies a (potentially enormous) collection of datasets (often in the form of \\ntables). In virtually every organization we have worked with, the vast majority of tables are not documented. The \\nabsence of documentation provides a number of challenges, including making it difficult for humans to discover \\nthe data needed for answering a business question, or more recently, for AI agents to automatically find \\ndatasets to use in response to questions (a key capability in our platform that we’re calling Data Intelligence).\\n34THE BIG BOOK OF GENERATIVE AI\\nRather than relying on humans to document these datasets, we prototyped as part of our quarterly hackathon \\na new workflow using an off-the-shelf SaaS-based LLM to automatically generate documentation for tables \\nand their columns based on their schema. This new workflow would automatically suggest descriptions for the \\ntables and columns and allow users to either individually accept, bulk accept, or modify the suggestions for \\nhigher fidelity, as shown below. When we showed this prototype to some users, their immediate question was \\nuniversally, “When can I have it?!”\\n35THE BIG BOOK OF GENERATIVE AI\\nCHALLENGES WITH LLMS\\nAs we moved toward launching this feature to all our customers, we ran into three challenges with the model:\\n1. Quality: The ultimate success of this feature depends on the quality of the generated documentation. \\nAlthough we could measure the quality (in terms of how often they are accepted), we had limited knobs \\nat our disposal to improve it, aside from basic prompting. During the private preview period, we also \\nsometimes noticed the quality of the suggestions degrading, without any change to our codebase. Our \\nspeculation is that the SaaS LLM controller rolled out updates to the model that sometimes affected \\nperformance on specific tasks.\\n2. Performance (throughput): We had limited API quota provisioned with the SaaS LLM provider. We \\nwork with tens of thousands of organizations, and it is not uncommon that a single organization would \\nhave millions of tables. It would take too long to generate documentation for all the tables based on the \\nthroughput quota.\\n3. Cost: Related to the above, it was not cost-effective unless we started charging customers for using this \\nspecific feature.\\nWe have heard similar concerns from a variety of customers as they try to move their LLM-based applications \\nfrom a proof-of-concept to production and saw this as an excellent opportunity for us to explore alternatives \\nfor an organization like ours.\\nWe experimented with different versions of the SaaS LLMs, but they all had the same challenges. This is not \\nsurprising in hindsight. The SaaS LLMs are an engineering marvel, but they are very general models that need to \\naddress all the use cases from table generation to conversing about the meaning of life. The generality means \\nit needs to have an extremely large number of parameters, which limits how fast and how cheap it can return \\nanswers. As it continues to evolve to optimize for different use cases, it might also regress the narrower use case \\nwe have.\\n36THE BIG BOOK OF GENERATIVE AI\\nBUILDING A BESPOKE MODEL\\nTo address the aforementioned challenges, we started building a bespoke model. It took a team of two \\nengineers one month to build a customized, smaller LLM that was better, faster, and cheaper:\\n ■ Quality: Based on our evaluation (see the following section), the model is significantly better than the \\ncheaper version of the SaaS model, and roughly equivalent to the more expensive version.\\n ■ Performance (throughput): Because the bespoke model is a lot smaller, it can fit in A10 GPUs, and we \\ncan increase the inference throughput with horizontal scaling. The smaller GPUs are also more available, \\nwhich enables us to generate the descriptions for all tables faster.\\n ■ Cost: Each fine-tuning run of the model only costs a few dollars, and in aggregate, it cost less than $1000 \\nto develop because we did a lot of experiments. It also resulted in a 10 fold reduction in inference cost.\\nThe first step was to treat this as an applied machine learning problem. “Applied machine learning” sounds \\ndaunting and complicated, but all it meant was that we needed to:\\n ■ Find training datasets so we can bootstrap an initial model\\n ■ Identify an evaluation mechanism so we can measure the quality, before rolling it out to production\\n ■ Train and select models\\n ■ Collect real-world usage metrics, so we can monitor how well a monitor does in production\\n ■ Iterate and roll out new models to continuously improve the three dimensions: quality, performance, cost\\n37THE BIG BOOK OF GENERATIVE AI\\nTRAINING DATA\\nWe created the initial training dataset for this fine-tuning task, using two different sources of data:\\n1. North American Industry Classification System (NAICS) codes. This is a public dataset used by Federal \\nstatistical agencies in classifying business establishments for the purpose of collecting, analyzing, and \\npublishing statistical data related to the U.S. business economy.\\n2. Databricks’ internal use case taxonomy curation datasets. This is a series of internal datasets created by \\nour solution architects to show customers best practice architectures.\\nThen we synthesized CREATE TABLE statements using the above use cases to yield a diverse set of tables and \\ngenerated sample responses including table descriptions and column comments using another LLM. In total,  \\nwe generated ~3600 training examples. \\nNotably, we didn’t use any customer data for training this powerful feature that all of our customers can  \\nbenefit from. \\nBOOTSTRAPPING MODEL EVALUATION\\nAfter the feature launch, we could measure a model’s quality through production metrics such as the rate of \\nusers accepting the suggestions. But before we made it to the launch, we needed a way to evaluate the model’s \\nquality against that of the SaaS LLM.\\nTo do that in an unbiased fashion, we set up a simple double-blind evaluation framework in which we asked \\n4 employees to rate table descriptions generated from the two models we wanted to compare using a set of \\n62 unseen tables. Our framework then generated a sheet where each row showed the input and showed both \\noutputs in a randomized order. The evaluator would vote on the better sample (or give a tie). The framework then \\nprocessed the votes from different evaluators to generate a report; it also summarizes the degree to which each \\nof the evaluators agreed.\\nBased on our experiences so far, having an evaluation dataset of tens to hundreds of data points is a sufficient \\ninitial milestone and can be generalized to other use cases as well.\\n38THE BIG BOOK OF GENERATIVE AI\\nMODEL SELECTION AND FINE-TUNING\\nWe considered the following criteria for model selection:\\n ■ Whether the license supports commercial use\\n ■ Performance (quality) of the model for text generation\\n ■ Speed of the model\\nBased on these criteria, MPT-7B and Llama2-7B were the leading candidates, as shown in our LLM guide. We \\nconsidered larger models such as MPT-30B and Llama-2-13B. In the end we chose MPT-7B, as it has the best \\ncombination of quality and inference performance:\\n ■ There was no discernable difference in the quality between the MPT-7B and Llama-2-7B fine-tuned \\nmodels for this task.\\n ■ The smaller 7B models, after fine-tuning, were already meeting the quality bar. It was significantly better \\nthan the cheaper version of the SaaS model, and roughly equivalent to the more expensive version.\\n ■ We did not yet observe a measurable benefit of using larger models for this task that would justify the \\nincreased serving costs.\\n ■ The latency for the smaller models was significantly better than the larger models while offering \\ncomparable quality so we could deliver a much snappier product experience.\\n ■ The smaller model could fit comfortably and be served using A10 GPUs, which were more readily \\navailable. Their abundance would mean higher inference throughput for the task.\\nThe total time it took to fine-tune the model on the ~3600 examples was only around 15 minutes!\\nWhile we chose MPT-7B for our model, we believe the LLM landscape is changing rapidly and the best model \\ntoday won’t be the best model tomorrow. That’s why we consider this to be an iterative and continuous process \\nand are focused on using tools that make our evaluation efficient and fast.\\n39THE BIG BOOK OF GENERATIVE AI\\nKEY ARCHITECTURAL COMPONENTS OF OUR PRODUCTION PIPELINE\\nWe were able to build this quickly by relying on the following key components of the Databricks Data \\nIntelligence Platform:\\n ■ Databricks LLM fine-tuning: It provides a very simple infrastructure for fine-tuning the models for our \\ntask. We prepared the training data in JSON format, and with a one-line CLI command, we were able to \\nfine-tune the LLMs.\\n ■ Unity Catalog: The models that we use in production are registered in Unity Catalog (UC), providing the \\ngovernance we need to not just for the data, but also the models. With its end-to-end lineage feature, UC \\nalso gives us traceability from the models back to the datasets they are trained on.\\n ■ Delta Sharing: We used Delta Sharing to distribute the model to all production regions we have around \\nthe world for faster serving.\\n ■ Databricks optimized LLM serving: Once the models are registered in UC, they can be served using the \\nnew optimized LLM serving, which provides significant performance improvement in terms of throughput \\nand latency improvement compared to traditional serving for LLM serving.\\n40THE BIG BOOK OF GENERATIVE AI\\nCOST\\nThe fine-tuning compute cost for the whole project was less than $1000 (each fine-tuning run cost only a few \\ndollars). And the final result is a more than 10-fold reduction in cost. Why is the cost-saving so significant? It is \\nnot surprising if we consider the following:\\n ■ As mentioned earlier, the SaaS LLMs need to address all the use cases, including acting as a general \\nchatbot. The generality requires an extremely large number of parameters, which incurs significant \\ncompute costs in inference.\\n ■ When we fine-tune for a more specific task, we can use a much smaller prompt. Larger, general-purpose \\nmodels require longer prompts that include detailed instructions on what the input is and what form the \\noutput should take. Fine-tuned models can bake instructions and expected structure into the model \\nitself. We found we were able to reduce the number of input tokens with no impact on performance by \\nmore than half.\\n ■ Inference costs scale with the number of input and output tokens, and costs scale linearly for SaaS \\nservices that are charged per token. With Databricks’ LLM Serving offering, we offer provisioned \\nthroughput charged per hour, which provides consistent latencies, uptime SLAs, and autoscaling. Because \\nsmaller LLMs can fit in smaller GPUs that are much cheaper and more available and because we offer a \\nhighly optimized runtime, we can aggressively drive down costs. Also, smaller LLMs scale up and down \\nfaster, meaning we can quickly scale up to meet peaks of demand and aggressively scale down when \\nusage is lighter, creating substantial cost efficiency in production.\\n41THE BIG BOOK OF GENERATIVE AI\\nCONCLUSION\\nHaving well-documented data is critical to all data users, and growing more important day-by-day to power \\nAI-based data platforms (what we’re calling Data Intelligence). We started with SaaS LLMs for prototyping this \\nnew GenAI feature but ran into challenges with quality, performance, and cost. We built a bespoke model to do \\nthe same task at better quality, and yet resulting in higher throughput with scale-out and 10x cost reduction. To \\nrecap what it took:\\n ■ 2 engineers\\n ■ 1 month\\n ■ Less than $1,000 in compute for training and experimentation\\n ■ MPT-7B fine-tuned on 3600 synthetically generated examples, in under 15 minutes\\n ■ 4 human evaluators, with 62 initial evaluation examples\\nThis experience demonstrates how easy it is to develop and deploy bespoke LLMs for specific tasks. This model \\nis now live on Databricks in Amazon Web Services and Google Cloud and is being used to power most data \\nannotations on the platform.\\n42THE BIG BOOK OF GENERATIVE AI\\nEfficient Fine-Tuning With LoRA: A Guide to Optimal Parameter Selection for Large Language Models\\nby Avinash Sooriyarachchi\\nWith the rapid advancement of neural network-based techniques and large language model (LLM) research, \\nbusinesses are increasingly interested in AI applications for value generation. They employ various machine \\nlearning approaches, both generative and non-generative, to address text-related challenges such as \\nclassification, summarization, sequence-to-sequence tasks, and controlled text generation. Organizations can \\nopt for third-party APIs, but fine-tuning models with proprietary data offers domain-specific and pertinent \\nresults, enabling cost-effective and independent solutions deployable across different environments in  \\na secure manner.\\nEnsuring efficient resource utilization and cost-effectiveness is crucial when choosing a strategy for fine-\\ntuning. This blog explores arguably the most popular and effective variant of such parameter efficient \\nmethods, Low Rank Adaptation (LoRA), with a particular emphasis on QLoRA (an even more efficient variant of \\nLoRA). The approach here will be to take an open large language model and fine-tune it to generate fictitious \\nproduct descriptions when prompted with a product name and a category. The model chosen for this \\nexercise is OpenLLaMA-3b-v2, an open large language model with a permissive license (Apache 2.0), and the \\ndataset chosen is Red Dot Design Award Product Descriptions, both of which can be downloaded from the \\nHuggingFace Hub at the links provided.\\nFINE-TUNING, LORA AND QLORA\\nIn the realm of language models, fine-tuning an existing language model to perform a specific task on specific \\ndata is a common practice. This involves adding a task-specific head, if necessary, and updating the weights of \\nthe neural network through backpropagation during the training process. It is important to note the distinction \\nbetween this fine-tuning process and training from scratch. In the latter scenario, the model\\'s weights are \\nrandomly initialized, while in fine-tuning, the weights are already optimized to a certain extent during the \\npretraining phase. The decision of which weights to optimize or update, and which ones to keep frozen, depends \\non the chosen technique.\\nFull fine-tuning involves optimizing or training all layers of the neural network. While this approach typically \\nyields the best results, it is also the most resource-intensive and time-consuming.\\n43THE BIG BOOK OF GENERATIVE AI\\nFortunately, there exist parameter-efficient approaches for fine-tuning that have proven to be effective. \\nAlthough most such approaches have yielded less performance, Low Rank Adaptation (LoRA) has bucked \\nthis trend by even outperforming full fine-tuning in some cases, as a consequence of avoiding catastrophic \\nforgetting (a phenomenon which occurs when the knowledge of the pretrained model is lost during the  \\nfine-tuning process).\\nLoRA is an improved fine-tuning method where instead of fine-tuning all the weights that constitute the  \\nweight matrix of the pretrained large language model, two smaller matrices that approximate this larger matrix \\nare fine-tuned. These matrices constitute the LoRA adapter. This fine-tuned adapter is then loaded to the \\npretrained model and used for inference.\\nQLoRA is an even more memory efficient version of LoRA where the pretrained model is loaded to GPU  \\nmemory as quantized 4-bit weights (compared to 8-bits in the case of LoRA), while preserving similar \\neffectiveness to LoRA. Probing this method, comparing the two methods when necessary, and figuring out the \\nbest combination of QLoRA hyperparameters to achieve optimal performance with the quickest training time \\nwill be the focus here.\\nLoRA is implemented in the Hugging Face Parameter Efficient Fine-Tuning (PEFT) library, offering ease  \\nof use and QLoRA can be leveraged by using bitsandbytes and PEFT together. HuggingFace Transformer \\nReinforcement Learning (TRL) library offers a convenient trainer for supervised fine-tuning with seamless \\nintegration for LoRA. These three libraries will provide the necessary tools to fine-tune the chosen pretrained \\nmodel to generate coherent and convincing product descriptions once prompted with an instruction  \\nindicating the desired attributes.\\n44THE BIG BOOK OF GENERATIVE AI\\nPREPPING THE DATA FOR SUPERVISED FINE-TUNING\\nTo probe the effectiveness of QLoRA for fine-tuning a model for instruction following, it is essential to transform \\nthe data to a format suited for supervised fine-tuning. Supervised fine-tuning in essence, further trains a \\npretrained model to generate text conditioned on a provided prompt. It is supervised in that the model is fine-\\ntuned on a dataset that has prompt-response pairs formatted in a consistent manner.\\nAn example observation from our chosen dataset from the Hugging Face hub looks as follows:\\nAs useful as this dataset is, this is not well formatted for fine-tuning of a language model for instruction following \\nin the manner described.\\nThe following code snippet loads the dataset from the Hugging Face hub into memory, transforms the \\nnecessary fields into a consistently formatted string representing the prompt, and inserts the response (i.e., \\nthe description), immediately afterward. This format is known as the ‘Alpaca format’ in large language model \\nresearch circles as it was the format used to fine-tune the original LlaMA model from Meta to result in the \\nAlpaca model, one of the first widely distributed instruction-following large language models (although not \\nlicensed for commercial use).\\nPRODUCT CATEGORY DESCRIPTION TEXT\\n“Biamp Rack Products” “Digital Audio Processors\" “High recognition value, uniform \\naesthetics and practical \\nscalability — this has been \\nimpressively achieved with the \\nBiamp brand language . . . “\\n“Product Name: Biamp Rack Products; \\nProduct Category: Digital Audio \\nProcessors; Product Description: High \\nrecognition value, uniform aesthetics \\nand practical scalability — this has been \\nimpressively achieved with the Biamp \\nbrand language . . . “\\n45THE BIG BOOK OF GENERATIVE AI\\n \\n1\\n2\\n3\\n4 \\n5\\n6\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\nimport pandas as pd\\nfrom datasets import load_dataset\\nfrom datasets import Dataset\\n#Load the dataset from the HuggingFace Hub\\nrd_ds = load_dataset(\"xiyuez/red-dot-design-award-product-description\")\\n#Convert to pandas dataframe for convenient processing\\nrd_df = pd.DataFrame(rd_ds[\\'train\\'])\\n#Combine the two attributes into an instruction string\\nrd_df[\\'instruction\\'] = \\'Create a detailed description for the following product: \\'+ rd_df[\\'product\\']+\\', belonging to \\ncategory: \\'+ rd_df[\\'category\\']\\nrd_df = rd_df[[\\'instruction\\', \\'description\\']]\\n#Get a 5000 sample subset for fine-tuning purposes\\nrd_df_sample = rd_df.sample(n=5000, random_state=42)\\n#Define template and format data into the template for supervised fine-tuning\\ntemplate = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the \\nrequest.\\n### Instruction:\\n{}\\n### Response:\\\\n\"\"\"\\nrd_df_sample[\\'prompt\\'] = rd_df_sample[\"instruction\"].apply(lambda x: template.format(x))\\nrd_df_sample.rename(columns={\\'description\\': \\'response\\'}, inplace=True)\\nrd_df_sample[\\'response\\'] = rd_df_sample[\\'response\\'] + \"\\\\n### End\"\\nrd_df_sample = rd_df_sample[[\\'prompt\\', \\'response\\']]\\nrd_df[\\'text\\'] = rd_df[\"prompt\"] + rd_df[\"response\"]\\nrd_df.drop(columns=[\\'prompt\\', \\'response\\'], inplace=True)\\nThe resulting prompts are then loaded into a hugging face dataset for supervised fine-tuning. Each such prompt \\nhas the following format.\\n46THE BIG BOOK OF GENERATIVE AI\\n \\n1\\n2\\n3\\n4 \\n5\\n6\\n8\\n9\\n10\\n11\\n12\\n13\\n```\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n### Instruction:\\nCreate a detailed description for the following product: Beseye Pro, belonging to category: Cloud-Based Home Security \\nCamera\\n### Response:\\nBeseye Pro combines intelligent home monitoring with decorative art. The camera, whose form is reminiscent of a water \\ndrop, is secured in the mounting with a neodymium magnet and can be rotated by 360 degrees. This allows it to be \\neasily positioned in the desired direction. The camera also houses modern technologies, such as infrared LEDs, cloud-\\nbased intelligent video analyses and SSL encryption.\\n### End\\n```\\nTo facilitate quick experimentation, each fine-tuning exercise will be done on a 5000 observation subset  \\nof this data.\\nTESTING MODEL PERFORMANCE BEFORE FINE-TUNING\\nBefore any fine-tuning, it’s a good idea to check how the model performs without any fine-tuning to get a \\nbaseline for pretrained model performance.\\nThe model can be loaded in 8-bit as follows and prompted with the format specified in the model card on \\nHugging Face.\\n47THE BIG BOOK OF GENERATIVE AI\\n \\n1\\n2\\n3\\n4 \\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\nimport torch\\nfrom transformers import LlamaTokenizer, LlamaForCausalLM\\nmodel_path = \\'openlm-research/open_llama_3b_v2\\'\\ntokenizer = LlamaTokenizer.from_pretrained(model_path)\\nmodel = LlamaForCausalLM.from_pretrained(\\nmodel_path, load_in_8bit=True, device_map=\\'auto\\',\\n)\\n#Pass in a prompt and infer with the model\\nprompt = \\'Q: Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: \\nOptical Mouse\\\\nA:\\'\\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\\ngeneration_output = model.generate(\\ninput_ids=input_ids, max_new_tokens=128\\n)\\nprint(tokenizer.decode(generation_output[0]))\\n \\n1\\n2\\n3\\n4 \\n5\\n6\\nQ: Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical \\nMouse A: The Corelogic Smooth Mouse is a wireless optical mouse that has a 1000 dpi resolution. It has a 2.4 GHz \\nwireless connection and a 12-month warranty. Q: What is the price of the Corelogic Smooth Mouse? A: The Corelogic \\nSmooth Mouse is priced at $29.99. Q: What is the weight of the Corelogic Smooth Mouse? A: The Corelogic Smooth Mouse \\nweighs 0.1 pounds. Q: What is the dimensions of the Corelogic Smooth Mouse? A: The Corelogic Smooth Mouse has a \\ndimension\\nThe output obtained is not quite what we want.\\nThe first part of the result is actually satisfactory, but the rest of it is more of a rambling mess.\\n48THE BIG BOOK OF GENERATIVE AI\\n \\n1\\n2\\n3\\n4 \\n5\\n6\\n7\\n8\\n9\\nprompt= \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n### Instruction:\\nCreate a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical Mouse\\n### Response:\"\"\"\\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\\ngeneration_output = model.generate(\\ninput_ids=input_ids, max_new_tokens=128\\n)\\nprint(tokenizer.decode(generation_output[0]))\\n \\n1\\n2\\n3\\n4 \\n5\\n6\\nCorelogic Smooth Mouse is a mouse that is designed to be used by people with disabilities. It is a wireless mouse \\nthat is designed to be used by people with disabilities. It is a wireless mouse that is designed to be used by people \\nwith disabilities. It is a wireless mouse that is designed to be used by people with disabilities. It is a wireless \\nmouse that is designed to be used by people with disabilities. It is a wireless mouse that is designed to be used by \\npeople with disabilities. It is a wireless mouse that is designed to be used by people with disabilities. It is a \\nwireless mouse that is designed to be used by\\nSimilarly, if the model is prompted with the input text in the ‘Alpaca format’ as discussed before, the output is \\nexpected to be just as suboptimal:\\nAnd sure enough, it is:\\nThe model performs what it was trained to do, predicts the next most probable token. The point of supervised \\nfine-tuning in this context is to generate the desired text in a controllable manner. Please note that in the \\nsubsequent experiments, while QLoRA leverages a model loaded in 4-bit with the weights frozen, the  \\ninference process to examine output quality is done once the model has been loaded in 8-bit as shown  \\nabove for consistency.\\n49THE BIG BOOK OF GENERATIVE AI\\nTHE TURNABLE KNOBS\\nWhen using PEFT to train a model with LoRA or QLoRA (note that, as mentioned before, the primary difference \\nbetween the two is that in the latter, the pretrained models are frozen in 4-bit during the fine-tuning process), \\nthe hyperparameters of the low rank adaptation process can be defined in a LoRA config as shown below:\\nTwo of these hyperparameters, r and target_modules are empirically shown to affect adaptation quality \\nsignificantly and will be the focus of the tests that follow. The other hyperparameters are kept constant at the \\nvalues indicated above for simplicity.\\nr represents the rank of the low rank matrices learned during the fine-tuning process. As this value is increased, \\nthe number of parameters needed to be updated during the low-rank adaptation increases. Intuitively, a lower \\nr may lead to a quicker, less computationally intensive training process, but may affect the quality of the model \\nthus produced. However, increasing r beyond a certain value may not yield any discernible increase in quality of \\nmodel output. How the value of r affects adaptation (fine-tuning) quality will be put to the test shortly.\\n \\n1\\n2\\n3\\n4 \\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\nfrom peft import LoraConfig\\n...\\n...\\n#If only targeting attention blocks of the model\\ntarget_modules = [\"q_proj\", \"v_proj\"]\\n#If targeting all linear layers\\ntarget_modules = [\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'o_proj\\',\\'gate_proj\\',\\'down_proj\\',\\'up_proj\\',\\'lm_head\\']\\nlora_config = LoraConfig(\\nr=16,\\ntarget_modules = target_modules,\\nlora_alpha=8,\\nlora_dropout=0.05,\\nbias=\"none\",\\ntask_type=\"CAUSAL_LM\",}\\n50THE BIG BOOK OF GENERATIVE AI\\nWhen fine-tuning with LoRA, it is possible to target specific modules in the model architecture. The adaptation \\nprocess will target these modules and apply the update matrices to them. Similar to the situation with \"r,\" \\ntargeting more modules during LoRA adaptation results in increased training time and greater demand for \\ncompute resources. Thus, it is a common practice to only target the attention blocks of the transformer. \\nHowever, recent work as shown in the QLoRA paper by Dettmers et al. suggests that targeting all linear layers \\nresults in better adaptation quality. This will be explored here as well.\\nNames of the linear layers of the model can be conveniently appended to a list with the following code snippet:\\nTUNING THE FINE-TUNING WITH LORA\\nThe developer experience of fine-tuning large language models in general have improved dramatically over the \\npast year or so. The latest high level abstraction from Hugging Face is the SFTTrainer class in the TRL library. To \\nperform QLoRA, all that is needed is the following:\\n1. Load the model to GPU memory in 4-bit (bitsandbytes enables this process)\\n2. Define the LoRA configuration as discussed previously\\n3. Define the train and test splits of the prepped instruction following data into Hugging Face  \\nDataset objects\\n4. Define training arguments: These include the number of epochs, batch size and other training \\nhyperparameters which will be kept constant during this exercise\\n5. Pass these arguments into an instance of SFTTrainer\\nThese steps are clearly indicated in the source file in the repository associated with this blog.\\n \\n1\\n2\\n3\\n4 \\n5\\n6\\n7\\n8\\n9\\nimport re\\nmodel_modules = str(model.modules)\\npattern = r\\'\\\\((\\\\w+)\\\\): Linear\\'\\nlinear_layer_names = re.findall(pattern, model_modules)\\nnames = []\\n# Print the names of the Linear layers\\nfor name in linear_layer_names:\\n    names.append(name)\\ntarget_modules = list(set(names))\\n51THE BIG BOOK OF GENERATIVE AI\\nThe actual training logic is abstracted away nicely as follows:\\nIf MLflow autologging is enabled in the Databricks workspace, which is highly recommended, all the training \\nparameters and metrics are automatically tracked and logged with the MLflow tracking server. This functionality \\nis invaluable in monitoring long-running training tasks. Needless to say, the fine-tuning process is performed \\nusing a compute cluster (in this case, a single node with a single A100 GPU) created using the latest Databricks \\nMachine Runtime with GPU support.\\n \\n1\\n2\\n3\\n4 \\n5\\n6\\n7\\n8\\n9\\n10\\n11\\ntrainer = SFTTrainer(\\nmodel,\\ntrain_dataset=dataset[\\'train\\'],\\neval_dataset = dataset[\\'test\\'],\\ndataset_text_field=\"text\",\\nmax_seq_length=256,\\nargs=training_args,\\n)\\n# Initiate the training process\\nwith mlflow.start_run(run_name= ‘run_name_of_choice’):\\ntrainer.train()\\n52THE BIG BOOK OF GENERATIVE AI\\nHYPERPARAMETER COMBINATION #1: QLoRA with r=8 and targeting “q_proj”, “v_proj”\\nThe first combination of QLoRA hyperparameters attempted is r=8 and targets only the attention blocks, namely \\n“q_proj” and “v_proj” for adaptation.\\nThe following code snippets gives the number of trainable parameters:\\nThese choices result in 2,662,400 parameters being updated during the fine-tuning process (~2.6 million) from a \\ntotal of ~3.2 billion parameters the model consists of. This is less than 0.1% of the model parameters. The entire \\nfine-tuning process on a single Nvidia A100 with 80 GBs of GPU for 3 epochs only takes roughly 12 minutes. The \\nGPU utilization metrics can be conveniently viewed at the metrics tab of the cluster configurations.\\n \\n1\\n2\\nmodel = get_peft_model(model, lora_config)\\nmodel.print_trainable_parameters()\\n53THE BIG BOOK OF GENERATIVE AI\\nAt the end of the training process, the fine-tuned model is obtained by loading the adapter weights to the \\npretrained model as follows:\\nThis model can now be used for inference as any other model.\\nQualitative Evaluation \\nA couple of example prompt-response pairs are listed below\\nPrompt (passed to the model in the Alpaca format, not shown for conciseness here): \\nCreate a detailed description for the following product: Corelogic Smooth Mouse, belonging to category:  \\nOptical Mouse\\nResponse:\\nPrompt: \\nCreate a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless \\nVacuum Cleaner\\nResponse:\\nThe model has clearly been adapted for generating more consistent descriptions. However the response to the \\nfirst prompt about the optical mouse is quite short and the following phrase “The vacuum cleaner is equipped \\nwith a dust container that can be emptied via a dust container” is logically flawed.\\n \\n1 peft_model = PeftModel.from_pretrained(model, adapter_location)\\n \\n1\\n2\\nThe Corelogic Smooth Mouse is a wireless optical mouse with a smooth surface. The mouse is equipped with a 1000 DPI \\nsensor and a 1000 Hz polling rate. The mouse is available in black and white.\\n \\n1\\n2\\n3\\n4\\nThe Hoover Lightspeed is a cordless vacuum cleaner that is equipped with a lithium-ion battery. The battery is \\ncharged via a USB cable. The vacuum cleaner is equipped with a 2-in-1 brush and a turbo brush. The brush is suitable \\nfor cleaning carpets and hard floors. The turbo brush is suitable for cleaning carpets and hard floors. The vacuum \\ncleaner is equipped with a dust container that can be emptied via a dust container.\\n54THE BIG BOOK OF GENERATIVE AI\\nHYPERPARAMETER COMBINATION #2: QLoRA with r=16 and targeting all linear layers\\nSurely, things can be improved here. It is worth exploring increasing the rank of low rank matrices learned during \\nadaptation to 16, i.e., double the value of r to 16 and keep all else  the same. This doubles the number of trainable \\nparameters to 5,324,800 (~5.3 million)\\nQualitative Evaluation \\nThe quality of output, however, remains unchanged for the same exact prompts.\\nPrompt: \\nCreate a detailed description for the following product: Corelogic Smooth Mouse, belonging to category:  \\nOptical Mouse\\nResponse:\\n \\n1\\n2\\nThe Corelogic Smooth Mouse is a high-quality optical mouse with a smooth surface. The mouse is equipped with a 1000 \\nDPI sensor and a 1000 Hz polling rate. The mouse is available in black and white.\\n \\n1\\n2\\n3\\n4\\n5\\nThe Hoover Lightspeed is a cordless vacuum cleaner that is equipped with a lithium-ion battery. The battery is \\ncharged via a USB cable. The vacuum cleaner is equipped with a 2-in-1 brush and a motorized brush. The brush is used \\nto clean hard floors and the motorized brush is used to clean carpets. The vacuum cleaner is equipped with a dust \\ncontainer that can be emptied via a dust container. The vacuum cleaner is equipped with a LED display that shows the \\nremaining battery capacity.\\nPrompt: \\nCreate a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless \\nVacuum Cleaner\\nResponse:\\nThe same lack of detail and logical flaws in detail where details are available persists. If this fine tuned model is \\nused for product description generation in a real-world scenario, this is not acceptable output.\\n55THE BIG BOOK OF GENERATIVE AI\\nHYPERPARAMETER COMBINATION #3: QLoRA with r=8 and targeting all linear layers\\nGiven that doubling r does not seemingly result in any perceivable increase in output quality, it is worth \\nchanging the other important knob. i.e., targeting all linear layers instead of just the attention blocks. Here, the \\nLoRA hyperparameters are r=8 and target_layers are  \\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'o_proj\\',\\'gate_proj\\',\\'down_proj\\',\\'up_\\nproj\\' and \\'lm_head\\'. This increases the number of parameters updated to 12,994,560 and increases the training \\ntime to roughly 15.5 minutes.\\nQualitative Evaluation \\nPrompting the model with the same prompts yield the following:\\nPrompt: \\nCreate a detailed description for the following product: Corelogic Smooth Mouse, belonging to category:  \\nOptical Mouse\\nResponse:\\nPrompt: \\nCreate a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless \\nVacuum Cleaner\\nResponse:\\nNow it is possible to see a somewhat longer coherent description of the fictitious optical mouse and there are \\nno logical flaws in the description of the vacuum cleaner. The product descriptions are not only logical, but \\nrelevant. Just as a reminder, these relatively high-quality results are obtained by fine-tuning less than a 1% of the \\nmodel’s weights with a total dataset of 5000 such prompt-description pairs formatted in a consistent manner.\\n \\n1\\n2\\n3\\nThe Corelogic Smooth Mouse is a wireless optical mouse with a smooth surface. The mouse is equipped with a 1000 DPI \\nsensor and a 1000 Hz polling rate. The mouse is equipped with a 2.4 GHz wireless connection and a USB receiver. The \\nmouse is available in black and white.\\n \\n1\\n2\\n3\\n4\\nThe Hoover Lightspeed cordless vacuum cleaner is a high-performance device that is easy to use. The device is \\nequipped with a lithium-ion battery that provides up to 40 minutes of runtime. The device is equipped with a \\nhigh-performance motor that provides a suction power of up to 200 watts. The device is equipped with a 2-in-1 dusting \\nbrush and a crevice tool. The device is equipped with a LED display that shows the remaining battery charge.\\n56THE BIG BOOK OF GENERATIVE AI\\nHYPERPARAMETER COMBINATION #4: LoRA with  r=8 and targeting all linear transformer layers\\nIt is also worth exploring whether the quality of output from the model improves if the pretrained model is \\nfrozen in 8-bit instead of 4-bit. In other words, replicating the exact fine-tuning process using LoRA instead of \\nQLoRA. Here, the LoRA hyperparameters are kept the same as before, in the new-found optimal configuration, \\ni.e., r=8 and targeting all linear transformer layers during the adaptation process.\\nQualitative Evaluation \\nThe results for the two prompts used throughout the article are as given below:\\nPrompt: \\nCreate a detailed description for the following product: Corelogic Smooth Mouse, belonging to category:  \\nOptical Mouse\\nResponse:\\nPrompt: \\nCreate a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless \\nVacuum Cleaner\\nResponse:\\nAgain, there isn’t much of an improvement in the quality of the output text. \\n \\n1\\n2\\n3\\nThe Corelogic Smooth Mouse is a high-quality optical mouse with a smooth surface. The mouse is equipped with a 1000 \\nDPI sensor and a 1000 Hz polling rate. The mouse is available in three colours: black, white and red. The mouse is \\nequipped with a USB receiver and is compatible with Windows and Mac.\\n \\n1\\n2\\n3 \\n4\\nThe Hoover Lightspeed cordless vacuum cleaner is a compact and lightweight device that is easy to use. The device is \\nequipped with a lithium-ion battery that provides up to 40 minutes of cleaning time. The vacuum cleaner is equipped \\nwith a high-performance filter that ensures that the air is cleaned of dust and allergens. The device is equipped \\nwith a 2-in-1 dusting brush and a crevice tool that can be used to clean hard-to-reach areas.\\n57THE BIG BOOK OF GENERATIVE AI\\nKEY OBSERVATIONS\\nBased on the above set of trials, and further evidence detailed in the excellent publication presenting QLoRA, \\nit can be deduced that the value of r (the rank of matrices updated during adaptation) does not improve \\nadaptation quality beyond a certain point. The biggest improvement is observed in targeting all linear layers \\nin the adaptation process, as opposed to just the attention blocks, as commonly documented in technical \\nliterature detailing LoRA and QLoRA. The trials executed above and other empirical evidence suggest that \\nQLoRA does not indeed suffer from any discernible reduction in quality of text generated, compared to LoRA.\\nFURTHER CONSIDERATIONS FOR USING LORA ADAPTERS IN DEPLOYMENT\\nIt\\'s important to optimize the usage of adapters and understand the limitations of the technique. The size of the \\nLoRA adapter obtained through fine-tuning is typically just a few megabytes, while the pretrained base model \\ncan be several gigabytes in memory and on disk. During inference, both the adapter and the pretrained LLM \\nneed to be loaded, so the memory requirement remains similar.\\nFurthermore, if the weights of the pre-trained LLM and the adapter aren’t merged, there will be a slight increase \\nin inference latency. Fortunately, with the PEFT library, the process of merging the weights with the adapter can \\nbe done with a single line of code as shown here:\\nThe figure below outlines the process from fine-tuning an adapter to model deployment.\\n \\n1 merged_model = peft_model.merge_and_unload()\\n58THE BIG BOOK OF GENERATIVE AI\\nWhile the adapter pattern offers significant benefits, merging adapters is not a universal solution. One \\nadvantage of the adapter pattern is the ability to deploy a single large pretrained model with task-specific \\nadapters. This allows for efficient inference by utilizing the pretrained model as a backbone for different \\ntasks. However, merging weights makes this approach impossible. The decision to merge weights depends on \\nthe specific use case and acceptable inference latency. Nonetheless, LoRA/ QLoRA continues to be a highly \\neffective method for parameter efficient fine-tuning and is widely used.\\nCONCLUSION\\nLow Rank Adaptation is a powerful fine-tuning technique that can yield great results if used with the right \\nconfiguration. Choosing the correct value of rank and the layers of the neural network architecture to target \\nduring adaptation could decide the quality of the output from the fine-tuned model. QLoRA results in further \\nmemory savings while preserving the adaptation quality. Even when the fine-tuning is performed,  there are \\nseveral important engineering considerations to ensure the adapted model is deployed in the correct manner.\\nIn summary, a concise table indicating the different combinations of LoRA parameters attempted, text quality \\noutput and number of parameters updated when fine-tuning OpenLLaMA-3b-v2 for 3 epochs on 5000 \\nobservations on a single A100 is shown below.\\nTry this on Databricks! Clone the GitHub repository associated with the blog into a Databricks Repo to get \\nstarted. More thoroughly documented examples to fine-tune models on Databricks are available here.\\nR TARGET_MODULES BASE MODEL \\nWEIGHTS QUALITY OF OUTPUT NUMBER OF PARAMETERS UPDATED  \\n(IN MILLIONS)\\n8 Attention blocks 4 low 2.662\\n16 Attention blocks 4 low 5.324\\n8 All linear layers 4 high 12.995\\n8 All linear layers 8 high 12.995\\n59THE BIG BOOK OF GENERATIVE AI\\nStage 4: Pretraining\\nPretraining a model from scratch refers to the process of training a language model on a large corpus of data \\n(e.g., text, code) without using any prior knowledge or weights from an existing model. This is in contrast to fine-\\ntuning, where an already pretrained model is further adapted to a specific task or dataset. The output of full \\npretraining is a base model that can be directly used or further fine-tuned for downstream tasks.\\nWHEN TO USE PRETRAINING\\nChoosing to pretrain an LLM from scratch is a significant commitment, both in terms of data and computational \\nresources. Here are some scenarios where it makes sense:\\n1. Unique data sources: If you possess a unique and extensive corpus of data that is distinct from what \\navailable pretrained LLMs have seen, it might be worth pretraining a model to capture this uniqueness\\n2. Domain specificity: Organizations might want a base model tailored to their specific domain (e.g., \\nmedical, legal, code) to ensure even the foundational knowledge of the model is domain-specific\\n3. Full control over training data: Pretraining from scratch offers transparency and control over the data \\nthe model is trained on. This may be essential for ensuring data security, privacy and custom tailoring of \\nthe model’s foundational knowledge.\\n4. Avoiding third-party biases: Pretraining ensures that your LLM application does not inherit biases or \\nlimitations from third-party pretrained models.\\n60THE BIG BOOK OF GENERATIVE AI\\nPRETRAINING IN PRACTICE\\nGiven the resource-intensive nature of pretraining, careful planning and sophisticated tooling are required. \\nLibraries like PyTorch FSDP and Deepspeed, mentioned in the fine-tuning section, are similarly required for \\ntheir distributed training capabilities when pretraining an LLM from scratch. The following only scratches the \\nsurface on some of the considerations one must take into account when pretraining an LLM: \\n ■ Large-scale data preprocessing: A pretrained model is only as good as the data it is trained on. Thus, \\nit becomes vitally important to ensure robust data preprocessing is conducted prior to model training. \\nGiven the scale of the training data involved, this preprocessing typically requires distributed frameworks \\nlike Apache Spark™. Consideration must be given to factors such as dataset mix and deduplication \\ntechniques to ensure the model is exposed to a wide variety of unique data points.\\n ■ Hyperparameter selection and tuning: Before executing full-scale training of an LLM, determining \\nthe set of optimal hyperparameters is crucial. Given the high computational cost associated with LLM \\ntraining, extensive hyperparameter sweeps are not always feasible. Instead, informed decisions based \\non smaller-scale searches or prior research are employed. Once a promising set is identified, these \\nhyperparameters are used for the full training run. Tooling like MLflow is essential to manage and track \\nthese experiments.\\n ■ Maximizing resource utilization: Given the high costs associated with long-running distributed GPU \\ntraining jobs, it is hugely important to maximize resource utilization. MosaicML’s composer is an example \\nof a library that uses PyTorch FSDP with additional optimizations to maximize Model FLOPs Utilization \\n(MFU) and Hardware FLOPs Utilization (HFU) during training.\\n ■ Handling GPU failures: Training large models can run for days or even weeks. During such large-scale \\ntraining for this length of time, hardware failures, especially GPU failures, can (and typically do) occur.  \\nIt is essential to have mechanisms in place to handle such failures gracefully. \\n ■ Monitoring and evaluation: Close monitoring of the training process is essential. Saving model \\ncheckpoints regularly and evaluating validation sets not only act as safeguards but also provide insights \\ninto model performance and convergence trends.\\nIn cases where pretraining an LLM from scratch is required, Mosaic AI Training provides a platform to conduct \\ntraining of multibillion-parameter models in a highly optimized and automated manner. Automatically handling \\nGPU failures and resuming training without human intervention and leveraging Mosaic AI Streaming for efficient \\nstreaming of data into the training process are just some of the capabilities provided out of the box.\\n61THE BIG BOOK OF GENERATIVE AI\\nThe Value of Training Models From Scratch on Databricks \\nAfter diving into the details of starting a model’s training from scratch, why you might do it and the advanced \\ntools needed, let’s look at a real-world example to show that training top-notch language models isn’t as \\ncomplex or expensive as it might seem. This shift highlights that even organizations watching their budget can \\nstart training their own models, with Databricks providing the necessary support and infrastructure. Databricks \\nstands out as uniquely capable to help customers train their own models from scratch, enabling them to fully \\nown their AI assets.\\nPretraining Use Cases\\nTraining Stable Diffusion From Scratch for <$50K With MosaicML\\nby Mihir Patel, Cory Stephenson, Landan Seguin, Austin Jacobson and Erica Ji Yuen\\nWe’ve replicated Stable Diffusion 2 for less than $50K, and we’ve open sourced the training code so you can \\ntoo! This is a 3x cost reduction from our last blog post and an 8x reduction from the original Stable Diffusion 2, \\nmaking training large-scale diffusion models from scratch more accessible than ever before.\\nToday, we are excited to show the results of our own training run: under $50K to train Stable Diffusion 2 base1 \\nfrom scratch in 7.45 days using the MosaicML platform.\\n62THE BIG BOOK OF GENERATIVE AI\\nFigure 1: Imagining mycelium couture. Integrating image generation into the design process pushes creative boundaries. All images in this mood board \\nwere created with our internal diffusion model trained from scratch on the MosaicML Platform.\\nTraining your own image generation model on your own data is now easy and accessible. By training your own \\ndiffusion models, you can:\\n ■ Use your proprietary data\\n ■ Tune the representations for certain art or photography styles\\n ■ Avoid violating intellectual property laws so your models can be used commercially\\nWe’ve open sourced our code and methods to train a diffusion model from scratch so that you can train your \\nown; check it out here! If you\\'re interested in training your own models, contact us for a demo, and read on to \\nlearn more about our engineering setup!\\n63THE BIG BOOK OF GENERATIVE AI\\nSETUP\\nModel: Our diffusion model is a ComposerModel \\ncomposed of a Variational Autoencoder (VAE), a \\nCLIP model, a U-Net, and a diffusion noise scheduler, \\nall from the HuggingFace\\'s Diffusers library. All of \\nthe model configurations were based on stabilityai/\\nstable-diffusion-2-base.\\nFigure 2: Getting creative and embracing serendipity. A variety of subjects, art, and photography styles are generated by our diffusion model.\\nFigure 3: Simplified diagram of the diffusion model.\\n64THE BIG BOOK OF GENERATIVE AI\\nData: We trained on a subset of LAION-5B that includes samples with English-only captions and an aesthetic \\nscore of 4.5+. Similar to Stable Diffusion 2 base, we did two phases of training based on the image resolution of \\nthe training data. For the first phase of training, we used all images with resolution >=256x256, amounting to 790 \\nmillion image-caption samples. For the second phase of training, we only used images with resolution >=512x512, \\namounting to 300 million image-caption samples.\\nCompute: Both phases of training ran on 128 NVIDIA A100 GPUs. The first training phase was run for 550k \\niterations in 1.6 days while the second phase was run for 850k iterations in 4.9 days, for a total of 20,051 A100 \\nhours for training. In addition to the training time, we pre-computed the latents for the VAE and CLIP model \\nto reduce training time and cost when making multiple passes over the dataset. Pre-computing the latents \\nrequired an additional 3,784 A100 hours, resulting in 23,835 A100 hours in total. Assuming a cost of $2 / A100 \\nhour, the total price tag is $47.7k.\\nTech Stack: We used Composer for our training framework, StreamingDataset to load our 100TB of data, and \\nthe MosaicML platform for overcoming infrastructure challenges when training and evaluating on 128 GPUs.\\nFigure 4: Loss curve for our training run. Our platform caught two hardware failures and automatically restarted the run with no human intervention. \\nThe loss discontinuity is because phase 2 increases the resolution from 256x256 to 512x512.\\n65THE BIG BOOK OF GENERATIVE AI\\nCHALLENGES AND SOLUTIONS\\nWhether for diffusion models or large language models, training at scale has significant challenges. We trained \\nour diffusion model using the MosaicML platform, which addresses these challenges automatically so you can \\nfocus on training the best possible model. Below are three main challenges with large-scale training and how our \\nplatform solves them.\\nINFRASTRUCTURE\\nTraining large models on large datasets requires significant compute. The MosaicML platform effortlessly \\norchestrates hundreds of GPUs on any cloud provider. For example, our primary training run took place on \\na cluster of 128 A100 GPUs. To ensure evaluating the model didn\\'t slow training, we automatically kicked off \\nevaluation runs at every checkpoint on different clusters using different cloud providers, seamlessly scaling up \\nto 64 GPUs and back down to 8 GPUs depending on availability.\\nEven after training is underway, software or hardware failures can halt training, leaving GPUs idle until someone \\nnotices or requiring someone on-call 24/7 to babysit the run. Thankfully, the Node Doctor and Watchdog \\nfeatures of the MosaicML platform automatically detect failed nodes and resume jobs as needed. With auto-\\nresumption, we recover from failures and continue training with zero human intervention, avoiding expensive \\ndowntime and human babysitting. Just launch and train!\\nEFFICIENT SOFTWARE\\nSoftware is difficult to configure optimally. Our PyTorch-based Composer library maximizes training efficiency \\nat scale. As shown in our previous blog post, Composer demonstrated excellent throughput scaling as the \\nnumber of GPUs increased. For this update, we added further optimizations (Low Precision GroupNorm and Low \\nPrecision LayerNorm, Fully Sharded Data Parallel) to achieve near-perfect strong scaling up to 128 GPUs, bringing \\nthe cost down to $50k. We also used Composer\\'s native Exponential Moving Average (EMA) algorithm, which \\nallowed us to start EMA close to the end of training (iteration 800k of the final phase) to gain all the benefits of \\nEMA while saving on memory and compute for the majority of training.\\n66THE BIG BOOK OF GENERATIVE AI\\nMANAGING 100TB OF DATA\\nWe trained with a subset of LAION-5B that contained 790 million samples, amounting to >100TB of data. The \\nsheer size of the dataset makes it difficult to manage, especially when working with multiple clusters with \\nseparate local storage. The MosaicML StreamingDataset library makes working with massive datasets much \\nsimpler and faster. There were three key features of the StreamingDataset library that were especially useful for \\nthis training run:\\n1. Mixing datasets stored in different locations. We bucketed samples based on image resolution into \\ndifferent datasets. At training time, we used the MosaicML StreamingDataset library to train on a mixture \\nof resolutions from these datasets.\\n2. Instant mid-epoch resumption. We were able to instantly resume training in the middle of an epoch. This \\nsaved hours by avoiding the need to iterate over the entire dataset to get back to where we left off.\\n3. Elastic determinism. The MosaicML StreamingDataset library deterministically shuffles data, even when \\nchanging the number of GPUs used for training. This made it possible for us to exactly reproduce training \\nruns, dramatically simplifying debugging.\\nHUMAN EVALUATION RESUL TS\\nEvaluating image generation models is difficult, and there is no substitute for human evaluation. In a blind human \\nevaluation, we measured user preferences in image quality and prompt alignment between Stable Diffusion 2 \\nand our diffusion model. Based on user preferences, we concluded that the two models were comparable in \\nquality (see Figure 5) All images were generated based on prompts from the Drawbench benchmark proposed \\nin the Imagen paper. For more details, see our follow-up blog post coming soon.\\n67THE BIG BOOK OF GENERATIVE AI\\nFigure 5: Results from our human evaluation of image quality (left) and prompt alignment (right). Error bars show 95% confidence intervals. In both ex-\\nperiments, the difference in user preference rates between the two models was comparable to the uncertainty in the measurement, so we conclude \\nthat the two models are of comparable overall quality.\\nDeep Dive: How We Trained Stable Diffusion for Less Than $50K \\nby Mihir Patel, Erica Ji Yuen, Cory Stephenson and Landan Seguin\\nIn our previous example, we showed how we used the MosaicML platform, Streaming datasets, and the \\nComposer library to train a Stable Diffusion model from scratch for less than $50,000. Now, we do a deep dive \\ninto the technical details behind this speedup, demonstrating how we were able to replicate the Stable Diffusion \\n2 base model in just 6.8 days.\\nTry out our code here!\\nMany organizations require high-performing large AI models tailored to their specific use cases. However, \\ntraining such models is often prohibitively time-consuming and expensive, requiring vast amounts of \\ncomputation and expertise. This is where MosaicML comes in: we provide a comprehensive solution that \\nsimplifies and accelerates the process of training these models.\\n68THE BIG BOOK OF GENERATIVE AI\\nIn our previous blog post, we announced that we have trained a diffusion model comparable to Stable Diffusion \\n2 from scratch for $47.7K. In this post, we dive into the technical details to highlight how we achieved an 8x \\nspeedup/cost reduction from the number reported by StabilityAI and a 3x cost reduction over our own \\nbaseline. All our code is open source and easy to modify for custom use cases. If you\\'re interested in learning \\nmore about our stack, please contact us for a demo.\\nACCELERATING TRAINING\\nWe’ve introduced a variety of techniques, from fusions to sharding strategies, that dramatically speed up \\ntraining and lower costs by almost 3x.\\nFigure 1: Stable Diffusion 2 model architecture. For training, the VAE image encoder, CLIP text encoder and U-Net are used. For inference,  \\nthe CLIP Text Encoder, U-Net, and VAE image decoder are used. Only the U-Net weights are updated during training; CLIP and VAE are fixed.\\n69THE BIG BOOK OF GENERATIVE AI\\nXFORMERS FLASHATTENTION\\nFigure 2: xFormers accelerates cross attention blocks in the U-Net.\\n70THE BIG BOOK OF GENERATIVE AI\\nThe attention layers in the Stable Diffusion architecture can be slow with a naive implementation, so most \\ncodebases use faster implementations that rely on fused kernels. In our stack, we leverage xFormers \\nFlashAttention.\\nWhile this was enabled in our original blog post, we found an issue with the usage that resulted in extra memory \\nbeing consumed on rank 0. After fixing this bug, we were able to increase our device microbatch size1 from 4 to \\n8. This yielded a sizable speedup, since A100s are more efficient at larger matrix sizes.\\nPRECOMPUTING LATENTS\\nFigure 3: Two phase training with precomputed latents. \\nFirst, all VAE and CLIP latents are precomputed and stored. \\nThen, the U-Net diffusion model is trained using these \\nprecomputed latents.\\n71THE BIG BOOK OF GENERATIVE AI\\nStable Diffusion is a combination of three models: a variational autoencoder (VAE), a text encoder (CLIP), and a \\nU-Net. During diffusion training, only the U-Net is trained, and the other two models are used to compute the \\nlatent encodings of the image and text inputs. Standard training involves computing the VAE and CLIP latents for \\nevery batch, but this does a lot of duplicate work when training for multiple epochs: latents are re-computed for \\neach image every time it is used. Instead, we precompute the latents once before training. Empirically, we have 2 \\nepochs at 256 resolution and 5 epochs at 512 resolution, so we avoid 6 extra VAE and CLIP calls per image-text \\npair in the dataset.\\nAdditionally, when pre-computing the latents, we can lower the precision of the VAE and CLIP models to \\nfp16. This could lead to numerical instability if we were training the VAE and CLIP and used this precision for \\nthe backward pass. However, since we\\'re only using them for inference, we can safely lower the precision, \\nwhich increases speed. The extra memory savings also let us use far larger batch sizes and improve hardware \\nutilization during the latent precomputation.\\n72THE BIG BOOK OF GENERATIVE AI\\nLOW PRECISION LAYERNORM AND GROUPNORM\\nFigure 4: Low Precision LayerNorm and Low Precision GroupNorm. Low precision gives faster training and lower memory usage, enabling larger \\nmicrobatches.\\n73THE BIG BOOK OF GENERATIVE AI\\nDiffusion training is done in automatic mixed precision by default. This uses half precision (fp16) in most \\nlayers, but fp32 in a few numerically unstable layers like normalization and softmax. The Stable Diffusion U-Net \\narchitecture uses several LayerNorm and GroupNorm layers, which by default are run in fp32.\\nMotivated by our finding that half precision LayerNorms are safe to use in language models, we decided to \\ntry out half precision LayerNorm and GroupNorm layers. This change resulted in identical loss curves and no \\ninstability in our experiments.\\nWhile we did observe some throughput improvement, the real benefit was decreased memory usage. Now, \\nalong with removing the VAE and CLIP memory by precomputing latents, we have enough space on our 40GB \\nA100 to increase our microbatch size from 8 to 16, 4x larger than what we started with!\\n74THE BIG BOOK OF GENERATIVE AI\\nFULLY SHARDED DATA PARALLELISM\\nFigure 5: Fully Sharded Data Parallel with SHARD_GRAD_OP speeds up the gradient update step and enables linear scaling.\\n75THE BIG BOOK OF GENERATIVE AI\\nMosaicML Composer, our go-to training library, includes support for PyTorch Fully Sharded Data Parallelism \\n(FSDP). We primarily use this to shard large scale models like 10B+ parameter LLMs that don\\'t fit in a single \\ndevice across hundreds of GPUs for incredibly fast training. Stable Diffusion doesn\\'t require sharding since it  \\nfits in a single GPU. However, some of the distributed features in FSDP are still useful for speeding up training  \\non a large number of GPUs.\\nWhen batches don’t fit into memory, we do several forward and backward passes on smaller microbatches, \\nfollowed by a single gradient update. If we use a small number of GPUs to train, we have far more forward and \\nbackward passes per gradient update, so the time spent on the gradient update doesn\\'t matter. However, at \\n128+ GPUs with a microbatch size of 16, we\\'re only doing one forward and one backward pass for each gradient \\nupdate. At this scale, the gradient update step starts to become a significant bottleneck.\\nTo tackle this problem, we use FSDP\\'s SHARD_GRAD_OP mode. In normal training, each GPU communicates all \\nits gradients to every other GPU, and then each GPU updates its local copy of the model. With this FSDP variant, \\neach GPU only gets the gradients and updates the weights for a small part of the model before sending the \\nupdated weights for that part of the model to all of the other GPUs. By dividing the update step across all the \\nGPUs, we can ensure the amount of work per GPU decreases as we increase the number of GPUs, helping us \\nachieve linear scaling.\\n76THE BIG BOOK OF GENERATIVE AI\\nSCHEDULED EMA\\nFigure 6: Loss curve of our training run with the scheduled exponential moving average (EMA) period highlighted.\\n77THE BIG BOOK OF GENERATIVE AI\\nStable Diffusion 2 uses Exponential Moving Averaging (EMA), which maintains an exponential moving average \\nof the weights. At every time step, the EMA model is updated by taking 0.9999 times the current EMA model \\nplus 0.0001 times the new weights after the latest forward and backward pass. By default, the EMA algorithm is \\napplied after every gradient update for the entire training period. However, this can be slow due to the memory \\noperations required to read and write all the weights at every step.\\nTo avoid this costly procedure, we start with a key observation: since the old weights are decayed by a factor of \\n0.9999 at every batch, the early iterations of training only contribute minimally to the final average. This means \\nwe only need to take the exponential moving average of the final few steps. Concretely, we train for 1,400,000 \\nbatches and only apply EMA for the final 50,000 steps, which is about 3.5% of the training period. The weights \\nfrom the first 1,350,000 iterations decay away by (0.9999)^50000, so their aggregate contribution would have \\na weight of less than 1% in the final model. Using this technique, we can avoid adding overhead for 96.5% of \\ntraining and still achieve a nearly equivalent EMA model.\\n78THE BIG BOOK OF GENERATIVE AI\\nFINAL TIME AND COST ESTIMATES\\nFigure 7: Throughput at 512x512 images on 128 GPUs as each speedup optimization is enabled. We achieve a total cumulative speedup of 2.71x over \\nthe baseline.\\nWe’ve shown how we obtained nearly a 3x reduction in time and cost to train Stable Diffusion compared to our \\noriginal results. With xFormers, precomputed latents, low precision LayerNorm, low precision GroupNorm, FSDP, \\nand scheduled EMA, Table 1 shows it\\'s possible to train Stable Diffusion in just 6.79 days using 21,000 A100-\\nhours for a total cost of less than $42,000. We estimated these times and costs by measuring throughput for \\ntraining 1.1 billion 256x256 images and 1.7 billion 512x512 images with a max tokenized length of 77 at a global \\nbatch size of 2048, as detailed in the Stable Diffusion 2 base model card. This is slightly cheaper than our \\npreviously reported run with a cost of $47.7k as it does not account for any time spent on evaluation or restarts \\ndue to hardware failures.\\n79THE BIG BOOK OF GENERATIVE AI\\nNUMBER  \\nOF A100S\\nTHROUGHPUT  \\nFOR U-NET \\n@ 256X256 \\n(IMAGES / \\nSECOND)\\nTHROUGHPUT \\nFOR U-NET \\n@ 512X512 \\n(IMAGES / \\nSECOND)\\nTHROUGHPUT \\nFOR U-NET @ \\n512X512 WITH \\nEMA (IMAGES /  \\nSECOND)\\nDAYS TO TRAIN \\nON MOSAICML \\nCLOUD\\nAPPROX. COST \\nON MOSAICML \\nCLOUD\\n8 1100 290 290 101.04 $38,800\\n16 2180 585 580 50.29 $38,630\\n32 4080 1195 1160 25.01 $38,420\\n64 8530 2340 2220 12.63 $38,800\\n128 11600 4590 3927 6.79 $41,710\\nTable 1: Estimated time and cost to train a Stable Diffusion model on 1.1 billion images at 256x256 resolution, followed by 1.7 billion images at 512x512 \\nresolution. Different rows show different numbers of NVIDIA 40GB A100 GPUs at a global batch size of 2048.\\nThese optimizations show that training image generation models from scratch is within reach for everyone. For \\nupdates on our latest work, join our Community Slack or follow us on Twitter. If your organization wants to start \\ntraining diffusion models today, please schedule a demo online or email us at demo@mosaicml.com.\\n1 When training large models with big batches that don\\'t fit in memory in a single pass, each batch is divided into smaller microbatches. On each \\ndevice, we can do a forward and backward pass for each microbatch and sum the gradients at the end to compute a gradient update equivalent to \\na single forward and backward pass with the entire batch all at once.\\n80THE BIG BOOK OF GENERATIVE AI\\nStage 5: LLM Evaluation\\nConstant evaluation and monitoring of deployed large language models (LLMs) and generative AI applications \\nare crucial due to the dynamic nature of both the data they interact with and the environments in which they \\noperate. These systems learn from vast datasets and can evolve over time, potentially leading to shifts in \\nperformance, accuracy or even the emergence of biases. Continuous monitoring ensures that any deviation \\nfrom expected behavior can be detected and corrected promptly, maintaining the integrity and reliability \\nof the AI application. As user needs and societal norms change, ongoing evaluation allows these models to \\nadapt, ensuring their outputs remain relevant, appropriate and effective. This vigilance not only mitigates risks \\nassociated with AI deployments, such as ethical concerns and regulatory compliance, but also maximizes the \\nvalue and utility these technologies bring to organizations and end users. \\nEvaluating LLMs is a challenging and evolving domain, primarily because LLMs often demonstrate uneven \\ncapabilities across different tasks. An LLM might excel in one benchmark, but slight variations in the prompt \\nor problem can drastically affect its performance. The dynamic nature of LLMs and their vast potential \\napplications only amplify the challenge of establishing comprehensive evaluation standards.\\n81THE BIG BOOK OF GENERATIVE AI\\nPresent challenges involved with evaluating LLM-powered applications include the following:\\n ■ Variable performance: LLMs can be sensitive to prompt variations, demonstrating high proficiency in \\none task but faltering with slight deviations in prompts.\\n ■ Lack of ground truth: Since most LLMs output natural language, it is very difficult to evaluate the outputs \\nvia traditional NLP metrics (BLEU, ROUGE, etc.). For example, suppose an LLM were used to summarize \\na news article. Two equally good summaries might have almost completely different words and word \\norders, so even defining a “ground truth” label becomes difficult or impossible.\\n ■ Domain-specific evaluation: For domain-specific fine-tuned LLMs, popular generic benchmarks \\nmay not capture their nuanced capabilities. Such models are tailored for specialized tasks, making \\ntraditional metrics less relevant. This divergence often necessitates the development of domain-specific \\nbenchmarks and evaluation criteria. See the example of Replit’s code generation LLM. \\n ■ Reliance on human judgment: It is often the case that LLM performance is being evaluated in domains \\nwhere text is scarce or there is a reliance on subject matter expert knowledge. In such scenarios, \\nevaluating LLM output can be costly and time-consuming.\\nTo help give examples of how this can be accomplished, here are two great examples of how you can monitor \\nand evaluate your deployed LLMs and generative AI applications using Databricks.\\nLLM Evaluation Examples\\nBest Practices for LLM Evaluation of RAG Applications  \\nA Case Study on the Databricks Documentation Bot\\nby Quinn Leng, Kasey Uhlenhuth and Alkis Polyzotis\\nChatbots are the most widely adopted use case for leveraging the powerful chat and reasoning capabilities \\nof large language models (LLM). The retrieval augmented generation (RAG) architecture is quickly becoming \\nthe industry standard for developing chatbots because it combines the benefits of a knowledge base (via a \\nvector store) and generative models (e.g., GPT-3.5 and GPT-4) to reduce hallucinations, maintain up-to-date \\ninformation, and leverage domain-specific knowledge. However, evaluating the quality of chatbot responses \\nremains an unsolved problem today. With no industry standards defined, organizations resort to human grading \\n(labeling) –which is time-consuming and hard to scale.\\n82THE BIG BOOK OF GENERATIVE AI\\nWe applied theory to practice to help form best practices for LLM automated evaluation so you can deploy RAG \\napplications to production quickly and with confidence. This blog represents the first in a series of investigations \\nwe’re running at Databricks to provide learnings on LLM evaluation. All research in this post was conducted by \\nQuinn Leng, Senior Software Engineer at Databricks and creator of the Databricks Documentation AI Assistant. \\nCHALLENGES WITH AUTO-EVALUATION IN PRACTICE\\nRecently, the LLM community has been exploring the use of “LLMs as a judge” for automated evaluation with \\nmany using powerful LLMs such as GPT-4 to do the evaluation for their LLM outputs. The lmsys group’s research \\npaper explores the feasibility and pros/cons of using various LLMs (GPT-4, ClaudeV1, GPT-3.5) as the judge for \\ntasks in writing, math, and world knowledge.\\nDespite all this great research, there are still many unanswered questions about how to apply LLM judges  \\nin practice:\\n ■ Alignment With Human Grading: Specifically for a document-Q&A chatbot, how well does an \\nLLM judge’s grading reflect the actual human preference in terms of correctness, readability and \\ncomprehensiveness of the answers? \\n ■ Accuracy Through Examples: What’s the effectiveness of providing a few grading examples to the LLM \\njudge and how much does it increase the reliability and reusability of the LLM judge on different metrics?\\n ■ Appropriate Grade Scales: What grading scale is recommended because different grading scales are \\nused by different frameworks (e.g., AzureML uses 0 to 100 whereas langchain uses binary scales)?\\n ■ Applicability Across Use Cases: With the same evaluation metric (e.g. correctness), to what extent can \\nthe evaluation metric be reused across different use cases (e.g. casual chat, content summarization, \\nretrieval augmented generation)?\\n83THE BIG BOOK OF GENERATIVE AI\\nAPPLYING EFFECTIVE AUTO-EVALUATION FOR RAG APPLICATIONS\\nWe explored the possible options for the questions outlined above in the context of our own chatbot \\napplication at Databricks. We believe that our findings generalize and can thus help your team effectively \\nevaluate RAG-based chatbots at a lower cost and faster speed:\\n ■ LLM-as-a-judge agrees with human grading on over 80% of judgments. Using LLMs-as-a-judge for our \\ndocument-based chatbot evaluation was as effective as human judges, matching the exact score in over \\n80% of judgments and being within a 1-score distance (using a scale of 0-3) in over 95% of judgments.\\n ■ Save costs by using GPT-3.5 with examples. GPT-3.5 can be used as an LLM judge if you provide \\nexamples for each grading score. Because of the context size limit it’s only practical to use a low-\\nprecision grading scale. Using GPT-3.5 with examples instead of GPT-4 drives down the cost of LLM judge \\nby 10x and improves the speed by more than 3x.\\n ■ Use low-precision grading scales for easier interpretation. We found lower-precision grading scores like 0, \\n1, 2, 3 or even binary (0, 1) can largely retain precision compared to higher precision scales like 0 to 10.0 or \\n0 to 100.0, while making it considerably easier to provide grading rubrics to both human annotators and \\nLLM judges. Using a lower precision scale also allows consistency of grading scales among different LLM \\njudges (e.g., between GPT-4 and claude2).\\n ■ RAG applications require their own benchmarks. A model might have good performance on a published \\nspecialized benchmark (e.g. casual chat, math, or creative writing) but that doesn’t guarantee good \\nperformance on other tasks (e.g. answering questions from a given context). Benchmarks should only be \\nused if the use case matches, i.e., a RAG application should only be evaluated with a RAG benchmark.\\nBased on our research, we recommend the following procedure when using an LLM judge: \\n ■ Use a 1-5 grading scale\\n ■ Use GPT-4 as an LLM judge with no examples to understand grading rules\\n ■ Switch your LLM judge to GPT-3.5 with one example per score\\n84THE BIG BOOK OF GENERATIVE AI\\nOUR METHODOLOGY FOR ESTABLISHING BEST PRACTICES\\nThe remainder of this post will walk through the series of experiments we conducted to form these  \\nbest practices. \\nEXPERIMENT SETUP\\n85THE BIG BOOK OF GENERATIVE AI\\n The experiment had three steps: \\n1. Generate evaluation dataset: We created a dataset from 100 questions and context from Databricks \\ndocuments. The context represents (chunks of) documents that are relevant to the question. \\n2. Generate answer sheets: Using the evaluation dataset, we prompted different language models to \\ngenerate answers and stored the question-context-answer pairs in a dataset called “answer sheets”. In \\nthis investigation, we used GPT-4, GPT-3.5, Claude-v1, Llama2-70b-chat, Vicuna-33b, and mpt-30b-chat.\\n3. Generate grades: Given the answer sheets, we used various LLMs to generate grades and reasoning \\nfor the grades. The grades are a composite score of Correctness (weighted: 60%), Comprehensiveness \\n(weighted: 20%) and Readability (weighted: 20%). We chose this weighting scheme to reflect our \\npreference for Correctness in the generated answers. Other applications may tune these weights \\ndifferently but we expect Correctness to remain a dominant factor.\\n86THE BIG BOOK OF GENERATIVE AI\\nAdditionally, the following techniques were used to avoid positional bias and improve reliability:\\n ■ Low temperature (temperature 0.1) to ensure reproducibility\\n ■ Single-answer grading instead of pairwise comparison\\n ■ Chain of thoughts to let the LLM reason about the grading process before giving the final score\\n ■ Few-shots generation where the LLM is provided with several examples in the grading rubric for each \\nscore value on each factor (Correctness, Comprehensiveness, Readability)\\nEXPERIMENT 1: ALIGNMENT WITH HUMAN GRADING\\nTo confirm the level of agreement between human annotators and LLM judges, we sent answer sheets  \\n(grading scale 0-3) from gpt-3.5-turbo and vicuna-33b to a labeling company to collect human labels,  \\nand then compared the result with GPT-4’s grading output. Below are the findings:\\nHuman and GPT-4 judges can reach above 80% agreement on the correctness and readability score.  \\nAnd if we lower the requirement to be smaller or equal than 1 score difference, the agreement level can  \\nreach above 95%. The Comprehensiveness metric has less alignment, which matches what we’ve heard  \\nfrom business stakeholders who shared that “comprehensive” seems more subjective than metrics like \\nCorrectness or Readability.\\n87THE BIG BOOK OF GENERATIVE AI\\nEXPERIMENT 2: ACCURACY THROUGH EXAMPLES\\nThe lmsys paper uses this prompt to instruct the LLM judge to evaluate based on the helpfulness, relevance, \\naccuracy, depth, creativity, and level of detail of the response. However, the paper doesn’t share specifics on the \\ngrading rubric. From our research, we found many factors can significantly affect the final score, for example:\\n ■ The importance of different factors: Helpfulness, Relevance, Accuracy, Depth, Creativity\\n ■ The interpretation of factors like Helpfulness is ambiguous \\n ■ If different factors conflict with each other, where an answer is helpful but is not accurate \\nWe developed a rubric for instructing an LLM judge for a given grading scale, by trying the following:\\n1. Original Prompt: Here is the original prompt used in the lmsys paper:\\nWe adapted the original lmsys paper prompt to emit our metrics about correctness, comprehensiveness and \\nreadability, and also prompt the judge to provide one line justification before giving each score (to benefit from \\nchain-of-thought reasoning). Below are the zero-shot version of the prompt which doesn’t provide any example, \\nand the few-shot version of the prompt which provides one example for each score. Then we used the same \\nanswer sheets as input and compared the graded results from the two prompt types.\\n2. Zero Shot Learning: Require the LLM judge to emit our metrics about correctness, comprehensiveness \\nand readability, and also prompt the judge to provide one line justification for each score.\\n \\nPlease act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question \\ndisplayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and \\nlevel of detail of the response. Begin your evaluation by providing a short explanation. Be as objective as possible. After \\nproviding your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format\\n \\nPlease act as an impartial judge and evaluate the quality of the provided answer which attempts to answer the provided \\nquestion based on a provided context. \\n  You\\'ll be given a function grading_function which you\\'ll call for each provided context, question and answer to submit your \\nreasoning and score for the correctness, comprehensiveness and readability of the answer\\n88THE BIG BOOK OF GENERATIVE AI\\n3. Few Shots Learning: We adapted the zero shot prompt to provide explicit examples for each score in the \\nscale. The new prompt:\\n \\nPlease act as an impartial judge and evaluate the quality of the provided answer which attempts to answer the provided \\nquestion based on a provided context.\\n  You\\'ll be given a function grading_function which you\\'ll call for each provided context, question and answer to submit your \\nreasoning and score for the correctness, comprehensiveness and readability of the answer. \\n  \\n  Below is your grading rubric: \\n- Correctness: If the answer correctly answer the question, below are the details for different scores:\\n  - Score 0: the answer is completely incorrect, doesn’t mention anything about the question or is completely contrary to the \\ncorrect answer.\\n      - For example, when asked “How to terminate a databricks cluster”, the answer is empty string, or content that’s \\ncompletely irrelevant, or sorry I don’t know the answer.\\n  - Score 1: the answer provides some relevance to the question and answers one aspect of the question correctly.\\n      - Example:\\n          - Question: How to terminate a databricks cluster\\n          - Answer: Databricks cluster is a cloud-based computing environment that allows users to process big data and run \\ndistributed data processing tasks efficiently.\\n          - Or answer:  In the Databricks workspace, navigate to the \"Clusters\" tab. And then this is a hard question that I \\nneed to think more about it\\n  - Score 2: the answer mostly answer the question but is missing or hallucinating on one critical aspect.\\n      - Example:\\n          - Question: How to terminate a databricks cluster”\\n          - Answer: “In the Databricks workspace, navigate to the \"Clusters\" tab.\\n          Find the cluster you want to terminate from the list of active clusters.\\n          And then you’ll find a button to terminate all clusters at once”\\n  - Score 3: the answer correctly answer the question and not missing any major aspect\\n      - Example:\\n          - Question: How to terminate a databricks cluster\\n          - Answer: In the Databricks workspace, navigate to the \"Clusters\" tab.\\n          Find the cluster you want to terminate from the list of active clusters.\\n          Click on the down-arrow next to the cluster name to open the cluster details.\\n          Click on the \"Terminate\" button. A confirmation dialog will appear. Click \"Terminate\" again to confirm the action.”\\n- Comprehensiveness: How comprehensive is the answer, does it fully answer all aspects of the question and provide \\ncomprehensive explanation and other necessary information. Below are the details for different scores:\\n  - Score 0: typically if the answer is completely incorrect, then the comprehensiveness is also zero score.\\n  - Score 1: if the answer is correct but too short to fully answer the question, then we can give score 1 for \\ncomprehensiveness.\\n      - Example:\\n          - Question: How to use databricks API to create a cluster?\\n          - Answer: First, you will need a Databricks access token with the appropriate permissions. You can generate this \\ntoken through the Databricks UI under the \\'User Settings\\' option. And then (the rest is missing)\\n  - Score 2: the answer is correct and roughly answer the main aspects of the question, but it’s missing description about \\ndetails. Or is completely missing details about one minor aspect.  \\n89THE BIG BOOK OF GENERATIVE AI\\n      - Example:\\n          - Question: How to use databricks API to create a cluster?\\n          - Answer: You will need a Databricks access token with the appropriate permissions. Then you’ll need to set up the \\nrequest URL, then you can make the HTTP Request. Then you can handle the request response.\\n      - Example:\\n          - Question: How to use databricks API to create a cluster?\\n          - Answer: You will need a Databricks access token with the appropriate permissions. Then you’ll need to set up the \\nrequest URL, then you can make the HTTP Request. Then you can handle the request response.\\n  - Score 3: the answer is correct, and covers all the main aspects of the question\\n- Readability: How readable is the answer, does it have redundant information or incomplete information that hurts the \\nreadability of the answer.\\n  - Score 0: the answer is completely unreadable, e.g. fully of symbols that’s hard to read; e.g. keeps repeating the words \\nthat it’s very hard to understand the meaning of the paragraph. No meaningful information can be extracted from the answer.\\n  - Score 1: the answer is slightly readable, there are irrelevant symbols or repeated words, but it can roughly form a \\nmeaningful sentence that cover some aspects of the answer.\\n      - Example:\\n          - Question: How to use databricks API to create a cluster?\\n          - Answer: You you  you  you  you  you  will need a Databricks access token with the appropriate permissions. And \\nthen then you’ll need to set up the request URL, then you can make the HTTP Request. Then Then Then Then Then Then Then Then \\nThen\\n  - Score 2: the answer is correct and mostly readable, but there is one obvious piece that’s affecting the readability \\n(mentioning of irrelevant pieces, repeated words)\\n      - Example:\\n          - Question: How to terminate a databricks cluster\\n          - Answer: In the Databricks workspace, navigate to the \"Clusters\" tab.\\n          Find the cluster you want to terminate from the list of active clusters.\\n          Click on the down-arrow next to the cluster name to open the cluster details.\\n          Click on the \"Terminate\" button…………………………………..\\n          A confirmation dialog will appear. Click \"Terminate\" again to confirm the action.\\n  - Score 3: the answer is correct and reader friendly, no obvious piece that affect readability.\\n- Then final rating:\\n    - Ratio: 60% correctness + 20% comprehensiveness + 20% readability\\nFrom this experiment, we learned several things:\\n ■ Using the Few Shots prompt with GPT-4 didn’t make an obvious difference in the consistency of results. \\nWhen we included the detailed grading rubric with examples we didn’t see a noticeable improvement in \\nGPT-4’s grading results across different LLM models. Interestingly, it caused a slight variance in the range \\nof the scores. \\n90THE BIG BOOK OF GENERATIVE AI\\n91THE BIG BOOK OF GENERATIVE AI\\n ■ Including few examples for GPT-3.5-turbo-16k significantly improves the consistency of the scores,  \\nand makes the result usable. Including detailed grading rubric/examples has very obvious improvement \\non the grading result from GPT-3.5. Though the actual average score value is slightly different between \\nGPT-4 and GPT-3.5 (score 3.0 vs score 2.6), the ranking and precision remains fairly consistent\\n ■ On the contrary, using GPT-3.5 without a grading rubric gets very inconsistent results and is  \\ncompletely unusable\\n ■ Note that we are using GPT-3.5-turbo-16k instead of GPT-3.5-turbo since the prompt can be larger than \\n4k tokens. \\n92THE BIG BOOK OF GENERATIVE AI\\nEXPERIMENT 3: APPROPRIATE GRADE SCALES\\nThe LLM-as-judge paper uses a non-integer 0~10 scale (i.e., float) for the grading scale; in other words, it uses \\na high precision rubric for the final score. We found these high-precision scales cause issues downstream with \\nthe following:\\n ■ Consistency: Evaluators–both human and LLM–struggled to hold the same standard for the same score \\nwhen grading on high precision. As a result, we found that output scores are less consistent across judges \\nif you move from low-precision to high-precision scales. \\n ■ Explainability: Additionally, if we want to cross-validate the LLM-judged results with human-judged \\nresults we must provide instructions on how to grade answers. It is very difficult to provide accurate \\ninstructions for each “score” in a high-precision grading scale–for example, what’s a good example for an \\nanswer that’s scored at 5.1 as compared to 5.6? \\n93THE BIG BOOK OF GENERATIVE AI\\nWe experimented with various low-precision grading scales to provide guidance on the “best” one to use, \\nultimately we recommend an integer scale of 0-3 or 0-4 (if you want to stick to the Likert scale). We tried  \\n0-10, 1-5, 0-3, and 0-1 and learned:\\n ■ Binary grading works for simple metrics like “usability” or “good/bad”.\\n ■ Scales like 0-10 are difficult to come up with distinguishing criteria between all scores.\\n94THE BIG BOOK OF GENERATIVE AI\\nAs shown in these plots, both GPT-4 and GPT-3.5 can retain consistent ranking of results using different  \\nlow-precision grading scales, thus using a lower grading scale like 0~3 or 1~5 can balance the precision  \\nwith explainability).\\nThus, we recommend 0-3 or 1-5 as a grading scale to make it easier to align with human labels, reason about \\nscoring criteria, and provide examples for each score in the range. \\n95THE BIG BOOK OF GENERATIVE AI\\nEXPERIMENT 4: APPLICABILITY ACROSS USE CASES\\nThe LLM-as-judge paper shows that both LLM and human judgment ranks the Vicuna-13B model as a close \\ncompetitor to GPT-3.5:\\nHowever, when we benchmarked the set of models for our document Q&A use cases, we found that even the \\nmuch larger Vicuna-33B model has a noticeably worse performance than GPT-3.5 when answering questions \\nbased on context. These findings are also verified by GPT-4, GPT-3.5 and human judges (as mentioned in \\nExperiment 1) which all agree that Vicuna-33B is performing worse than GPT-3.5.\\nFigure 4: Average win rate of nine models under different judges on Chatbot Arena.\\n96THE BIG BOOK OF GENERATIVE AI\\nWe looked closer at the benchmark dataset proposed by the paper and found that the 3 categories of tasks \\n(writing, math, knowledge) don’t directly reflect or contribute to the model’s ability to synthesize an answer \\nbased on a context. Instead, intuitively, document Q&A use cases need benchmarks on reading comprehension \\nand instruction following. Thus evaluation results can’t be transferred between use cases and we need to build \\nuse-case-specific benchmarks in order to properly evaluate how good a model can meet customer needs.\\n97THE BIG BOOK OF GENERATIVE AI\\nUSE MLFLOW TO LEVERAGE OUR BEST PRACTICES\\nWith the experiments above, we explored how different factors can significantly affect the evaluation of a \\nchatbot and confirmed that LLM as a judge can largely reflect human preferences for the document Q&A use \\ncase. At Databricks, we are evolving the MLflow Evaluation API to help your team effectively evaluate your LLM \\napplications based on these findings. MLflow 2.4 introduced the Evaluation API for LLMs to compare various \\nmodels’ text output side-by-side, MLflow 2.6 introduced LLM-based metrics for evaluation like toxicity and \\nperplexity, and we’re working to support LLM-as-a-judge in the near future!\\nIn the meantime, we compiled the list of resources we referenced in our research below:\\n ■ Doc_qa repository\\n ■ The code and data we used to conduct the experiments\\n ■ LLM-as-Judge Research paper from lmsys group \\n ■ The paper is the first research for using LLM as judge for the casual chat use cases, it extensively \\nexplored the feasibility and pros and cons of using LLM (GPT-4, ClaudeV1, GPT-3.5) as the judge for \\ntasks in writing, math, world knowledge\\nOffline LLM Evaluation: Step-by-Step GenAI Application Assessment on Databricks\\nby Abe Omorogbe, Liang Zhang, Sunish Sheth, Corey Zumar, Maheswaran Venkatachalam, Emil Lysgaard  \\nand Mathias Christiansen\\nBACKGROUND\\nIn an era where retrieval augmented generation (RAG) is revolutionizing the way we interact with AI-driven \\napplications, ensuring the efficiency and effectiveness of these systems has never been more essential. \\nDatabricks and MLflow are at the forefront of this innovation, offering streamlined solutions for the critical \\nevaluation of GenAI applications. \\nThis blog post guides you through the simple and effective process of leveraging the Databricks Data \\nIntelligence Platform to enhance and evaluate the quality of the three core components of your GenAI \\napplications: Prompts, Retrieval System, and Foundation LLM, ensuring that your GenAI applications continue  \\nto generate accurate results.\\n98THE BIG BOOK OF GENERATIVE AI\\nUSE CASE\\nWe are going to be creating a QA chatbot that will answer questions from the MLflow documentation and then \\nevaluate the results.\\n99THE BIG BOOK OF GENERATIVE AI\\nSET UP EXTERNAL MODELS IN DATABRICKS\\nDatabricks Model Serving feature can be used to manage, govern, and access external models from various \\nlarge language model (LLM) providers, such as Azure OpenAI GPT, Anthropic Claude, or AWS Bedrock, within \\nan organization. It offers a high-level interface that simplifies the interaction with these services by providing a \\nunified endpoint to handle specific LLM related requests.\\nMajor advantages of using Model Serving:\\n ■ Query Models Through a Unified Interface:  Simplifies the interface to call multiple LLMs in your \\norganization. Query models through a unified OpenAI-compatible API and SDK and manage all models \\nthrough a single UI.\\n ■ Govern and Manage Models: Centralizes endpoint management of multiple LLMs in your organization.  \\nThis includes the ability to manage permissions and track usage limits.\\n ■ Central Key Management: Centralizes API key management in a secure location, which enhances \\norganizational security by minimizing key exposure in the system and code, and reduces the burden  \\non end-users.\\n100THE BIG BOOK OF GENERATIVE AI\\nCREATE A SERVING ENDPOINT WITH AN EXTERNAL MODEL IN DATABRICKS\\n \\n1\\n2\\n3 \\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n23\\n25\\n26\\nimport mlflow\\nimport mlflow.deployments\\nclient = mlflow.deployments.get_deploy_client(\"databricks\")\\nendpoint_name = f\"test-endpoint-{uuid.uuid4()}\"\\nclient.create_endpoint(\\nname=endpoint_name,\\nconfig={\\n        \"served_entities\": [\\n            {\\n                \"name\": \"test\",\\n                \"external_model\": {\\n                    \"name\": \"gpt-3.5-turbo-instruct\",\\n                    \"provider\": \"openai\",\\n                    \"task\": \"llm/v1/completions\",\\n                    \"openai_config\": {\\n                        \"openai_api_type\": \"azure\",\\n                        \"openai_api_key\": \"{{secrets/<your-scope-name>/<your-key-name>}}\", ## Use Databricks Secrets. \\n                        \"openai_api_base\": \"https://<your-endpoint>.openai.azure.com/\",\\n                        \"openai_deployment_name\": \"<your-deployment-name>\",\\n                        \"openai_api_version\": \"2023-05-15\",\\n                    },\\n                },\\n            }\\n        ],\\n     },\\n)\\n101THE BIG BOOK OF GENERATIVE AI\\nEXPLORE PROMPTS WITH THE DATABRICKS AI PLAYGROUND\\nIn this section, we will understand: How well do different prompts perform with the chosen LLM?\\nWe recently introduced the Databricks AI Playground, which provides a best-in-class experience for crafting the \\nperfect prompt. With no code required, you can try out multiple LLMs served as Endpoints in Databricks, and \\ntest different parameters and prompts.\\nMajor advantages of the Databricks AI Playground are:\\n ■ Quick Testing: Quickly test deployed models directly in Databricks.\\n ■ Easy Comparison: Central location to compare multiple models on different prompts and parameters for \\ncomparison and selection.\\nUSING DATABRICKS AI PLAYGROUND\\nWe delve into testing relevant prompts with OpenAI GPT 3.5 Turbo, leveraging the Databricks AI Playground. \\n102THE BIG BOOK OF GENERATIVE AI\\nCOMPARING DIFFERENT PROMPTS AND PARAMETERS\\nIn the Playground, you are able to compare the output of multiple prompts to see which gives better results. \\nDirectly in the Playground, you can try several prompts,  models, and parameters to figure out which \\ncombination provides the best results. The model and parameters combo can then be added to the GenAI  \\napp and used for answer generation with the right context.\\n103THE BIG BOOK OF GENERATIVE AI\\nADDING MODEL AND PARAMETERS TO YOUR GENAI APPLICATION\\nAfter playing with a few prompts and parameters, you can use the same settings and model in your  \\nGenAI application.\\n104THE BIG BOOK OF GENERATIVE AI\\nExample of how to import the same external model in LangChain. We will cover how we turn this into a GenAI \\nPOC in the next section.\\nCREATE GENAI POC WITH LANGCHAIN AND LOG WITH MLFLOW\\nNow that we have found a good model and prompt parameters for your use case, we are going to create a \\nsample GenAI app that is a QA chatbot that will answer questions from the MLflow documentation using a \\nvector database, embedding model with the Databricks Foundation Model API and Azure OpenAI GPT 3.5 as \\nthe generation model.\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\nfrom langchain.llms import Databricks\\nllm = Databricks(\\n    endpoint_name=\"<endpoint-name>\",\\n    extra_params={\"temperature\": 0.1,\\n                 \"top_p\": 0.1,\\n                 \"max_tokens\": 500,\\n                 } #parameters used in AI Playground\\n)\\n105THE BIG BOOK OF GENERATIVE AI\\nCREATE A SAMPLE GENAI APP WITH LANGCHAIN USING DOCS FROM THE MLFLOW WEBSITE\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\n27\\n28\\n29\\n30\\n31\\n32\\n33\\n34\\n35\\n36\\n37\\nimport os\\nimport pandas as pd\\nimport mlflow\\nimport chromadb\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.document_loaders import WebBaseLoader\\nfrom langchain.llms import Databricks\\nfrom langchain.embeddings.databricks import DatabricksEmbeddings\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\\nloader = WebBaseLoader(\\n    [ \\n     \"https://mlflow.org/docs/latest/index.html\",\\n     \"https://mlflow.org/docs/latest/tracking/autolog.html\", \\n     \"https://mlflow.org/docs/latest/getting-started/tracking-server-overview/index.html\",\\n     \"https://mlflow.org/docs/latest/python_api/mlflow.deployments.html\" ])\\ndocuments = loader.load()\\nCHUNK_SIZE = 1000\\ntext_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=0)\\ntexts = text_splitter.split_documents(documents)\\nllm = Databricks(\\n    endpoint_name=\"<endpoint-name>\",\\n    extra_params={\"temperature\": 0.1,\\n                 \"top_p\": 0.1,\\n                 \"max_tokens\": 500,\\n                 } #parameters used in AI Playground\\n)\\n# create the embedding function using Databricks Foundation Model APIs\\nembedding_function = DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\")\\ndocsearch = Chroma.from_documents(texts, embedding_function)\\nqa = RetrievalQA.from_chain_type(\\n    llm=llm,\\n    chain_type=\"stuff\",\\n    retriever=docsearch.as_retriever(fetch_k=3),\\n    return_source_documents=True,\\n)\\n106THE BIG BOOK OF GENERATIVE AI\\nFor customers wanting to scale the retriever used in their GenAI application, we advise using Databricks Vector \\nSearch, a serverless similarity search engine that allows you to store a vector representation of your data, \\nincluding metadata, in a vector database.\\nEVALUATION OF RETRIEVAL SYSTEM WITH MLFLOW\\nIn this section, we will understand: How well does the retriever work with a given query?\\nIn MLflow 2.9.1, Evaluation for retrievers was introduced and provides a way for you to assess the efficiency \\nof their retriever with the MLflow evaluate API. You can use this API to evaluate the effectiveness of your \\nembedding model, the top K threshold choice, or the chunking strategy.\\nCREATING A GROUND TRUTH DATASET\\nCurating a ground truth dataset for evaluating your GenAI often involves the meticulous task of manually \\nannotating test sets, a process that demands both time and domain expertise. In this blog, we’re taking a \\ndifferent route. We’re leveraging the power of an LLM to generate synthetic data for testing, offering a quick-\\nstart approach to get a sense of your GenAI app’s retrieval capability, and a warm-up for all the in-depth \\nevaluation work that may follow. To our readers and customers, we emphasize the importance of crafting a \\ndataset that mirrors the expected inputs and outputs of your GenAI application. It’s a journey worth taking for \\nthe incredible insights you’ll gain!\\nYou can explore with the full dataset but let\\'s demo with a subset of the generated data. The question column \\ncontains all the questions that will be evaluated and the source column is the expected source for the answer \\nfor the questions as an ordered list of strings.\\n107THE BIG BOOK OF GENERATIVE AI\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\neval_data = pd.DataFrame(\\n    {\\n        \"question\": [\\n            \"What is MLflow?\",\\n            \"What is Databricks?\",\\n            \"How to serve a model on Databricks?\",\\n            \"How to enable MLflow Autologging for my workspace by default?\",\\n        ],\\n        \"source\": [\\n            [\"https://mlflow.org/docs/latest/index.html\"],\\n            [\"https://mlflow.org/docs/latest/getting-started/tracking-server-overview/index.html\"],\\n            [\"https://mlflow.org/docs/latest/python_api/mlflow.deployments.html\"],\\n            [\"https://mlflow.org/docs/latest/tracking/autolog.html\"],\\n        ],\\n    }\\n)\\nEVALUATE THE EMBEDDING MODEL WITH MLFLOW\\nThe quality of your embedding model is pivotal for accurate retrieval. In MLflow 2.9.0, we introduced three built-\\nin metrics mlflow.metrics.precision_at_k(k), mlflow.metrics.recall_at_k(k) and mlflow.metrics.ndcg_at_k(k) \\nto help determine how effective your retriever is at predicting the most relevant results for you. For example; \\nSuppose the vector database returns 10 results (k=10), and out of these 10 results, 4 are relevant to your query. \\nThe precision_at_10 would be 4/10 or 40%. \\n108THE BIG BOOK OF GENERATIVE AI\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\ndef evaluate_embedding(embedding_function):\\n    CHUNK_SIZE = 1000\\n    list_of_documents = loader.load()\\n    text_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=0)\\n    docs = text_splitter.split_documents(list_of_documents)\\n    retriever = Chroma.from_documents(docs, embedding_function).as_retriever()\\n    def retrieve_doc_ids(question: str) -> List[str]:\\n        docs = retriever.get_relevant_documents(question)\\n        doc_ids = [doc.metadata[\"source\"] for doc in docs]\\n        return doc_ids\\n    def retriever_model_function(question_df: pd.DataFrame) -> pd.Series:\\n        return question_df[\"question\"].apply(retrieve_doc_ids)\\n    with mlflow.start_run() as run:\\n        evaluate_results = mlflow.evaluate(\\n                model=retriever_model_function,\\n                data=eval_data,\\n                model_type=\"retriever\",\\n                targets=\"source\",\\n                evaluators=\"default\",\\n            )\\n    return evaluate_results\\nresult1 = evaluate_embedding(DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\"))result2 = evaluate_embed-\\nding(<another-embedding-function>)\\neval_results_of_retriever_df_bge = result1.tables[\"eval_results_table\"]\\ndisplay(eval_results_of_retriever_df_bge)\\n109THE BIG BOOK OF GENERATIVE AI\\nThe evaluation will return a table with the results of your evaluation for each question. i.e., for this test, we can \\nsee that the retriever seems to performing great for the questions \"How to enable MLflow Autologging for my \\nworkspace by default?” with a Precision @ K score is 1, and is not retrieving any of the right documentation for \\nthe questions \"What is MLflow?” since the precision @ K score is 0. With this insight, we can debug the retriever \\nand improve the retriever for questions like “What is MLflow?”\\nEvaluation results when using databricks-bge-large-en embedding model\\nEVALUATE RETRIEVER WITH DIFFERENT TOP K VALUES WITH MLFLOW\\nYou can quickly calculate the metrics for different Ks by specifying the extra_metrics argument.\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\nwith mlflow.start_run() as run:\\n        evaluate_results = mlflow.evaluate(\\n        data=eval_results_of_retriever_df_bge,\\n        targets=\"source\",\\n        predictions=\"outputs\",\\n        evaluators=\"default\",\\n        extra_metrics=[\\n            mlflow.metrics.precision_at_k(1),\\n            mlflow.metrics.precision_at_k(2),\\n            mlflow.metrics.precision_at_k(3),\\n            mlflow.metrics.recall_at_k(1),\\n            mlflow.metrics.recall_at_k(2),\\n            mlflow.metrics.recall_at_k(3),\\n            mlflow.metrics.ndcg_at_k(1),\\n            mlflow.metrics.ndcg_at_k(2),\\n            mlflow.metrics.ndcg_at_k(3),\\n        ],\\n    )\\ndisplay(evaluate_results.tables[\"eval_results_table\"])\\n110THE BIG BOOK OF GENERATIVE AI\\nThe evaluation will return a table with the results of your evaluation for each question, and you can better \\nunderstand which K value to use when retrieving documents. i.e., for this test we can see changing the top K \\nvalue can positively affect the precision of the retriever for questions like “What is Databricks?”\\nEvaluation result with all precision at K values\\n111THE BIG BOOK OF GENERATIVE AI\\nEVALUATE THE CHUNKING STRATEGY WITH MLFLOW\\nThe effectiveness of your chunking strategy is critical. We explore how MLflow can assist in this evaluation, \\nfocusing on the retrieval model type and its impact on overall performance.\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\ndef evaluate_chunk_size(chunk_size):\\n  list_of_documents = loader.load()\\n  text_splitter = CharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=0)\\n  docs = text_splitter.split_documents(list_of_documents)\\n  embedding_function = DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\")\\n  retriever = Chroma.from_documents(docs, embedding_function).as_retriever()\\n  \\n  def retrieve_doc_ids(question: str) -> List[str]:\\n    docs = retriever.get_relevant_documents(question)\\n    doc_ids = [doc.metadata[\"source\"] for doc in docs]\\n    return doc_ids\\n   \\n  def retriever_model_function(question_df: pd.DataFrame) -> pd.Series:\\n    return question_df[\"question\"].apply(retrieve_doc_ids)\\n  with mlflow.start_run() as run:\\n      evaluate_results = mlflow.evaluate(\\n          model=retriever_model_function,\\n          data=eval_data,\\n          model_type=\"retriever\",\\n          targets=\"source\",\\n          evaluators=\"default\",\\n      )\\n  return evaluate_results\\nresult1 = evaluate_chunk_size(500)\\nresult2 = evaluate_chunk_size(2000)\\ndisplay(result1.tables[\"eval_results_table\"])\\ndisplay(result2.tables[\"eval_results_table\"])\\n112THE BIG BOOK OF GENERATIVE AI\\nThe evaluation will return 2 tables with the results of your evaluation for each question using 2 different chunk \\nsizes, and you can better understand which chunk size to use when retrieving documents (i.e., for this example, \\nit seems like changing the chunk size did not affect any metric).\\nEvaluation result with Chunk size of 1000\\nEvaluation result with Chunk size of 2000\\nCheck out the in-depth notebook on retrieval evaluation\\n113THE BIG BOOK OF GENERATIVE AI\\nEVALUATION OF GENAI RESUL TS WITH MLFLOW\\nIn this section, we will understand: How good is the response of the GenAI app with a given prompt and context?\\nAssessing the quality of generated responses is key. We will augment the manual process of evaluating with \\nquestions and answers by leveraging MLflow\\'s QA metrics, and comparing them against a GPT-4 model as a \\nbenchmark to understand the effectiveness of the generated answers. \\nUsing an LLM like GPT-4 as a judge to assist in evaluation can offer several benefits, here are some key benefits:\\n ■ Rapid and Scalable Experimentation:  In many situations, we think LLM judges represent a sweet-spot: \\nthey can evaluate unstructured outputs (like a response from a chat-bot) automatically, rapidly, and  \\nat low-cost.  \\n ■ Cost-Effective: By automating some evaluations with LLMs, we consider it a worthy companion to human \\nevaluation, which is slower and more expensive but represents the gold standard of model evaluation.\\nUSE MLFLOW EVALUATE AND LLM AS A JUDGE\\nWe take some sample questions and use the LLM as a judge, and inspect the results with MLflow, providing a \\ncomprehensive analysis of the outcome with built-in metrics. We are going to judge the GenAI app on relevance \\n(how relevant is the output with respect to both the input and the context).\\nCreate a simple function that runs each input through the chain\\n \\n1\\n2\\ndef model(input_df):\\n    return input_df[\"questions\"].map(qa).tolist()\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\neval_df = pd.DataFrame(\\n    {\\n        \"questions\": [\\n            \"What is MLflow?\",\\n            \"What is Databricks?\",\\n            \"How to serve a model on Databricks?\",\\n            \"How to enable MLflow Autologging for my workspace by default?\",\\n        ],\\n    }\\n)\\n114THE BIG BOOK OF GENERATIVE AI\\nUse relevance metric to determine the relevance of the answer and context. There are other metrics you can \\nuse too.\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\nfrom mlflow.deployments import set_deployments_target\\nfrom  mlflow.metrics.genai.metric_definitions import relevance\\nset_deployments_target(\"databricks\") #To retrieve all endpoint in your Databricks Workspace\\nrelevance_metric = relevance(model=f\"endpoints:/{endpoint_name}\") #You can also use any model you have hosted on Da-\\ntabricks, models from the Marketplace or models in the Foundation model API\\nwith mlflow.start_run():\\n    results =  mlflow.evaluate(\\n        model,\\n        eval_df,\\n        model_type=\"question-answering\",\\n        evaluators=\"default\",\\n        predictions=\"result\",\\n        extra_metrics=[relevance_metric, mlflow.metrics.latency()],\\n        evaluator_config={\\n            \"col_mapping\": {\\n                \"inputs\": \"questions\",\\n                \"context\": \"source_documents\",\\n            }\\n        }\\n    )\\n    print(results.metrics)\\n115THE BIG BOOK OF GENERATIVE AI\\nIn your Databricks Workspace, you can compare and evaluate all your inputs and outputs, as well as the source \\ndocuments, relevance and any other metrics you added to your evaluation function.\\nCheck out more in depth notebooks on LLM evaluation\\n116THE BIG BOOK OF GENERATIVE AI\\nSummary Whether you’re looking to disrupt traditional industries, enhance creative endeavors or solve complex problems \\nin novel ways, the potential applications of generative AI are limited only by your imagination and willingness to \\nexperiment. Remember, every significant advancement in this field began with a simple idea and the courage to \\nexplore it further.\\nFor those seeking more knowledge or simply curious about the latest developments in the realm of generative \\nAI, we’ve provided some resources on training, demos and product information. \\nGenAI Training\\nGenerative AI Engineer Learning Pathway: Take self-paced, on-demand and instructor-led courses on \\ngenerative AI\\nFree LLM Course (edX): In-depth course to learn GenAI and LLMs inside and out\\nGenAI Webinar: Learn how to take control of your GenAI app performance, privacy and cost, and drive value \\nwith generative AI\\nAdditional Resources\\nBig Book of MLOps: A deep dive into the architectures and technologies behind MLOps — including LLMs  \\nand GenAI \\nMosaic AI: Product page covering the features of Mosaic AI within Databricks\\n117Build Production-Quality GenAI Applications — See How\\nCreate high-quality generative AI applications and ensure your output is accurate, \\ngoverned and safe. See why over 10,000 organizations worldwide rely on Databricks for \\nall their workloads from BI to AI — test-drive the full Databricks Platform free for 14 days.\\nAbout Databricks\\nDatabricks is the data and AI company. More than 10,000 organizations worldwide — \\nincluding Comcast, Condé Nast, Grammarly and over 50% of the Fortune 500 — rely on \\nthe Databricks Data Intelligence Platform to unify and democratize data, analytics and \\nAI. Databricks is headquartered in San Francisco, with offices around the globe, and was \\nfounded by the original creators of Lakehouse, Apache Spark™, Delta Lake and MLflow.  \\nTo learn more, follow Databricks on LinkedIn, X and Facebook.\\nTry Databricks free Take Generative AI Fundamentals On-Demand Training\\n© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark and the Spark \\nlogo are trademarks of the Apache Software Foundation . Privacy Policy  | Terms of Use')]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(doc_ques_gen[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "XcnAAxs4-9Xx",
        "outputId": "89aa7290-d4af-4e53-9cf6-c8426cc23f1f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.documents.base.Document"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.documents.base.Document</b><br/>def __init__(page_content: str, **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/langchain_core/documents/base.py</a>Class for storing a piece of text and associated metadata.\n",
              "\n",
              "Example:\n",
              "\n",
              "    .. code-block:: python\n",
              "\n",
              "        from langchain_core.documents import Document\n",
              "\n",
              "        document = Document(\n",
              "            page_content=&quot;Hello, world!&quot;,\n",
              "            metadata={&quot;source&quot;: &quot;https://example.com&quot;}\n",
              "        )</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 256);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "splitter_ans = CharacterTextSplitter(\n",
        "    chunk_size=1000,   # number of characters, not tokens\n",
        "    chunk_overlap=200\n",
        ")\n",
        "\n",
        "chunks_ans = splitter_ans.split_documents(doc_ques_gen)\n"
      ],
      "metadata": {
        "id": "l8lhNZu9_Aov"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-google-genai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MV0yHIymebW4",
        "outputId": "8c6bf343-346b-408d-af15-8087de266f58"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall google-ai-generativelanguage==0.6.15\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TCQIRe45gLOy",
        "outputId": "bc4f9b7a-f763-4156-caf3-c081ed2e7604"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-ai-generativelanguage==0.6.15\n",
            "  Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15)\n",
            "  Using cached google_api_core-2.25.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 (from google-ai-generativelanguage==0.6.15)\n",
            "  Using cached google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15)\n",
            "  Using cached proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 (from google-ai-generativelanguage==0.6.15)\n",
            "  Using cached protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15)\n",
            "  Using cached googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting requests<3.0.0,>=2.18.0 (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15)\n",
            "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting grpcio<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15)\n",
            "  Using cached grpcio-1.74.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15)\n",
            "  Using cached grpcio_status-1.74.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0 (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage==0.6.15)\n",
            "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage==0.6.15)\n",
            "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage==0.6.15)\n",
            "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15)\n",
            "  Using cached grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.72.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.71.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage==0.6.15)\n",
            "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting charset_normalizer<4,>=2 (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15)\n",
            "  Using cached charset_normalizer-3.4.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15)\n",
            "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15)\n",
            "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15)\n",
            "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
            "Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
            "Using cached google_api_core-2.25.1-py3-none-any.whl (160 kB)\n",
            "Using cached google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
            "Using cached proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
            "Using cached protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
            "Using cached googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
            "Using cached grpcio-1.74.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "Using cached grpcio_status-1.71.2-py3-none-any.whl (14 kB)\n",
            "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
            "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
            "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
            "Using cached charset_normalizer-3.4.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (151 kB)\n",
            "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
            "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
            "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
            "Installing collected packages: urllib3, pyasn1, protobuf, idna, grpcio, charset_normalizer, certifi, cachetools, rsa, requests, pyasn1-modules, proto-plus, googleapis-common-protos, grpcio-status, google-auth, google-api-core, google-ai-generativelanguage\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "  Attempting uninstall: pyasn1\n",
            "    Found existing installation: pyasn1 0.6.1\n",
            "    Uninstalling pyasn1-0.6.1:\n",
            "      Successfully uninstalled pyasn1-0.6.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.74.0\n",
            "    Uninstalling grpcio-1.74.0:\n",
            "      Successfully uninstalled grpcio-1.74.0\n",
            "  Attempting uninstall: charset_normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.3\n",
            "    Uninstalling charset-normalizer-3.4.3:\n",
            "      Successfully uninstalled charset-normalizer-3.4.3\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.8.3\n",
            "    Uninstalling certifi-2025.8.3:\n",
            "      Successfully uninstalled certifi-2025.8.3\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 5.5.2\n",
            "    Uninstalling cachetools-5.5.2:\n",
            "      Successfully uninstalled cachetools-5.5.2\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.9.1\n",
            "    Uninstalling rsa-4.9.1:\n",
            "      Successfully uninstalled rsa-4.9.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.5\n",
            "    Uninstalling requests-2.32.5:\n",
            "      Successfully uninstalled requests-2.32.5\n",
            "  Attempting uninstall: pyasn1-modules\n",
            "    Found existing installation: pyasn1_modules 0.4.2\n",
            "    Uninstalling pyasn1_modules-0.4.2:\n",
            "      Successfully uninstalled pyasn1_modules-0.4.2\n",
            "  Attempting uninstall: proto-plus\n",
            "    Found existing installation: proto-plus 1.26.1\n",
            "    Uninstalling proto-plus-1.26.1:\n",
            "      Successfully uninstalled proto-plus-1.26.1\n",
            "  Attempting uninstall: googleapis-common-protos\n",
            "    Found existing installation: googleapis-common-protos 1.70.0\n",
            "    Uninstalling googleapis-common-protos-1.70.0:\n",
            "      Successfully uninstalled googleapis-common-protos-1.70.0\n",
            "  Attempting uninstall: grpcio-status\n",
            "    Found existing installation: grpcio-status 1.71.2\n",
            "    Uninstalling grpcio-status-1.71.2:\n",
            "      Successfully uninstalled grpcio-status-1.71.2\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.40.3\n",
            "    Uninstalling google-auth-2.40.3:\n",
            "      Successfully uninstalled google-auth-2.40.3\n",
            "  Attempting uninstall: google-api-core\n",
            "    Found existing installation: google-api-core 2.25.1\n",
            "    Uninstalling google-api-core-2.25.1:\n",
            "      Successfully uninstalled google-api-core-2.25.1\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.18\n",
            "    Uninstalling google-ai-generativelanguage-0.6.18:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.18\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-google-genai 2.1.10 requires google-ai-generativelanguage<0.7.0,>=0.6.18, but you have google-ai-generativelanguage 0.6.15 which is incompatible.\n",
            "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cachetools-5.5.2 certifi-2025.8.3 charset_normalizer-3.4.3 google-ai-generativelanguage-0.6.15 google-api-core-2.25.1 google-auth-2.40.3 googleapis-common-protos-1.70.0 grpcio-1.74.0 grpcio-status-1.71.2 idna-3.10 proto-plus-1.26.1 protobuf-5.29.5 pyasn1-0.6.1 pyasn1-modules-0.4.2 requests-2.32.5 rsa-4.9.1 urllib3-2.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "google",
                  "idna",
                  "requests",
                  "urllib3"
                ]
              },
              "id": "81f1fd791730435aae5a219c115ec18a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n"
      ],
      "metadata": {
        "id": "UcfhB3CVAtRZ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_que_gen_pipeline = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=gemini_api,\n",
        "    temperature=0.2\n",
        ")\n",
        "\n",
        "# response = llm.invoke(\"Hello! Summarize LangChain for me.\")\n",
        "# print(response.content)"
      ],
      "metadata": {
        "id": "Hd2aetRwhAvH"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"\n",
        "You are an expert at creating questions based on coding materials and documentation.\n",
        "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
        "You do this by asking questions about the text below:\n",
        "\n",
        "------------\n",
        "{text}\n",
        "------------\n",
        "\n",
        "Create questions that will prepare the coders or programmers for their tests.\n",
        "Make sure not to lose any important information.\n",
        "\n",
        "QUESTIONS:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "o7g4x3FLhvsP"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "b5IgdpzQhzzT"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_ques=PromptTemplate(template=prompt_template,input_variables=[\"text \"])"
      ],
      "metadata": {
        "id": "o2bxoNvIhzob"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "refine_template = (\"\"\"\n",
        "You are an expert at creating practice questions based on coding material and documentation.\n",
        "Your goal is to help a coder or programmer prepare for a coding test.\n",
        "We have received some practice questions to a certain extent: {existing_answer}.\n",
        "We have the option to refine the existing questions or add new ones.\n",
        "(only if necessary) with some more context below.\n",
        "------------\n",
        "{text}\n",
        "------------\n",
        "\n",
        "Given the new context, refine the original questions in English.\n",
        "If the context is not helpful, please provide the original questions.\n",
        "QUESTIONS:\n",
        "\"\"\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "7KvOne9OiOvC"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "refine_template=PromptTemplate(template=refine_template,input_variables=[\"existing_answer\",\"text\"])"
      ],
      "metadata": {
        "id": "MrWVESP4iOlU"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.summarize import load_summarize_chain"
      ],
      "metadata": {
        "id": "lXvI3xaViu3P"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Question_gen_chain=load_summarize_chain(llm=llm_que_gen_pipeline,\n",
        "                                        chain_type=\"refine\",\n",
        "                                        verbose=True,\n",
        "                                        question_prompt=prompt_ques,\n",
        "                                        refine_prompt=refine_template)"
      ],
      "metadata": {
        "id": "_lQlS_DMi4Jo"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ques = Question_gen_chain.run(doc_ques_gen)\n",
        "\n",
        "print(ques)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11S1BxRxkaKI",
        "outputId": "d9a09432-e8f3-484b-d64a-f0e375ab6f49"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "You are an expert at creating questions based on coding materials and documentation.\n",
            "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
            "You do this by asking questions about the text below:\n",
            "\n",
            "------------\n",
            "THE BIG BOOK OF GENERATIVE AI\n",
            "CONTENTS\n",
            "Introduction  ......................................................................................................................................................................................................... 3\n",
            "The Path to Deploying Production-Quality GenAI Applications ........................................................................................... 5 \n",
            "Stage 0: Foundation Models  ........................................................................................................................................................................................................................................................... 5 \n",
            " Use Case: Introducing DBRX: A New State-of-the-Art Open LLM  ................................................................................................................................................................... 5 \n",
            "Stage 1: Prompt Engineering  ......................................................................................................................................................................................................................................................... 19 \n",
            " Use Case: Automated Analysis of Product Reviews Using Large Language Models ........................................................................................................................ 20 \n",
            "Stage 2: Retrieval Augmented Generation  ....................................................................................................................................................................................................................... 25 \n",
            " Use Case: Improve Your RAG Application Response Quality With Real-Time Structured Data  ................................................................................................ 27 \n",
            "Stage 3: Fine-Tuning a Foundation Model  ......................................................................................................................................................................................................................... 33 \n",
            " Use Case: Creating a Bespoke LLM for AI-Generated Documentation  ..................................................................................................................................................... 34 \n",
            " Use Case: Efficient Fine-Tuning With LoRA: A Guide to Optimal Parameter Selection for Large Language Models  ................................................... 43 \n",
            "Stage 4: Pretraining  ........................................................................................................................................................................................................................................................................... 60 \n",
            " Use Case: Training Stable Diffusion From Scratch for <$50K With MosaicML  ..................................................................................................................................... 62 \n",
            " Use Case: Deep Dive: How We Trained Stable Diffusion for Less Than $50K  ........................................................................................................................................ 68 \n",
            "Stage 5: LLM Evaluation  .................................................................................................................................................................................................................................................................... 81 \n",
            " Use Case : Best Practices for LLM Evaluation of RAG Application  ................................................................................................................................................................. 82 \n",
            " Use Case: Offline LLM Evaluation: Step-by-Step GenAI Application Assessment on Databricks ........................................................................................... 98\n",
            "Summary  ............................................................................................................................................................................................................. 117 \n",
            "GenAI Training  ......................................................................................................................................................................................................................................................................................... 117 \n",
            "Additional Resources  ........................................................................................................................................................................................................................................................................ 117\n",
            "2THE BIG BOOK OF GENERATIVE AI\n",
            "Achieving Production-Quality GenAI Requires New Tools and Skills\n",
            "Generative AI has opened new worlds of possibilities for businesses and is being emphatically embraced \n",
            "across organizations. According to a recent MIT Tech Review report, all 600 CIOs surveyed stated they are \n",
            "increasing their investment in AI, and 71% are planning to build their own custom large language models (LLMs) \n",
            "or other GenAI models. However, many organizations have found it challenging to deploy these applications at \n",
            "production quality. To meet the standard of quality required for customer-facing applications, AI output must \n",
            "be accurate, governed and safe. \n",
            "Data Infrastructure Must Evolve to Support  \n",
            "GenAI-Powered Applications\n",
            "Making the leap to generative AI is not just about deploying a chatbot; it requires a reshaping of the foundational \n",
            "aspects of data management. Central to this transformation is the emergence of data lakehouses as the new \n",
            "“modern data stack.” These advanced data architectures are essential to harnessing the full potential of GenAI, \n",
            "driving faster, more cost-effective and wider democratization of data and AI technologies. As businesses \n",
            "increasingly rely on GenAI-powered tools and applications for competitive advantage, the underlying data \n",
            "infrastructure must evolve to support these advanced technologies effectively and securely.\n",
            "No Matter Where You Are on Your Path to Deploying GenAI Applications, \n",
            "the Quality of Your Data Matters\n",
            "Businesses need to achieve production quality with their GenAI applications. Developers need rich tools for \n",
            "understanding the quality of their data and model outputs, along with an underlying platform that lets them \n",
            "combine and optimize all aspects of the GenAI process. GenAI has many components such as data preparation, \n",
            "retrieval models, language models (either SaaS or open source), ranking and post-processing pipelines, prompt \n",
            "engineering, and training models on custom enterprise data.\n",
            "To help you overcome common enterprise challenges with building GenAI, we’ve compiled a collection of \n",
            "technical content and code samples. We’ll start each section with a brief overview and then provide use cases \n",
            "and example code for reference. \n",
            "Introduction\n",
            "3THE BIG BOOK OF GENERATIVE AI\n",
            "In this eBook, you’ll learn: \n",
            " ■ How to plan a path from basic to advanced GenAI applications, leveraging your organization’s data\n",
            " ■ How to use retrieval augmented generation (RAG) to make an off-the-shelf AI system smarter\n",
            " ■ How to evaluate LLMs and where you want to invest in more powerful AI tools and systems that drive \n",
            "more significant operational gain\n",
            " ■ How to build a custom LLM that may be better, faster and cheaper for your organization\n",
            " ■ When it might be worth it to pretrain your own model — and more\n",
            "Use cases for GenAI covered:\n",
            " ■ How to use LLMs to gain actionable insights from product reviews\n",
            " ■ How to use RAG for a chatbot to improve the quality of output\n",
            " ■ How to train your own generative AI model in a cost-effective manner\n",
            " ■ How to monitor and evaluate your deployed LLMs and GenAI applications\n",
            "4THE BIG BOOK OF GENERATIVE AI\n",
            "The Path to Deploying \n",
            "Production-Quality \n",
            "GenAI Applications\n",
            "Stage 0: Foundation Models\n",
            "Before setting off to create production-quality GenAI applications, we need to cover the base language models \n",
            "that serve as the foundation for layers of increasingly complex techniques. Foundation models commonly refer \n",
            "to large language models that have been trained over extensive datasets to be generally good at some task \n",
            "(chat, instruction following, code generation, etc.).\n",
            "We won’t cover many models, as it is a constantly shifting landscape, but it is important to note that while \n",
            "underlying architectures may differ drastically, foundation models generally fall under two categories: \n",
            "proprietary (such as GPT-3.5 and Gemini) and open source (such as Llama2-70B and DBRX). The main difference \n",
            "between the two is that while proprietary models historically have an edge on outright performance, users have \n",
            "to send their data out to a third party and don’t have control over the underlying model as they’re often being \n",
            "updated and changed. \n",
            "Open source models, on the other hand, offer users full control over the model and the ability to run it on their \n",
            "own terms with their own governance and data privacy. Here’s a current list of many open source GenAI models \n",
            "across different domains that are all free for commercial use. Databricks has also created their own state-of-\n",
            "the-art open source foundation model so users can build the highest-quality production GenAI applications.\n",
            "Foundation Model Use Case\n",
            "INTRODUCING DBRX: A NEW STATE-OF-THE-ART OPEN LLM\n",
            "We are excited to introduce DBRX, an open, general-purpose LLM created by Databricks. Across a range of \n",
            "standard benchmarks, DBRX sets a new state-of-the-art for established open LLMs. Moreover, it provides \n",
            "the open community and enterprises building their own LLMs with capabilities that were previously limited \n",
            "to closed model APIs; according to our measurements, it surpasses GPT-3.5, and it is competitive with \n",
            "Gemini 1.0 Pro. It is an especially capable code model, surpassing specialized models like CodeLLaMA-70B on \n",
            "programming, in addition to its strength as a general-purpose LLM.\n",
            "5THE BIG BOOK OF GENERATIVE AI\n",
            "This state-of-the-art quality comes with marked improvements in training and inference performance. DBRX \n",
            "advances the state-of-the-art in efficiency among open models thanks to its fine-grained mixture-of-experts \n",
            "(MoE) architecture. Inference is up to 2x faster than LLaMA2-70B, and DBRX is about 40% of the size of Grok-1 in \n",
            "terms of both total and active parameter-counts. When hosted on Mosaic AI Model Serving, DBRX can generate \n",
            "text at up to 150 tok/s/user. Our customers will find that training MoEs is also about 2x more FLOP-efficient \n",
            "than training dense models for the same final model quality. End-to-end, our overall recipe for DBRX (including \n",
            "the pretraining data, model architecture, and optimization strategy) can match the quality of our previous-\n",
            "generation MPT models with nearly 4x less compute.\n",
            "Figure 1: DBRX outperforms established open source models on language understanding (MMLU), \n",
            "Programming (HumanEval), and Math (GSM8K).\n",
            "6THE BIG BOOK OF GENERATIVE AI\n",
            "The weights of the base model (DBRX Base) and the fine-tuned model (DBRX Instruct) are available on Hugging \n",
            "Face under an open license. Starting today, DBRX is available for Databricks customers to use via APIs, and \n",
            "Databricks customers can pretrain their own DBRX-class models from scratch or continue training on top of  \n",
            "one of our checkpoints using the same tools and science we used to build it. DBRX is already being integrated \n",
            "into our GenAI-powered products, where — in applications like SQL — early rollouts have surpassed GPT-3.5 \n",
            "Turbo and are challenging GPT-4 Turbo. It is also a leading model among open models and GPT-3.5 Turbo on \n",
            "RAG tasks.\n",
            "Training mixture-of-experts models is hard. We had to overcome a variety of scientific and performance \n",
            "challenges to build a pipeline robust enough to repeatedly train DBRX-class models in an efficient manner. Now \n",
            "that we have done so, we have a one-of-a-kind training stack that allows any enterprise to train world-class MoE \n",
            "foundation models from scratch. We look forward to sharing that capability with our customers and sharing our \n",
            "lessons learned with the community.\n",
            "Download DBRX today from Hugging Face (DBRX Base, DBRX Instruct), or try out DBRX Instruct in our HF Space, \n",
            "or see our model repository on github: databricks/dbrx.\n",
            "What Is DBRX?\n",
            "DBRX is a transformer-based decoder-only large language model (LLM) that was trained using next-token \n",
            "prediction. It uses a fine-grained mixture-of-experts (MoE) architecture with 132B total parameters of which \n",
            "36B parameters are active on any input. It was pre-trained on 12T tokens of text and code data. Compared \n",
            "to other open MoE models like Mixtral and Grok-1, DBRX is fine-grained, meaning it uses a larger number of \n",
            "smaller experts. DBRX has 16 experts and chooses 4, while Mixtral and Grok-1 have 8 experts and choose 2. This \n",
            "provides 65x more possible combinations of experts and we found that this improves model quality. DBRX uses \n",
            "rotary position encodings (RoPE), gated linear units (GLU), and grouped query attention (GQA). It uses the GPT-\n",
            "4 tokenizer as provided in the tiktoken repository. We made these choices based on exhaustive evaluation and \n",
            "scaling experiments.\n",
            "7THE BIG BOOK OF GENERATIVE AI\n",
            "DBRX was pretrained on 12T tokens of carefully curated data and a maximum context length of 32k tokens. We \n",
            "estimate that this data is at least 2x better token-for-token than the data we used to pretrain the MPT family of \n",
            "models. This new dataset was developed using the full suite of Databricks tools, including Apache Spark™ and \n",
            "Databricks notebooks for data processing, Unity Catalog for data management and governance, and MLflow for \n",
            "experiment tracking. We used curriculum learning for pretraining, changing the data mix during training in ways \n",
            "we found to substantially improve model quality.\n",
            "Quality on Benchmarks vs. Leading Open Models\n",
            "Table 1 shows the quality of DBRX Instruct and leading established, open models. DBRX Instruct is the leading \n",
            "model on composite benchmarks, programming and mathematics benchmarks, and MMLU. It surpasses all chat \n",
            "or instruction fine-tuned models on standard benchmarks.\n",
            "Composite benchmarks. We evaluated DBRX Instruct and peers on two composite benchmarks: the Hugging \n",
            "Face Open LLM Leaderboard (the average of ARC-Challenge, HellaSwag, MMLU, TruthfulQA, WinoGrande,  \n",
            "and GSM8k) and the Databricks Model Gauntlet (a suite of over 30 tasks spanning six categories: world \n",
            "knowledge, commonsense reasoning, language understanding, reading comprehension, symbolic problem \n",
            "solving, and programming).\n",
            "Among the models we evaluated, DBRX Instruct scores the highest on two composite benchmarks: the Hugging \n",
            "Face Open LLM Leaderboard (74.5% vs. 72.7% for the next highest model, Mixtral Instruct) and the Databricks \n",
            "Gauntlet (66.8% vs. 60.7% for the next highest model, Mixtral Instruct).\n",
            "Programming and mathematics. DBRX Instruct is especially strong at programming and mathematics. It scores \n",
            "higher than the other open models we evaluated on HumanEval (70.1% vs. 63.2% for Grok-1, 54.8% for Mixtral \n",
            "Instruct, and 32.2% for the best-performing LLaMA2-70B variant) and GSM8k (66.9% vs. 62.9% for Grok-1, 61.1% \n",
            "for Mixtral Instruct, and 54.1% for the best-performing LLaMA2-70B variant). DBRX outperforms Grok-1, the next \n",
            "best model on these benchmarks, despite the fact that Grok-1 has 2.4x as many parameters. On HumanEval, \n",
            "DBRX Instruct even surpasses CodeLLaMA-70B Instruct, a model built explicitly for programming, despite the \n",
            "fact that DBRX Instruct is designed for general-purpose use (70.1% vs. 67.8% on HumanEval as reported by Meta \n",
            "in the CodeLLaMA blog).\n",
            "MMLU. DBRX Instruct scores higher than all other models we consider on MMLU, reaching 73.7%.\n",
            "8THE BIG BOOK OF GENERATIVE AI\n",
            "MODEL DBRX  \n",
            "INSTRUCT\n",
            "MIXTRAL \n",
            "INSTRUCT\n",
            "MIXTRAL \n",
            "BASE\n",
            "LLAMA2-70  \n",
            "B CHAT\n",
            "LLAMA2-70  \n",
            "B BASE GROK-11\n",
            "Open LLM Leaderboard2 \n",
            "(Avg of next 6 rows) 74.5% 72.7% 68.4% 62.4% 67.9% —\n",
            "ARC-challenge 25-shot 68.9% 70.1% 66.4% 64.6% 67.3% —\n",
            "HellaSwag 10-shot 89.0% 87.6% 86.5% 85.9% 87.3% —\n",
            "MMLU 5-shot 73.7% 71.4% 71.9% 63.9% 69.8% 73.0%\n",
            "Truthful QA 0-shot 66.9% 65.0% 46.8% 52.8% 44.9% —\n",
            "WinoGrande 5-shot 81.8% 81.1% 81.7% 80.5% 83.7% —\n",
            "GSM8k CoT 5-shot \n",
            "maj@13 66.9% 61.1% 57.6% 26.7% 54.1% 62.9% \n",
            "(8-shot)\n",
            "Gauntlet v0.34 \n",
            "(Avg of 30+ diverse tasks) 66.8% 60.7% 56.8% 52.8% 56.4% —\n",
            "HumanEval5 \n",
            "0-Shot, pass@1 \n",
            "(Programming)\n",
            "70.1% 54.8% 40.2% 32.2% 31.0% 63.2%\n",
            "LLaMA2-70B Base\n",
            "Table 1: Quality of DBRX Instruct and leading open models. See footnotes for details on how numbers were collected. \n",
            "Bolded and underlined is the highest score.\n",
            "9THE BIG BOOK OF GENERATIVE AI\n",
            "Quality on Benchmarks vs. Leading Closed Models\n",
            "Table 2 shows the quality of DBRX Instruct and leading closed models. According to the scores reported by \n",
            "each model creator, DBRX Instruct surpasses GPT-3.5 (as described in the GPT-4 paper), and it is competitive \n",
            "with Gemini 1.0 Pro and Mistral Medium.\n",
            "Across nearly all benchmarks we considered, DBRX Instruct surpasses or - at worst - matches GPT-3.5. DBRX \n",
            "Instruct outperforms GPT-3.5 on general knowledge as measured by MMLU (73.7% vs. 70.0%) and commonsense \n",
            "reasoning as measured by HellaSwag (89.0% vs. 85.5%) and WinoGrande (81.8% vs. 81.6%). DBRX Instruct \n",
            "especially shines on programming and mathematical reasoning as measured by HumanEval (70.1% vs. 48.1%) and \n",
            "GSM8k (72.8% vs. 57.1%).\n",
            "DBRX Instruct is competitive with Gemini 1.0 Pro and Mistral Medium. Scores for DBRX Instruct are higher than \n",
            "Gemini 1.0 Pro on Inflection Corrected MTBench, MMLU, HellaSwag, and HumanEval, while Gemini 1.0 Pro is \n",
            "stronger on GSM8k. Scores for DBRX Instruct and Mistral Medium are similar for HellaSwag, while Mistral Medium \n",
            "is stronger on Winogrande and MMLU and DBRX Instruct is stronger on HumanEval, GSM8k, and Inflection \n",
            "Corrected MTBench.\n",
            "10MODEL DBRX  \n",
            "INSTRUCT GPT-3.57 GPT-48 CLAUDE  \n",
            "3 HAIKU\n",
            "CLAUDE 3 \n",
            "SONNET\n",
            "CLAUDE 3 \n",
            "OPUS\n",
            "GEMINI  \n",
            "1.0 PRO\n",
            "GEMINI  \n",
            "1.5 PRO\n",
            "MISTRAL  \n",
            "MEDIUM\n",
            "MISTRAL \n",
            "LARGE\n",
            "MT Bench  \n",
            "(Inflection corrected , n=5) 8.39 ± 0.08 — — 8.41 ± \n",
            "0.04 \n",
            "8.54 ± \n",
            "0.09\n",
            "9.03 ± \n",
            "0.06\n",
            "8.23 ± 0.08 — 8.05 ± 0.12 8.90 ± \n",
            "0.06\n",
            "MMLU 5-shot 73.7% 70.0% 86.4% 75.2% 79.0% 86.8% 71.8% 81.9% 75.3% 81.2%\n",
            "HellaSwag 10-shot 89.0% 85.5% 95.3% 85.9% 89.0% 95.4% 84.7% 92.5% 88.0% 89.2%\n",
            "HumanEval 0-Shot \n",
            "pass@1 \n",
            "(Programming)\n",
            "70.1%  \n",
            "temp=0, \n",
            "N=1\n",
            "48.1% 67.0% 75.9% 73.0% 84.9% 67.7% 71.9% 38.4% 45.1%\n",
            "GSM8k CoT maj@1 72.8% \n",
            "(5-shot)\n",
            "57.1%  \n",
            "(5-shot)\n",
            "92.0%  \n",
            "(5-shot) 88.9% 92.3% 95.0%\n",
            "86.5% \n",
            "(maj1@32)\n",
            "91.7%  \n",
            "(11-shot)\n",
            "66.7%  \n",
            "(5-shot)\n",
            "81.0%  \n",
            "(5-shot)\n",
            "WinoGrande 5-shot 81.8% 81.6% 87.5% — — — — — 88.0% 86.7%\n",
            "Table 2: Quality of DBRX Instruct and leading closed models. Other than Inflection Corrected MTBench (which we measured ourselves on model \n",
            "endpoints), numbers were as reported by the creators of these models in their respective whitepapers. See footnotes for additional details.\n",
            "11\n",
            "THE BIG BOOK OF GENERATIVE AITHE BIG BOOK OF GENERATIVE AI\n",
            "Quality on Long-Context Tasks and RAG\n",
            "DBRX Instruct was trained with up to a 32K token context window. Table 3 compares its performance to that of \n",
            "Mixtral Instruct and the latest versions of the GPT-3.5 Turbo and GPT-4 Turbo APIs on a suite of long-context \n",
            "benchmarks (KV-Pairs from the Lost in the Middle paper and HotpotQAXL, a modified version of HotPotQA that \n",
            "extends the task to longer sequence lengths). GPT-4 Turbo is generally the best model at these tasks. However, \n",
            "with one exception, DBRX Instruct performs better than GPT-3.5 Turbo at all context lengths and all parts of the \n",
            "sequence. Overall performance for DBRX Instruct and Mixtral Instruct are similar.\n",
            "MODEL DBRX  \n",
            "INSTRUCT\n",
            "MIXTRAL  \n",
            "INSTRUCT\n",
            "GPT-3.5 TURBO \n",
            "(API)\n",
            "GPT-4 TURBO \n",
            "(API)\n",
            "Answer in Beginning Third of Context 45.1% 41.3% 37.3%* 49.3%\n",
            "Answer in Middle Third of Context 45.3% 42.7% 37.3%* 49.0%\n",
            "Answer in Last Third of Context 48.0% 44.4% 37.0%* 50.9%\n",
            "2K Context 59.1% 64.6% 36.3% 69.3%\n",
            "4K Context 65.1% 59.9% 35.9% 63.5%\n",
            "8K Context 59.5% 55.3% 45.0% 61.5%\n",
            "16K Context 27.0% 20.1% 31.7% 26.0%\n",
            "32K Context 19.9% 14.0% — 28.5%\n",
            "Table 3: The average performance of models on the KV-Pairs and HotpotQAXL benchmarks. Bold is the highest score. Underlined is the highest score \n",
            "other than GPT-4 Turbo. GPT-3.5 Turbo supports a maximum context length of 16K, so we could not evaluate it at 32K. *Averages for the beginning, \n",
            "middle, and end of the sequence for GPT-3.5 Turbo include only contexts up to 16K.\n",
            "12THE BIG BOOK OF GENERATIVE AI\n",
            "One of the most popular ways to leverage a model’s context is retrieval augmented generation (RAG). \n",
            "In RAG, content relevant to a prompt is retrieved from a database and presented alongside the prompt to \n",
            "give the model more information than it would otherwise have. Table 4 shows the quality of DBRX on two RAG \n",
            "benchmarks — Natural Questions and HotPotQA — when the model is also provided with the top 10 passages \n",
            "retrieved from a corpus of Wikipedia articles using the embedding model bge-large-en-v1.5. DBRX Instruct  \n",
            "is competitive with open models like Mixtral Instruct and LLaMA2-70B Chat and the current version  \n",
            "of GPT-3.5 Turbo.\n",
            "MODEL DBRX  \n",
            "INSTRUCT\n",
            "MIXTRAL  \n",
            "INSTRUCT\n",
            "LLAMA2-70B \n",
            "CHAT\n",
            "GPT 3.5 TUR -\n",
            "BO (API)\n",
            "GPT 4 TURBO \n",
            "(API)\n",
            "Natural Questions 60.0% 59.1% 56.5% 57.7% 63.9%\n",
            "HotPotQA 55.0% 54.2% 54.7% 53.0% 62.9%\n",
            "Table 4: The performance of the models measured when each model is given the top 10 passages retrieved from a Wikipedia corpus \n",
            "using bge-large-en-v1.5. Accuracy is measured by matching within the model’s answer. Bold is the highest score. Underlined is the \n",
            "highest score other than GPT-4 Turbo. \n",
            "Training Efficiency\n",
            "Model quality must be placed in the context of how efficient the model is to train and use. This is especially \n",
            "so at Databricks, where we build models like DBRX to establish a process for our customers to train their own \n",
            "foundation models.\n",
            "We found training mixture-of-experts models to provide substantial improvements in compute-efficiency for \n",
            "training (Table 5). For example, training a smaller member of the DBRX family called DBRX MoE-B (23.5B total \n",
            "parameters, 6.6B active parameters) required 1.7x fewer FLOPs to reach a score of 45.5% on the Databricks LLM \n",
            "Gauntlet than LLaMA2-13B required to reach 43.8%. DBRX MoE-B also contains half as many active parameters \n",
            "as LLaMA2-13B.\n",
            "13THE BIG BOOK OF GENERATIVE AI\n",
            "Looking holistically, our end-to-end LLM pretraining pipeline has become nearly 4x more compute-efficient \n",
            "in the past ten months. On May 5, 2023, we released MPT-7B, a 7B parameter model trained on 1T tokens that \n",
            "reached a Databricks LLM Gauntlet score of 30.9%. A member of the DBRX family called DBRX MoE-A (7.7B total \n",
            "parameters, 2.2B active parameters) reached a Databricks Gauntlet score of 30.5% with 3.7x fewer FLOPs. This \n",
            "efficiency is the result of a number of improvements, including using an MoE architecture, other architecture \n",
            "changes to the network, better optimization strategies, better tokenization, and - very importantly - better \n",
            "pretraining data.\n",
            "In isolation, better pretraining data made a substantial impact on model quality. We trained a 7B model on 1T \n",
            "tokens (called DBRX Dense-A) using the DBRX pretraining data. It reached 39.0% on the Databricks Gauntlet \n",
            "compared to 30.9% for MPT-7B. We estimate that our new pretraining data is at least 2x better token-for-token \n",
            "than the data used to train MPT-7B. In other words, we estimate that half as many tokens are necessary to reach \n",
            "the same model quality. We determined this by training DBRX Dense-A on 500B tokens; it outperformed MPT-7B \n",
            "on the Databricks Gauntlet, reaching 32.1%. In addition to better data quality, another important contributor to \n",
            "this token-efficiency may be the GPT-4 tokenizer, which has a large vocabulary and is believed to be especially \n",
            "token-efficient. These lessons about improving data quality translate directly into practices and tools that our \n",
            "customers use to train foundation models on their own data.\n",
            "MODEL TOTAL PARAMS ACTIVE PARAMS GAUNTLET SCORE RELATIVE FLOPS\n",
            "DBRX MoE-A 7.7B 2.2B 30.5% 1x\n",
            "MPT-7B (1T tokens) — 6.7B 30.9% 3.7x\n",
            "DBRX Dense-A (1T tokens) — 6.7B 39.0% 3.7x\n",
            "DBRX Dense-A (500B tokens) — 6.7B 32.1% 1.85x\n",
            "DBRX MoE-B 23.5B 6.6B 45.5% 1x\n",
            "LLaMA2-13B — 13.0B 43.8% 1.7x\n",
            "Table 5:  Details of several test articles that we used to validate the training efficiency of the DBRX MoE architecture and end-to-end training pipeline.\n",
            "14THE BIG BOOK OF GENERATIVE AI\n",
            "Inference Efficiency\n",
            "Figure 2 shows the end-to-end inference efficiency of serving DBRX and similar models using NVIDIA \n",
            "TensorRT-LLM with our optimized serving infrastructure and 16-bit precision. We aim for this benchmark \n",
            "to reflect real-world usage as closely as possible, including multiple users simultaneously hitting the same \n",
            "inference server. We spawn one new user per second, each user request contains an approximately 2000 \n",
            "token prompt, and each response comprises 256 tokens.\n",
            "In general, MoE models are faster at inference than their total parameter-counts would suggest. This is due \n",
            "to the fact that they use relatively few parameters for each input. We find that DBRX is no exception in this \n",
            "respect. DBRX inference throughput is 2-3x higher than a 132B non-MoE model.\n",
            "Inference efficiency and model quality are typically in tension: bigger models typically reach higher quality, but \n",
            "smaller models are more efficient for inference. Using an MoE architecture makes it possible to attain better \n",
            "tradeoffs between model quality and inference efficiency than dense models typically achieve. For example, \n",
            "DBRX is both higher quality than LLaMA2-70B and - thanks to having about half as many active parameters - \n",
            "DBRX inference throughput is up to 2x faster (Figure 2). Mixtral is another point on the improved pareto frontier \n",
            "attained by MoE models: it is smaller than DBRX, and it is correspondingly lower in terms of quality but reaches \n",
            "higher inference throughput. Users of the Databricks Foundation Model APIs can expect to see up to 150 \n",
            "tokens per second for DBRX on our optimized model serving platform with 8-bit quantization.\n",
            "15Figure 2:  Inference throughput for various model configurations on our optimized serving infrastructure using NVIDIA TensorRT-LLM at 16-bit \n",
            "precision with the best optimization flags we could find. Models are run in tensor-parallel across the entire node. The input prompt contains \n",
            "approximately 2000 prompt tokens and we generate 256 output tokens. One new user spawns every second.\n",
            "16\n",
            "THE BIG BOOK OF GENERATIVE AITHE BIG BOOK OF GENERATIVE AI\n",
            "How We Built DBRX\n",
            "DBRX was trained on 3072 NVIDIA H100s connected by 3.2Tbps Infiniband. The main process of building DBRX \n",
            "- including pretraining, post-training, evaluation, red-teaming, and refining - took place over the course of \n",
            "three months. It was the continuation of months of science, dataset research, and scaling experiments, not to \n",
            "mention years of LLM development at Databricks that includes the MPT and Dolly projects and the thousands \n",
            "of models we have built and brought to production with our customers.\n",
            "To build DBRX, we leveraged the same suite of Databricks tools that are available to our customers. We \n",
            "managed and governed our training data using Unity Catalog. We explored this data using newly acquired  \n",
            "Lilac AI. We processed and cleaned this data using Apache Spark™ and Databricks notebooks. We trained \n",
            "DBRX using optimized versions of our open-source training libraries: MegaBlocks, LLM Foundry, Composer, \n",
            "and Streaming. We managed large scale model training and finetuning across thousands of GPUs using our \n",
            "Mosaic AI Training service. We logged our results using MLflow. We collected human feedback for quality and \n",
            "safety improvements through Mosaic AI Model Serving and Inference Tables. We manually experimented with \n",
            "the model using the Databricks Playground. We found the Databricks tools to be best-in-class for each of their \n",
            "purposes, and we benefited from the fact that they were all part of a unified product experience.\n",
            "Get Started With DBRX on Databricks\n",
            "If you’re looking to start working with DBRX right away, it’s easy to do so with the Databricks Mosaic AI \n",
            "Foundation Model APIs. You can quickly get started with our pay-as-you-go pricing and query the model from \n",
            "our AI Playground chat interface. For production applications, we offer a provisioned throughput option to \n",
            "provide performance guarantees, support for finetuned models, and additional security and compliance. To \n",
            "privately host DBRX, you can download the model from the Databricks Marketplace and deploy the model on \n",
            "Model Serving.\n",
            "17THE BIG BOOK OF GENERATIVE AI\n",
            "Conclusions\n",
            "At Databricks, we believe that every enterprise should have the ability to control its data and its destiny in the \n",
            "emerging world of GenAI. DBRX is a central pillar of our next generation of GenAI products, and we look forward \n",
            "to the exciting journey that awaits our customers as they leverage the capabilities of DBRX and the tools we \n",
            "used to build it. In the past year, we have trained thousands of LLMs with our customers. DBRX is only one \n",
            "example of the powerful and efficient models being built at Databricks for a wide range of applications, from \n",
            "internal features to ambitious use-cases for our customers.\n",
            "As with any new model, the journey with DBRX is just the beginning, and the best work will be done by those \n",
            "who build on it: enterprises and the open community. This is also just the beginning of our work on DBRX, and \n",
            "you should expect much more to come.\n",
            "Contributions\n",
            "The development of DBRX was led by the Mosaic team that previously built the MPT model family, in \n",
            "collaboration with dozens of engineers, lawyers, procurement and finance specialists, program managers, \n",
            "marketers, designers, and other contributors from across Databricks. We are grateful to our colleagues, friends, \n",
            "family, and the community for their patience and support over the past months.\n",
            "In creating DBRX, we stand on the shoulders of giants in the open and academic community. By making DBRX \n",
            "available openly, we intend to invest back in the community in hopes that we will build even greater technology \n",
            "together in the future. With that in mind, we gratefully acknowledge the work and collaboration of Trevor Gale \n",
            "and his MegaBlocks project (Trevor’s PhD adviser is Databricks CTO Matei Zaharia), the PyTorch team and \n",
            "the FSDP project, NVIDIA and the TensorRT-LLM project, the vLLM team and project, EleutherAI and their \n",
            "LLM evaluation project, Daniel Smilkov and Nikhil Thorat at Lilac AI, and our friends at the Allen Institute for \n",
            "Artificial Intelligence (AI2).\n",
            "18THE BIG BOOK OF GENERATIVE AI\n",
            "Stage 1: Prompt Engineering\n",
            "Many companies still remain in the foundational stages of adopting generative AI technology. They have  \n",
            "no overarching AI strategy in place, no clear use cases to pursue and no access to a team of data scientists and \n",
            "other professionals who can help guide the company’s AI adoption journey.\n",
            "If this is like your business, a good starting point is an off-the-shelf LLM. While these LLMs lack the domain-\n",
            "specific expertise of custom AI models, experimentation can help you plot your next steps. Your employees can \n",
            "craft specialized prompts and workflows to guide their usage. Your leaders can get a better understanding of \n",
            "the strengths and weaknesses of these tools as well as a clearer vision of what early success in AI might look \n",
            "like. Your organization can use things like the Databricks AI Playground to figure out where to invest in more \n",
            "powerful AI tools and systems that drive more significant operational gain and even use LLMs as a judge to help \n",
            "evaluate responses.\n",
            "PRACTICAL APPLICATIONS OF GENAI TECHNOLOGY\n",
            "Let’s delve into a compelling use case that illustrates the power of prompt engineering with off-the-shelf \n",
            "LLMs. Consider the challenge many businesses face: sifting through vast amounts of product reviews  \n",
            "to glean actionable insights. Without a dedicated team of data scientists or a clear AI strategy, this task  \n",
            "might seem daunting. However, leveraging the flexibility of LLMs through prompt engineering offers a \n",
            "straightforward solution.\n",
            "19THE BIG BOOK OF GENERATIVE AI\n",
            "Prompt Engineering Use Case\n",
            "Automated Analysis of Product Reviews Using Large Language Models\n",
            "Keep track of customer feedback at scale\n",
            "Check out our LLM Solution Accelerators for Retail for more details and to download the notebooks.\n",
            "While conversational AI has garnered a lot of media attention in recent months, the capabilities of large \n",
            "language models (LLMs) extend well beyond conversational interactions. It's in these less prominent  \n",
            "capabilities such as query response, summarization, classification and search that many organizations  \n",
            "are finding immediate opportunities to supercharge their workforce and up-level customer experiences.\n",
            "The potential of these applications is staggering. By one estimate, LLMs (and other generative AI \n",
            " technologies) could, in the near future, address tasks that today occupy 60%–70% of employees’ time.  \n",
            "Through augmentation, numerous studies have shown that the time to complete various tasks performed  \n",
            "by knowledge workers such as background research, data analysis and document writing can be cut in half.  \n",
            "And still other studies have shown that the use of these technologies can dramatically reduce the time for  \n",
            "new workers to achieve full productivity.\n",
            "But before these benefits can be fully realized, organizations must first rethink the management of the \n",
            "unstructured information assets on which these models depend and find ways to mitigate the issues of bias \n",
            "and accuracy that affect their output. This is why so many organizations are currently focusing their efforts \n",
            "on focused, internal applications where a limited scope provides opportunities for better information access \n",
            "and human oversight can serve as a check to errant results. These applications, aligned with core capabilities \n",
            "already residing within the organization, have the potential to deliver real and immediate value, while LLMs and \n",
            "their supporting technologies continue to evolve and mature.\n",
            "20THE BIG BOOK OF GENERATIVE AI\n",
            "PRODUCT REVIEW SUMMARIZATION COULD USE A BOOST\n",
            "To illustrate the potential of a more focused approach to LLM adoption, we consider a fairly simple and common \n",
            "task performed within many online retail organizations: product review summarization. Today, most organizations \n",
            "employ a modestly-sized team of workers to read and digest user feedback for insights that may help improve a \n",
            "product's performance or otherwise identify issues related to customer satisfaction.\n",
            "The work is important but anything but sexy. A worker reads a review, takes notes, and moves on to the next. \n",
            "Individual reviews that require a response are flagged and a summary of the feedback from across multiple \n",
            "reviews are compiled for review by product or category managers.\n",
            "This is a type of work that's ripe for automation. The volume of reviews that pour into a site mean the more \n",
            "detailed portions of this work are often performed on a limited subset of products across variable windows \n",
            "depending on a products importance. In more sophisticated organizations, rules detecting course or \n",
            "inappropriate language and models estimating user sentiment or otherwise classifying reviews for positive, \n",
            "negative or neutral experiences may be applied to help identify problematic content and draw a reviewer's \n",
            "attention to it. But either way, a lot is missed simply because we can't throw enough bodies at the problem to \n",
            "keep up and those bodies tend to become bored or fatigued with the monotony of the work.\n",
            "LARGE LANGUAGE MODELS CAN AUTOMATE PRODUCT REVIEW ANALYSIS\n",
            "By using an LLM, issues of scale and consistency can be easily addressed. All we need to do is bring the product \n",
            "reviews to the model and ask:\n",
            " ■ What are the top three points of negative feedback found across these reviews?\n",
            " ■ What features do our customers like best about this product?\n",
            " ■ Do customers feel they are receiving sufficient value from the product relative to what they are being \n",
            "asked to pay?\n",
            " ■ Are there any reviews that are especially negative or are using inappropriate language?\n",
            "21THE BIG BOOK OF GENERATIVE AI\n",
            "Within seconds we can have a tidy response, allowing our product managers to focus on responding to issues \n",
            "instead of simply detecting them.\n",
            "But what about the problem of accuracy and bias? Standards for identifying inaccuracies and bias in LLM \n",
            "output are evolving as are techniques for better ensuring that outputs align with an organization's expectations, \n",
            "and the fine-tuning of models using approved content can go a long way to ensure models have a preference to \n",
            "generate content that's at least aligned with how an organization prefers to communicate.\n",
            "This is a long-winded way of saying there is no ideal solution to the problem as of yet. But when compared  \n",
            "to where we are with human-driven processes and more simplistic models or rules-based approaches,  \n",
            "the results are expected to be better or at a minimum no worse than what we currently experience.  \n",
            "And given that these review summaries are for internal consumption, the impact of an errant model can \n",
            "be easily managed.\n",
            "YOU CAN BUILD A SOLUTION FOR THIS TODAY\n",
            "To demonstrate exactly how this work could be performed, we have built a Solution Accelerator for summarizing \n",
            "product reviews. This is based heavily on a previously published blog from Sean Owen that addressed some of \n",
            "the core technical challenges of tuning an LLM on the Databricks platform. For the accelerator, we are using the \n",
            "Amazon Product Reviews Dataset, which contains 51 million user-generated reviews across 2 million distinct \n",
            "books as this provides access to a wide range of reviewer content and presents a scaling challenge many \n",
            "organizations will recognize.\n",
            "We imagine a scenario in which a team of product managers receives customer feedback through online \n",
            "reviews. These reviews are important for identifying issues that may need to be addressed regarding a \n",
            "particular item and for steering future books to be offered by the site. Without the use of technology, this team \n",
            "struggles to read all the feedback and summarize into a workable set notes. As a result, they limit their attention \n",
            "to just the most critical items and are able to only process the feedback on a sporadic basis.\n",
            "22THE BIG BOOK OF GENERATIVE AI\n",
            "But using Databricks, they are able to set up a pipeline to collect feedback from a wider range of products \n",
            "and summarize these on a regular basis. Recognizing that positively rated products are likely to highlight the \n",
            "strengths of these books while lower rated products are likely to focus on their weaknesses, they separate  \n",
            "these reviews based on user-provided ratings and task an LLM to extract different sets of information from  \n",
            "each high-level category of reviews.\n",
            "Summary metrics are provided to allow product managers an overview of the feedback received and are \n",
            "backed by more detailed summaries generated by the LLM (Figure 1).\n",
            "Figure 1: Summary metrics and bullet-point details extracted from user reviews extracted using an LLM\n",
            "23THE BIG BOOK OF GENERATIVE AI\n",
            "DATABRICKS BRINGS TOGETHER ALL THE COMPONENTS OF A SOLUTION\n",
            "The scenario demonstrated above depends on the use of an LLM. In months prior, the use of such an LLM \n",
            "required access to specialized computational infrastructures, but with advances in the open source community \n",
            "and investments in the Databricks platform, we are now able to run the LLM in our local Databricks environment.\n",
            "In this particular scenario, the sensitivity of the data was not a motivating factor for this choice. Instead, we \n",
            "found that the volume of reviews to be processed tipped the cost scales toward the use of Databricks, allowing \n",
            "us to trim about one-third of the cost of implementing a similar solution using a third-party service.\n",
            "In addition, we found that by implementing our own infrastructure, we were able to scale the environment up \n",
            "for faster processing, tackling as many as 760,000 reviews per hour in one test without having to be concerned \n",
            "with constraints imposed by an external service. While most organizations will not have the need to scale quite \n",
            "to that level, it's nice to know it is there should it be.\n",
            "But this solution is more than just an LLM. To bring together the whole solution we needed to develop a data \n",
            "processing workflow to receive incoming reviews, prepare them for submission to the model and to capture \n",
            "model output for further analysis. As a unified data platform, Databricks provides us the means to address \n",
            " both data engineering and data science requirements without data replication. And when we are done \n",
            "processing the reviews, our analysts can use their tools of choice to query the output and make business \n",
            "decisions. Through Databricks, we have access to the full array of capabilities for us to build a solution aligned \n",
            "with our business’ needs.\n",
            "24THE BIG BOOK OF GENERATIVE AI\n",
            "Stage 2: Retrieval Augmented Generation\n",
            "Retrieval augmented generation (RAG) lets you bring in supplemental knowledge resources to make an  \n",
            "off-the-shelf AI system smarter. RAG won’t change the underlying behavior of the model, but it will improve  \n",
            "the quality and accuracy of the responses.\n",
            "However, at this point, your business should not be uploading its “mission-critical” data. Instead, the RAG \n",
            "process typically involves smaller amounts of nonsensitive information.\n",
            "For example, plugging in an employee handbook can allow your workers to start asking the underlying model \n",
            "questions about the organization’s vacation policy. Uploading instruction manuals can help power a service \n",
            "chatbot. With the ability to query support tickets using AI, support agents can get answers quicker; however, \n",
            "inputting confidential financial data so employees can inquire about the company’s performance is likely a step \n",
            "too far.\n",
            "To get started, your team should first consolidate and cleanse the data you intend to use. With RAG, it’s vital \n",
            "that your company stores the data in sizes that will be appropriate for the downstream models. Often, that \n",
            "requires users to splice it into smaller segments.\n",
            "Then, you should seek out a tool like Databricks Vector Search, which enables users to quickly set up their own \n",
            "vector database. And because it’s governed by Unity Catalog, granular controls can be put in place to ensure \n",
            "employees are only accessing the datasets for which they have credentials.\n",
            "Finally, you can then plug that endpoint into a LLM. A tool like Databricks MLflow helps to centralize the \n",
            "management of those APIs.\n",
            "25THE BIG BOOK OF GENERATIVE AI\n",
            "Among the benefits of RAG are reduced hallucinations, more up-to-date and accurate responses,  \n",
            "and better domain-specific intelligence. RAG-assisted models are also a more cost-effective approach  \n",
            "for most organizations.\n",
            "While RAG will help improve the results from commercial models, there are still many limitations to the use \n",
            "of RAG. If your business is unable to get the results it wants, it’s time to move on to heavier-weight solutions, \n",
            "but moving beyond RAG-supported models often requires a much deeper commitment. The additional \n",
            "customization costs more and requires a lot more data.\n",
            "That’s why it’s key that organizations first build a core understanding of how to use LLMs. By reaching the \n",
            "performance limitations of off-the-shelf models before moving on, you and your leadership can further hone  \n",
            "in on where to allocate resources.\n",
            "Enhance the Performance of Off-the-Shelf AI Models With RAG\n",
            "Let’s explore a practical use case that demonstrates how real-time structured data can significantly improve \n",
            "the response quality of your RAG applications. This example will showcase how integrating dynamic information \n",
            "can transform the effectiveness and applicability of AI in your business operations.\n",
            "26THE BIG BOOK OF GENERATIVE AI\n",
            "RAG Use Case\n",
            "Improve Your RAG Application Response Quality With Real-Time Structured Data\n",
            "by Mani Parkhe, Aakrati Talati, Sue Ann Hong, Craig Wiley, Chenen Liang and Mingyang Ge\n",
            "Retrieval augmented generation (RAG) is an efficient mechanism to provide relevant data as context in \n",
            "GenAI applications. Most RAG applications typically use vector indexes to search for relevant context from \n",
            "unstructured data such as documentation, wikis, and support tickets. Yesterday, we announced Databricks \n",
            "Vector Search Public Preview that helps with exactly that. However, GenAI response quality can be enhanced by \n",
            "augmenting these text-based contexts with relevant and personalized structured data. Imagine a GenAI tool on \n",
            "a retail website where customers inquire, \"Where’s my recent order?\" This AI must understand that the query \n",
            "is about a specific purchase, then gather up-to-date shipment information for line items, before using LLMs to \n",
            "generate a response. Developing these scalable applications demands substantial work, integrating technologies \n",
            "for handling both structured and unstructured data with GenAI capabilities.\n",
            "We are excited to announce the public preview of Databricks Feature & Function Serving, a low latency real-\n",
            "time service designed to serve structured data from the Databricks Data Intelligence Platform. You can instantly \n",
            "access pre-computed ML features as well as perform real-time data transformations by serving any Python \n",
            "function from Unity Catalog. The retrieved data can then be used in real-time rule engines, classical ML, and \n",
            "GenAI applications.\n",
            "Using Feature and Function Serving (AWS)(Azure) for structured data in coordination with Databricks Vector \n",
            "Search (AWS)(Azure) for unstructured data significantly simplifies productionalization of GenAI applications. \n",
            "Users can build and deploy these applications directly in Databricks and rely on existing data pipelines, \n",
            "governance, and other enterprise features. Databricks customers across various industries are using these \n",
            "technologies along with open source frameworks to build powerful GenAI applications such as the ones \n",
            "described in the table below.\n",
            "27INDUSTRY USE CASE\n",
            "Retail  ■ Product Recommendations / Search Ranking using user preferences, search history, location . . . etc.\n",
            " ■ Image and metadata based product search\n",
            " ■ Inventory management and forecasting using sales data, seasonal trends and market/competitive analysis\n",
            "Education  ■ Personalized learning plans based on past mistakes, historical trends and  cohorts\n",
            " ■ Automated grading, feedback, follow-ups and progress reporting\n",
            " ■ Content filtering for issued devices\n",
            "Financial Services  ■ Natural language apps for analysts and investors to correlate earning calls and reports with market intelligence and historical trends\n",
            " ■ Fraud and risk analysis\n",
            " ■ Personalized wealth management, retirement planning, what-if analysis and next-best actions \n",
            "Travel and Hospitality  ■ Chatbots for personalized customer interactions and tailored travel recommendations\n",
            " ■ Dynamic route planning using weather, live traffic patterns, and historical data\n",
            " ■ Dynamic price optimization using competitive analysis and demand-based pricing\n",
            "Healthcare and Life Sciences  ■ Patient/member engagement and health summaries\n",
            " ■ Support apps for personalized care, clinical decisions and care coordination\n",
            " ■ R&D report summarization, clinical trial analysis, drug repurposing\n",
            "Insurance  ■ Risk assessment for mortgage underwriting using text and structured data about properties and neighborhoods\n",
            " ■ User chatbots for questions about policies, risk and what-if analysis\n",
            " ■ Claim processing automation\n",
            "Technology and Manufacturing  ■ Prescriptive maintenance and diagnostics for equipment using guided instruction\n",
            " ■ Anomaly detection on live data stream against historical statistics\n",
            " ■ Automated analysis for daily production / shift analysis and future planning\n",
            "Media and Entertainment  ■ In-app content discovery and recommendations, personalized email and digital marketing\n",
            " ■ Content localization\n",
            " ■ Personalized gaming experiences and game review\n",
            "28\n",
            "THE BIG BOOK OF GENERATIVE AITHE BIG BOOK OF GENERATIVE AI\n",
            "SERVING STRUCTURED DATA TO RAG APPLICATIONS\n",
            "To demonstrate how structured data can help enhance the quality of a GenAI application, we use the following \n",
            "example for a travel planning chatbot. The example shows how user preferences (example: “ocean view” or \n",
            "“family friendly”) can be paired with unstructured information sourced about hotels to search for hotel matches. \n",
            "Typically hotel prices dynamically change based on demand and seasonality. A price calculator built into the \n",
            "GenAI application ensures that the recommendations are within the user's budget. The GenAI application that \n",
            "powers the bot uses Databricks Vector Search and Databricks Feature and Function Serving as building blocks \n",
            "to serve the necessary personalized user preferences and budget and hotel information using LangChain’s \n",
            "agents API.\n",
            "You can find the complete notebook for this RAG Chain application as depicted above. This application can be \n",
            "run locally within the notebook or deployed as an endpoint accessible by a chatbot user interface.\n",
            "29THE BIG BOOK OF GENERATIVE AI\n",
            "ACCESS YOUR DATA AND FUNCTIONS AS REAL-TIME ENDPOINTS\n",
            "With Feature Engineering in Unity Catalog you can already use any table with a primary key to serve features \n",
            "for training and serving. Databricks Model Serving supports using Python functions to compute features on-\n",
            "demand. Built using the same technology available under the hood for Databricks Model Serving, feature and \n",
            "function endpoints can be used to access any pre-computed feature or compute them on-demand. With a \n",
            "simple syntax you can define a feature spec function in Unity Catalog that can encode the directed acyclic \n",
            "graph to compute and serve features as a REST endpoint.\n",
            " \n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "from databricks.feature_engineering import (\n",
            "  FeatureFunction,\n",
            "  FeatureLookup,\n",
            "  FeatureEngineeringClient,\n",
            ")\n",
            "features = [\n",
            "  # Lookup columns `latitude` and `longitude` from `restaurants` table in UC using the input `restaurant_id` as key\n",
            "  FeatureLookup(\n",
            "    table_name=\"main.default.restaurants\",\n",
            "    lookup_key=\"restaurant_id\",\n",
            "    features=[\"latitude”, “longitude\"]\n",
            "  ),\n",
            "  # Calculate a new feature called `distance` using the restaurant and user's current location\n",
            "  FeatureFunction(\n",
            "    udf_name=\"main.default.distance\",\n",
            "    output_name=\"distance\",\n",
            "    # bind the function parameter with input from other features or from request.\n",
            "    input_bindings={\"user_latitude\": \"user_latitude\", \"user_longitude\": \"user_longitude\",\n",
            "                    \"restaurant_latitude\": \"latitude\", \"restaurant_longitude\": \"longitude\"},\n",
            "  ),\n",
            "]\n",
            "fe = FeatureEngineeringClient()\n",
            "# Create a feature spec with the features listed above.\n",
            "# The FeatureSpec can be accessed in UC as a Function.\n",
            "fe.create_feature_spec(\n",
            "  name=\"main.default.restaurant_features\",\n",
            "  features=features,\n",
            ")\n",
            "30THE BIG BOOK OF GENERATIVE AI\n",
            " \n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            " \n",
            "5\n",
            "6\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "from databricks.feature_engineering.entities.feature_serving_endpoint import (\n",
            "  ServedEntity,\n",
            "  EndpointCoreConfig,\n",
            ")\n",
            "fe.create_feature_serving_endpoint(\n",
            "  name=\"restaurant-features\",\n",
            "    config=EndpointCoreConfig(\n",
            "    served_entities=ServedEntity(\n",
            "      feature_spec_name=\"main.default.restaurant_features\",\n",
            "      workload_size=\"Small\",\n",
            "      scale_to_zero_enabled=True\n",
            "    )\n",
            "  )\n",
            ")\n",
            "This feature spec function can be served in real-time as a REST endpoint. All endpoints are accessible in \n",
            "the Serving left navigation tab including features, function, custom trained models, and foundation models. \n",
            "Provision the endpoint using this API.\n",
            "The endpoint can also be created using a UI workflow as shown in the following graphic\n",
            "31THE BIG BOOK OF GENERATIVE AI\n",
            "Now features can be accessed in real time by querying the endpoint:\n",
            "To serve structured data to real-time AI applications, precomputed data needs to be deployed to operational \n",
            "databases. Users can already use external online stores as a source of precomputed features — for example \n",
            "DynamoDB and Cosmos DB are commonly used to serve features in Databricks Model Serving. Databricks \n",
            "Online Tables (AWS)(Azure) adds new functionality that simplifies synchronization of precomputed features to a \n",
            "data format optimized for low latency data lookups. You can sync any table with a primary key as an online table \n",
            "and the system will set up an automatic pipeline to ensure data freshness.\n",
            " \n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            " \n",
            "6\n",
            "curl \\\n",
            "  -u token:$DATABRICKS_TOKEN \\\n",
            "  -X POST \\\n",
            "  -H \"Content-Type: application/json\" \\\n",
            "  -d '{\"dataframe_records\": [{\"user_latitude\": 37.9711, \"user_longitude\": -122.3940, \"restaurant_id\": 5}]}' \\ \n",
            "  https://<databricks-instance>/serving-endpoints/restaurant-features/invocations\n",
            "32THE BIG BOOK OF GENERATIVE AI\n",
            "Stage 3: Fine-Tuning a Foundation Model\n",
            "Moving beyond RAG to model fine-tuning lets you start building models that are much more deeply \n",
            "personalized to the business. If you have already been experimenting with commercial models across your \n",
            "operations, you are likely ready to advance to this stage. There’s a clear understanding at the executive level of \n",
            "the value of generative AI, as well as an understanding of the limitations of publicly available LLMs. Specific use \n",
            "cases have been established. And now, you and your enterprise are ready to go deeper.\n",
            "With fine-tuning, you can take a general-purpose model and train it on your own specific data. For example, \n",
            "data management provider Stardog relies on the Mosaic AI tools from Databricks to fine-tune the off-the-shelf \n",
            "LLMs they use as a foundation for their Knowledge Graph Platform. This enables Stardog’s customers to query \n",
            "their own data across the different silos simply by using natural language.\n",
            "It’s imperative that organizations at this stage have an underlying architecture in place that will help ensure the \n",
            "data supporting the models is secure and accurate. Fine-tuning an AI system requires an immense amount of \n",
            "proprietary information and, as your business advances on the AI maturity curve, the number of models running \n",
            "will only grow, increasing the demand for data access.\n",
            "That’s why you need to have the right mechanisms in place to track data from the moment it’s generated to \n",
            "when it’s eventually used, and why Unity Catalog is such a popular feature among Databricks customers. With \n",
            "Unity Catalog’s data lineage capabilities, businesses always know where data is moving and who is accessing it.\n",
            "Any Unity Catalog table with primary keys can be used to serve features in GenAI applications using Databricks \n",
            "Online Tables.\n",
            "33THE BIG BOOK OF GENERATIVE AI\n",
            "Fine-Tuning Use Cases\n",
            "Creating a Bespoke LLM for AI-Generated Documentation \n",
            "It’s easier than you think: 2 engineers, 1 month and less than $1,000\n",
            "by Matthew Hayes, Hongyi Zhang, Tao Feng, Jan van der Vegt, Zaheera Valani and Reynold Xin\n",
            "In this example, we share our experience from prototyping a hackathon project using off-the-shelf SaaS-based \n",
            "LLMs to creating a bespoke LLM that is better, faster, and cheaper. The new model took 2 engineers, 1 month \n",
            "and less than $1,000 in compute cost to develop. We hope you will find the learnings useful, as we believe \n",
            "they apply to a wide class of GenAI use cases. More importantly, it has allowed us to take advantage of rapid \n",
            "advances being made in open-source LLMs.\n",
            "WHAT IS AI-GENERATED DOCUMENTATION?\n",
            "At the center of each data platform lies a (potentially enormous) collection of datasets (often in the form of \n",
            "tables). In virtually every organization we have worked with, the vast majority of tables are not documented. The \n",
            "absence of documentation provides a number of challenges, including making it difficult for humans to discover \n",
            "the data needed for answering a business question, or more recently, for AI agents to automatically find \n",
            "datasets to use in response to questions (a key capability in our platform that we’re calling Data Intelligence).\n",
            "34THE BIG BOOK OF GENERATIVE AI\n",
            "Rather than relying on humans to document these datasets, we prototyped as part of our quarterly hackathon \n",
            "a new workflow using an off-the-shelf SaaS-based LLM to automatically generate documentation for tables \n",
            "and their columns based on their schema. This new workflow would automatically suggest descriptions for the \n",
            "tables and columns and allow users to either individually accept, bulk accept, or modify the suggestions for \n",
            "higher fidelity, as shown below. When we showed this prototype to some users, their immediate question was \n",
            "universally, “When can I have it?!”\n",
            "35THE BIG BOOK OF GENERATIVE AI\n",
            "CHALLENGES WITH LLMS\n",
            "As we moved toward launching this feature to all our customers, we ran into three challenges with the model:\n",
            "1. Quality: The ultimate success of this feature depends on the quality of the generated documentation. \n",
            "Although we could measure the quality (in terms of how often they are accepted), we had limited knobs \n",
            "at our disposal to improve it, aside from basic prompting. During the private preview period, we also \n",
            "sometimes noticed the quality of the suggestions degrading, without any change to our codebase. Our \n",
            "speculation is that the SaaS LLM controller rolled out updates to the model that sometimes affected \n",
            "performance on specific tasks.\n",
            "2. Performance (throughput): We had limited API quota provisioned with the SaaS LLM provider. We \n",
            "work with tens of thousands of organizations, and it is not uncommon that a single organization would \n",
            "have millions of tables. It would take too long to generate documentation for all the tables based on the \n",
            "throughput quota.\n",
            "3. Cost: Related to the above, it was not cost-effective unless we started charging customers for using this \n",
            "specific feature.\n",
            "We have heard similar concerns from a variety of customers as they try to move their LLM-based applications \n",
            "from a proof-of-concept to production and saw this as an excellent opportunity for us to explore alternatives \n",
            "for an organization like ours.\n",
            "We experimented with different versions of the SaaS LLMs, but they all had the same challenges. This is not \n",
            "surprising in hindsight. The SaaS LLMs are an engineering marvel, but they are very general models that need to \n",
            "address all the use cases from table generation to conversing about the meaning of life. The generality means \n",
            "it needs to have an extremely large number of parameters, which limits how fast and how cheap it can return \n",
            "answers. As it continues to evolve to optimize for different use cases, it might also regress the narrower use case \n",
            "we have.\n",
            "36THE BIG BOOK OF GENERATIVE AI\n",
            "BUILDING A BESPOKE MODEL\n",
            "To address the aforementioned challenges, we started building a bespoke model. It took a team of two \n",
            "engineers one month to build a customized, smaller LLM that was better, faster, and cheaper:\n",
            " ■ Quality: Based on our evaluation (see the following section), the model is significantly better than the \n",
            "cheaper version of the SaaS model, and roughly equivalent to the more expensive version.\n",
            " ■ Performance (throughput): Because the bespoke model is a lot smaller, it can fit in A10 GPUs, and we \n",
            "can increase the inference throughput with horizontal scaling. The smaller GPUs are also more available, \n",
            "which enables us to generate the descriptions for all tables faster.\n",
            " ■ Cost: Each fine-tuning run of the model only costs a few dollars, and in aggregate, it cost less than $1000 \n",
            "to develop because we did a lot of experiments. It also resulted in a 10 fold reduction in inference cost.\n",
            "The first step was to treat this as an applied machine learning problem. “Applied machine learning” sounds \n",
            "daunting and complicated, but all it meant was that we needed to:\n",
            " ■ Find training datasets so we can bootstrap an initial model\n",
            " ■ Identify an evaluation mechanism so we can measure the quality, before rolling it out to production\n",
            " ■ Train and select models\n",
            " ■ Collect real-world usage metrics, so we can monitor how well a monitor does in production\n",
            " ■ Iterate and roll out new models to continuously improve the three dimensions: quality, performance, cost\n",
            "37THE BIG BOOK OF GENERATIVE AI\n",
            "TRAINING DATA\n",
            "We created the initial training dataset for this fine-tuning task, using two different sources of data:\n",
            "1. North American Industry Classification System (NAICS) codes. This is a public dataset used by Federal \n",
            "statistical agencies in classifying business establishments for the purpose of collecting, analyzing, and \n",
            "publishing statistical data related to the U.S. business economy.\n",
            "2. Databricks’ internal use case taxonomy curation datasets. This is a series of internal datasets created by \n",
            "our solution architects to show customers best practice architectures.\n",
            "Then we synthesized CREATE TABLE statements using the above use cases to yield a diverse set of tables and \n",
            "generated sample responses including table descriptions and column comments using another LLM. In total,  \n",
            "we generated ~3600 training examples. \n",
            "Notably, we didn’t use any customer data for training this powerful feature that all of our customers can  \n",
            "benefit from. \n",
            "BOOTSTRAPPING MODEL EVALUATION\n",
            "After the feature launch, we could measure a model’s quality through production metrics such as the rate of \n",
            "users accepting the suggestions. But before we made it to the launch, we needed a way to evaluate the model’s \n",
            "quality against that of the SaaS LLM.\n",
            "To do that in an unbiased fashion, we set up a simple double-blind evaluation framework in which we asked \n",
            "4 employees to rate table descriptions generated from the two models we wanted to compare using a set of \n",
            "62 unseen tables. Our framework then generated a sheet where each row showed the input and showed both \n",
            "outputs in a randomized order. The evaluator would vote on the better sample (or give a tie). The framework then \n",
            "processed the votes from different evaluators to generate a report; it also summarizes the degree to which each \n",
            "of the evaluators agreed.\n",
            "Based on our experiences so far, having an evaluation dataset of tens to hundreds of data points is a sufficient \n",
            "initial milestone and can be generalized to other use cases as well.\n",
            "38THE BIG BOOK OF GENERATIVE AI\n",
            "MODEL SELECTION AND FINE-TUNING\n",
            "We considered the following criteria for model selection:\n",
            " ■ Whether the license supports commercial use\n",
            " ■ Performance (quality) of the model for text generation\n",
            " ■ Speed of the model\n",
            "Based on these criteria, MPT-7B and Llama2-7B were the leading candidates, as shown in our LLM guide. We \n",
            "considered larger models such as MPT-30B and Llama-2-13B. In the end we chose MPT-7B, as it has the best \n",
            "combination of quality and inference performance:\n",
            " ■ There was no discernable difference in the quality between the MPT-7B and Llama-2-7B fine-tuned \n",
            "models for this task.\n",
            " ■ The smaller 7B models, after fine-tuning, were already meeting the quality bar. It was significantly better \n",
            "than the cheaper version of the SaaS model, and roughly equivalent to the more expensive version.\n",
            " ■ We did not yet observe a measurable benefit of using larger models for this task that would justify the \n",
            "increased serving costs.\n",
            " ■ The latency for the smaller models was significantly better than the larger models while offering \n",
            "comparable quality so we could deliver a much snappier product experience.\n",
            " ■ The smaller model could fit comfortably and be served using A10 GPUs, which were more readily \n",
            "available. Their abundance would mean higher inference throughput for the task.\n",
            "The total time it took to fine-tune the model on the ~3600 examples was only around 15 minutes!\n",
            "While we chose MPT-7B for our model, we believe the LLM landscape is changing rapidly and the best model \n",
            "today won’t be the best model tomorrow. That’s why we consider this to be an iterative and continuous process \n",
            "and are focused on using tools that make our evaluation efficient and fast.\n",
            "39THE BIG BOOK OF GENERATIVE AI\n",
            "KEY ARCHITECTURAL COMPONENTS OF OUR PRODUCTION PIPELINE\n",
            "We were able to build this quickly by relying on the following key components of the Databricks Data \n",
            "Intelligence Platform:\n",
            " ■ Databricks LLM fine-tuning: It provides a very simple infrastructure for fine-tuning the models for our \n",
            "task. We prepared the training data in JSON format, and with a one-line CLI command, we were able to \n",
            "fine-tune the LLMs.\n",
            " ■ Unity Catalog: The models that we use in production are registered in Unity Catalog (UC), providing the \n",
            "governance we need to not just for the data, but also the models. With its end-to-end lineage feature, UC \n",
            "also gives us traceability from the models back to the datasets they are trained on.\n",
            " ■ Delta Sharing: We used Delta Sharing to distribute the model to all production regions we have around \n",
            "the world for faster serving.\n",
            " ■ Databricks optimized LLM serving: Once the models are registered in UC, they can be served using the \n",
            "new optimized LLM serving, which provides significant performance improvement in terms of throughput \n",
            "and latency improvement compared to traditional serving for LLM serving.\n",
            "40THE BIG BOOK OF GENERATIVE AI\n",
            "COST\n",
            "The fine-tuning compute cost for the whole project was less than $1000 (each fine-tuning run cost only a few \n",
            "dollars). And the final result is a more than 10-fold reduction in cost. Why is the cost-saving so significant? It is \n",
            "not surprising if we consider the following:\n",
            " ■ As mentioned earlier, the SaaS LLMs need to address all the use cases, including acting as a general \n",
            "chatbot. The generality requires an extremely large number of parameters, which incurs significant \n",
            "compute costs in inference.\n",
            " ■ When we fine-tune for a more specific task, we can use a much smaller prompt. Larger, general-purpose \n",
            "models require longer prompts that include detailed instructions on what the input is and what form the \n",
            "output should take. Fine-tuned models can bake instructions and expected structure into the model \n",
            "itself. We found we were able to reduce the number of input tokens with no impact on performance by \n",
            "more than half.\n",
            " ■ Inference costs scale with the number of input and output tokens, and costs scale linearly for SaaS \n",
            "services that are charged per token. With Databricks’ LLM Serving offering, we offer provisioned \n",
            "throughput charged per hour, which provides consistent latencies, uptime SLAs, and autoscaling. Because \n",
            "smaller LLMs can fit in smaller GPUs that are much cheaper and more available and because we offer a \n",
            "highly optimized runtime, we can aggressively drive down costs. Also, smaller LLMs scale up and down \n",
            "faster, meaning we can quickly scale up to meet peaks of demand and aggressively scale down when \n",
            "usage is lighter, creating substantial cost efficiency in production.\n",
            "41THE BIG BOOK OF GENERATIVE AI\n",
            "CONCLUSION\n",
            "Having well-documented data is critical to all data users, and growing more important day-by-day to power \n",
            "AI-based data platforms (what we’re calling Data Intelligence). We started with SaaS LLMs for prototyping this \n",
            "new GenAI feature but ran into challenges with quality, performance, and cost. We built a bespoke model to do \n",
            "the same task at better quality, and yet resulting in higher throughput with scale-out and 10x cost reduction. To \n",
            "recap what it took:\n",
            " ■ 2 engineers\n",
            " ■ 1 month\n",
            " ■ Less than $1,000 in compute for training and experimentation\n",
            " ■ MPT-7B fine-tuned on 3600 synthetically generated examples, in under 15 minutes\n",
            " ■ 4 human evaluators, with 62 initial evaluation examples\n",
            "This experience demonstrates how easy it is to develop and deploy bespoke LLMs for specific tasks. This model \n",
            "is now live on Databricks in Amazon Web Services and Google Cloud and is being used to power most data \n",
            "annotations on the platform.\n",
            "42THE BIG BOOK OF GENERATIVE AI\n",
            "Efficient Fine-Tuning With LoRA: A Guide to Optimal Parameter Selection for Large Language Models\n",
            "by Avinash Sooriyarachchi\n",
            "With the rapid advancement of neural network-based techniques and large language model (LLM) research, \n",
            "businesses are increasingly interested in AI applications for value generation. They employ various machine \n",
            "learning approaches, both generative and non-generative, to address text-related challenges such as \n",
            "classification, summarization, sequence-to-sequence tasks, and controlled text generation. Organizations can \n",
            "opt for third-party APIs, but fine-tuning models with proprietary data offers domain-specific and pertinent \n",
            "results, enabling cost-effective and independent solutions deployable across different environments in  \n",
            "a secure manner.\n",
            "Ensuring efficient resource utilization and cost-effectiveness is crucial when choosing a strategy for fine-\n",
            "tuning. This blog explores arguably the most popular and effective variant of such parameter efficient \n",
            "methods, Low Rank Adaptation (LoRA), with a particular emphasis on QLoRA (an even more efficient variant of \n",
            "LoRA). The approach here will be to take an open large language model and fine-tune it to generate fictitious \n",
            "product descriptions when prompted with a product name and a category. The model chosen for this \n",
            "exercise is OpenLLaMA-3b-v2, an open large language model with a permissive license (Apache 2.0), and the \n",
            "dataset chosen is Red Dot Design Award Product Descriptions, both of which can be downloaded from the \n",
            "HuggingFace Hub at the links provided.\n",
            "FINE-TUNING, LORA AND QLORA\n",
            "In the realm of language models, fine-tuning an existing language model to perform a specific task on specific \n",
            "data is a common practice. This involves adding a task-specific head, if necessary, and updating the weights of \n",
            "the neural network through backpropagation during the training process. It is important to note the distinction \n",
            "between this fine-tuning process and training from scratch. In the latter scenario, the model's weights are \n",
            "randomly initialized, while in fine-tuning, the weights are already optimized to a certain extent during the \n",
            "pretraining phase. The decision of which weights to optimize or update, and which ones to keep frozen, depends \n",
            "on the chosen technique.\n",
            "Full fine-tuning involves optimizing or training all layers of the neural network. While this approach typically \n",
            "yields the best results, it is also the most resource-intensive and time-consuming.\n",
            "43THE BIG BOOK OF GENERATIVE AI\n",
            "Fortunately, there exist parameter-efficient approaches for fine-tuning that have proven to be effective. \n",
            "Although most such approaches have yielded less performance, Low Rank Adaptation (LoRA) has bucked \n",
            "this trend by even outperforming full fine-tuning in some cases, as a consequence of avoiding catastrophic \n",
            "forgetting (a phenomenon which occurs when the knowledge of the pretrained model is lost during the  \n",
            "fine-tuning process).\n",
            "LoRA is an improved fine-tuning method where instead of fine-tuning all the weights that constitute the  \n",
            "weight matrix of the pretrained large language model, two smaller matrices that approximate this larger matrix \n",
            "are fine-tuned. These matrices constitute the LoRA adapter. This fine-tuned adapter is then loaded to the \n",
            "pretrained model and used for inference.\n",
            "QLoRA is an even more memory efficient version of LoRA where the pretrained model is loaded to GPU  \n",
            "memory as quantized 4-bit weights (compared to 8-bits in the case of LoRA), while preserving similar \n",
            "effectiveness to LoRA. Probing this method, comparing the two methods when necessary, and figuring out the \n",
            "best combination of QLoRA hyperparameters to achieve optimal performance with the quickest training time \n",
            "will be the focus here.\n",
            "LoRA is implemented in the Hugging Face Parameter Efficient Fine-Tuning (PEFT) library, offering ease  \n",
            "of use and QLoRA can be leveraged by using bitsandbytes and PEFT together. HuggingFace Transformer \n",
            "Reinforcement Learning (TRL) library offers a convenient trainer for supervised fine-tuning with seamless \n",
            "integration for LoRA. These three libraries will provide the necessary tools to fine-tune the chosen pretrained \n",
            "model to generate coherent and convincing product descriptions once prompted with an instruction  \n",
            "indicating the desired attributes.\n",
            "44THE BIG BOOK OF GENERATIVE AI\n",
            "PREPPING THE DATA FOR SUPERVISED FINE-TUNING\n",
            "To probe the effectiveness of QLoRA for fine-tuning a model for instruction following, it is essential to transform \n",
            "the data to a format suited for supervised fine-tuning. Supervised fine-tuning in essence, further trains a \n",
            "pretrained model to generate text conditioned on a provided prompt. It is supervised in that the model is fine-\n",
            "tuned on a dataset that has prompt-response pairs formatted in a consistent manner.\n",
            "An example observation from our chosen dataset from the Hugging Face hub looks as follows:\n",
            "As useful as this dataset is, this is not well formatted for fine-tuning of a language model for instruction following \n",
            "in the manner described.\n",
            "The following code snippet loads the dataset from the Hugging Face hub into memory, transforms the \n",
            "necessary fields into a consistently formatted string representing the prompt, and inserts the response (i.e., \n",
            "the description), immediately afterward. This format is known as the ‘Alpaca format’ in large language model \n",
            "research circles as it was the format used to fine-tune the original LlaMA model from Meta to result in the \n",
            "Alpaca model, one of the first widely distributed instruction-following large language models (although not \n",
            "licensed for commercial use).\n",
            "PRODUCT CATEGORY DESCRIPTION TEXT\n",
            "“Biamp Rack Products” “Digital Audio Processors\" “High recognition value, uniform \n",
            "aesthetics and practical \n",
            "scalability — this has been \n",
            "impressively achieved with the \n",
            "Biamp brand language . . . “\n",
            "“Product Name: Biamp Rack Products; \n",
            "Product Category: Digital Audio \n",
            "Processors; Product Description: High \n",
            "recognition value, uniform aesthetics \n",
            "and practical scalability — this has been \n",
            "impressively achieved with the Biamp \n",
            "brand language . . . “\n",
            "45THE BIG BOOK OF GENERATIVE AI\n",
            " \n",
            "1\n",
            "2\n",
            "3\n",
            "4 \n",
            "5\n",
            "6\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "import pandas as pd\n",
            "from datasets import load_dataset\n",
            "from datasets import Dataset\n",
            "#Load the dataset from the HuggingFace Hub\n",
            "rd_ds = load_dataset(\"xiyuez/red-dot-design-award-product-description\")\n",
            "#Convert to pandas dataframe for convenient processing\n",
            "rd_df = pd.DataFrame(rd_ds['train'])\n",
            "#Combine the two attributes into an instruction string\n",
            "rd_df['instruction'] = 'Create a detailed description for the following product: '+ rd_df['product']+', belonging to \n",
            "category: '+ rd_df['category']\n",
            "rd_df = rd_df[['instruction', 'description']]\n",
            "#Get a 5000 sample subset for fine-tuning purposes\n",
            "rd_df_sample = rd_df.sample(n=5000, random_state=42)\n",
            "#Define template and format data into the template for supervised fine-tuning\n",
            "template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the \n",
            "request.\n",
            "### Instruction:\n",
            "{}\n",
            "### Response:\\n\"\"\"\n",
            "rd_df_sample['prompt'] = rd_df_sample[\"instruction\"].apply(lambda x: template.format(x))\n",
            "rd_df_sample.rename(columns={'description': 'response'}, inplace=True)\n",
            "rd_df_sample['response'] = rd_df_sample['response'] + \"\\n### End\"\n",
            "rd_df_sample = rd_df_sample[['prompt', 'response']]\n",
            "rd_df['text'] = rd_df[\"prompt\"] + rd_df[\"response\"]\n",
            "rd_df.drop(columns=['prompt', 'response'], inplace=True)\n",
            "The resulting prompts are then loaded into a hugging face dataset for supervised fine-tuning. Each such prompt \n",
            "has the following format.\n",
            "46THE BIG BOOK OF GENERATIVE AI\n",
            " \n",
            "1\n",
            "2\n",
            "3\n",
            "4 \n",
            "5\n",
            "6\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "```\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "### Instruction:\n",
            "Create a detailed description for the following product: Beseye Pro, belonging to category: Cloud-Based Home Security \n",
            "Camera\n",
            "### Response:\n",
            "Beseye Pro combines intelligent home monitoring with decorative art. The camera, whose form is reminiscent of a water \n",
            "drop, is secured in the mounting with a neodymium magnet and can be rotated by 360 degrees. This allows it to be \n",
            "easily positioned in the desired direction. The camera also houses modern technologies, such as infrared LEDs, cloud-\n",
            "based intelligent video analyses and SSL encryption.\n",
            "### End\n",
            "```\n",
            "To facilitate quick experimentation, each fine-tuning exercise will be done on a 5000 observation subset  \n",
            "of this data.\n",
            "TESTING MODEL PERFORMANCE BEFORE FINE-TUNING\n",
            "Before any fine-tuning, it’s a good idea to check how the model performs without any fine-tuning to get a \n",
            "baseline for pretrained model performance.\n",
            "The model can be loaded in 8-bit as follows and prompted with the format specified in the model card on \n",
            "Hugging Face.\n",
            "47THE BIG BOOK OF GENERATIVE AI\n",
            " \n",
            "1\n",
            "2\n",
            "3\n",
            "4 \n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "import torch\n",
            "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
            "model_path = 'openlm-research/open_llama_3b_v2'\n",
            "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
            "model = LlamaForCausalLM.from_pretrained(\n",
            "model_path, load_in_8bit=True, device_map='auto',\n",
            ")\n",
            "#Pass in a prompt and infer with the model\n",
            "prompt = 'Q: Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: \n",
            "Optical Mouse\\nA:'\n",
            "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
            "generation_output = model.generate(\n",
            "input_ids=input_ids, max_new_tokens=128\n",
            ")\n",
            "print(tokenizer.decode(generation_output[0]))\n",
            " \n",
            "1\n",
            "2\n",
            "3\n",
            "4 \n",
            "5\n",
            "6\n",
            "Q: Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical \n",
            "Mouse A: The Corelogic Smooth Mouse is a wireless optical mouse that has a 1000 dpi resolution. It has a 2.4 GHz \n",
            "wireless connection and a 12-month warranty. Q: What is the price of the Corelogic Smooth Mouse? A: The Corelogic \n",
            "Smooth Mouse is priced at $29.99. Q: What is the weight of the Corelogic Smooth Mouse? A: The Corelogic Smooth Mouse \n",
            "weighs 0.1 pounds. Q: What is the dimensions of the Corelogic Smooth Mouse? A: The Corelogic Smooth Mouse has a \n",
            "dimension\n",
            "The output obtained is not quite what we want.\n",
            "The first part of the result is actually satisfactory, but the rest of it is more of a rambling mess.\n",
            "48THE BIG BOOK OF GENERATIVE AI\n",
            " \n",
            "1\n",
            "2\n",
            "3\n",
            "4 \n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "prompt= \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "### Instruction:\n",
            "Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical Mouse\n",
            "### Response:\"\"\"\n",
            "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
            "generation_output = model.generate(\n",
            "input_ids=input_ids, max_new_tokens=128\n",
            ")\n",
            "print(tokenizer.decode(generation_output[0]))\n",
            " \n",
            "1\n",
            "2\n",
            "3\n",
            "4 \n",
            "5\n",
            "6\n",
            "Corelogic Smooth Mouse is a mouse that is designed to be used by people with disabilities. It is a wireless mouse \n",
            "that is designed to be used by people with disabilities. It is a wireless mouse that is designed to be used by people \n",
            "with disabilities. It is a wireless mouse that is designed to be used by people with disabilities. It is a wireless \n",
            "mouse that is designed to be used by people with disabilities. It is a wireless mouse that is designed to be used by \n",
            "people with disabilities. It is a wireless mouse that is designed to be used by people with disabilities. It is a \n",
            "wireless mouse that is designed to be used by\n",
            "Similarly, if the model is prompted with the input text in the ‘Alpaca format’ as discussed before, the output is \n",
            "expected to be just as suboptimal:\n",
            "And sure enough, it is:\n",
            "The model performs what it was trained to do, predicts the next most probable token. The point of supervised \n",
            "fine-tuning in this context is to generate the desired text in a controllable manner. Please note that in the \n",
            "subsequent experiments, while QLoRA leverages a model loaded in 4-bit with the weights frozen, the  \n",
            "inference process to examine output quality is done once the model has been loaded in 8-bit as shown  \n",
            "above for consistency.\n",
            "49THE BIG BOOK OF GENERATIVE AI\n",
            "THE TURNABLE KNOBS\n",
            "When using PEFT to train a model with LoRA or QLoRA (note that, as mentioned before, the primary difference \n",
            "between the two is that in the latter, the pretrained models are frozen in 4-bit during the fine-tuning process), \n",
            "the hyperparameters of the low rank adaptation process can be defined in a LoRA config as shown below:\n",
            "Two of these hyperparameters, r and target_modules are empirically shown to affect adaptation quality \n",
            "significantly and will be the focus of the tests that follow. The other hyperparameters are kept constant at the \n",
            "values indicated above for simplicity.\n",
            "r represents the rank of the low rank matrices learned during the fine-tuning process. As this value is increased, \n",
            "the number of parameters needed to be updated during the low-rank adaptation increases. Intuitively, a lower \n",
            "r may lead to a quicker, less computationally intensive training process, but may affect the quality of the model \n",
            "thus produced. However, increasing r beyond a certain value may not yield any discernible increase in quality of \n",
            "model output. How the value of r affects adaptation (fine-tuning) quality will be put to the test shortly.\n",
            " \n",
            "1\n",
            "2\n",
            "3\n",
            "4 \n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "from peft import LoraConfig\n",
            "...\n",
            "...\n",
            "#If only targeting attention blocks of the model\n",
            "target_modules = [\"q_proj\", \"v_proj\"]\n",
            "#If targeting all linear layers\n",
            "target_modules = ['q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_proj','lm_head']\n",
            "lora_config = LoraConfig(\n",
            "r=16,\n",
            "target_modules = target_modules,\n",
            "lora_alpha=8,\n",
            "lora_dropout=0.05,\n",
            "bias=\"none\",\n",
            "task_type=\"CAUSAL_LM\",}\n",
            "50THE BIG BOOK OF GENERATIVE AI\n",
            "When fine-tuning with LoRA, it is possible to target specific modules in the model architecture. The adaptation \n",
            "process will target these modules and apply the update matrices to them. Similar to the situation with \"r,\" \n",
            "targeting more modules during LoRA adaptation results in increased training time and greater demand for \n",
            "compute resources. Thus, it is a common practice to only target the attention blocks of the transformer. \n",
            "However, recent work as shown in the QLoRA paper by Dettmers et al. suggests that targeting all linear layers \n",
            "results in better adaptation quality. This will be explored here as well.\n",
            "Names of the linear layers of the model can be conveniently appended to a list with the following code snippet:\n",
            "TUNING THE FINE-TUNING WITH LORA\n",
            "The developer experience of fine-tuning large language models in general have improved dramatically over the \n",
            "past year or so. The latest high level abstraction from Hugging Face is the SFTTrainer class in the TRL library. To \n",
            "perform QLoRA, all that is needed is the following:\n",
            "1. Load the model to GPU memory in 4-bit (bitsandbytes enables this process)\n",
            "2. Define the LoRA configuration as discussed previously\n",
            "3. Define the train and test splits of the prepped instruction following data into Hugging Face  \n",
            "Dataset objects\n",
            "4. Define training arguments: These include the number of epochs, batch size and other training \n",
            "hyperparameters which will be kept constant during this exercise\n",
            "5. Pass these arguments into an instance of SFTTrainer\n",
            "These steps are clearly indicated in the source file in the repository associated with this blog.\n",
            " \n",
            "1\n",
            "2\n",
            "3\n",
            "4 \n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "import re\n",
            "model_modules = str(model.modules)\n",
            "pattern = r'\\((\\w+)\\): Linear'\n",
            "linear_layer_names = re.findall(pattern, model_modules)\n",
            "names = []\n",
            "# Print the names of the Linear layers\n",
            "for name in linear_layer_names:\n",
            "    names.append(name)\n",
            "target_modules = list(set(names))\n",
            "51THE BIG BOOK OF GENERATIVE AI\n",
            "The actual training logic is abstracted away nicely as follows:\n",
            "If MLflow autologging is enabled in the Databricks workspace, which is highly recommended, all the training \n",
            "parameters and metrics are automatically tracked and logged with the MLflow tracking server. This functionality \n",
            "is invaluable in monitoring long-running training tasks. Needless to say, the fine-tuning process is performed \n",
            "using a compute cluster (in this case, a single node with a single A100 GPU) created using the latest Databricks \n",
            "Machine Runtime with GPU support.\n",
            " \n",
            "1\n",
            "2\n",
            "3\n",
            "4 \n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "trainer = SFTTrainer(\n",
            "model,\n",
            "train_dataset=dataset['train'],\n",
            "eval_dataset = dataset['test'],\n",
            "dataset_text_field=\"text\",\n",
            "max_seq_length=256,\n",
            "args=training_args,\n",
            ")\n",
            "# Initiate the training process\n",
            "with mlflow.start_run(run_name= ‘run_name_of_choice’):\n",
            "trainer.train()\n",
            "52THE BIG BOOK OF GENERATIVE AI\n",
            "HYPERPARAMETER COMBINATION #1: QLoRA with r=8 and targeting “q_proj”, “v_proj”\n",
            "The first combination of QLoRA hyperparameters attempted is r=8 and targets only the attention blocks, namely \n",
            "“q_proj” and “v_proj” for adaptation.\n",
            "The following code snippets gives the number of trainable parameters:\n",
            "These choices result in 2,662,400 parameters being updated during the fine-tuning process (~2.6 million) from a \n",
            "total of ~3.2 billion parameters the model consists of. This is less than 0.1% of the model parameters. The entire \n",
            "fine-tuning process on a single Nvidia A100 with 80 GBs of GPU for 3 epochs only takes roughly 12 minutes. The \n",
            "GPU utilization metrics can be conveniently viewed at the metrics tab of the cluster configurations.\n",
            " \n",
            "1\n",
            "2\n",
            "model = get_peft_model(model, lora_config)\n",
            "model.print_trainable_parameters()\n",
            "53THE BIG BOOK OF GENERATIVE AI\n",
            "At the end of the training process, the fine-tuned model is obtained by loading the adapter weights to the \n",
            "pretrained model as follows:\n",
            "This model can now be used for inference as any other model.\n",
            "Qualitative Evaluation \n",
            "A couple of example prompt-response pairs are listed below\n",
            "Prompt (passed to the model in the Alpaca format, not shown for conciseness here): \n",
            "Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category:  \n",
            "Optical Mouse\n",
            "Response:\n",
            "Prompt: \n",
            "Create a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless \n",
            "Vacuum Cleaner\n",
            "Response:\n",
            "The model has clearly been adapted for generating more consistent descriptions. However the response to the \n",
            "first prompt about the optical mouse is quite short and the following phrase “The vacuum cleaner is equipped \n",
            "with a dust container that can be emptied via a dust container” is logically flawed.\n",
            " \n",
            "1 peft_model = PeftModel.from_pretrained(model, adapter_location)\n",
            " \n",
            "1\n",
            "2\n",
            "The Corelogic Smooth Mouse is a wireless optical mouse with a smooth surface. The mouse is equipped with a 1000 DPI \n",
            "sensor and a 1000 Hz polling rate. The mouse is available in black and white.\n",
            " \n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "The Hoover Lightspeed is a cordless vacuum cleaner that is equipped with a lithium-ion battery. The battery is \n",
            "charged via a USB cable. The vacuum cleaner is equipped with a 2-in-1 brush and a turbo brush. The brush is suitable \n",
            "for cleaning carpets and hard floors. The turbo brush is suitable for cleaning carpets and hard floors. The vacuum \n",
            "cleaner is equipped with a dust container that can be emptied via a dust container.\n",
            "54THE BIG BOOK OF GENERATIVE AI\n",
            "HYPERPARAMETER COMBINATION #2: QLoRA with r=16 and targeting all linear layers\n",
            "Surely, things can be improved here. It is worth exploring increasing the rank of low rank matrices learned during \n",
            "adaptation to 16, i.e., double the value of r to 16 and keep all else  the same. This doubles the number of trainable \n",
            "parameters to 5,324,800 (~5.3 million)\n",
            "Qualitative Evaluation \n",
            "The quality of output, however, remains unchanged for the same exact prompts.\n",
            "Prompt: \n",
            "Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category:  \n",
            "Optical Mouse\n",
            "Response:\n",
            " \n",
            "1\n",
            "2\n",
            "The Corelogic Smooth Mouse is a high-quality optical mouse with a smooth surface. The mouse is equipped with a 1000 \n",
            "DPI sensor and a 1000 Hz polling rate. The mouse is available in black and white.\n",
            " \n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "The Hoover Lightspeed is a cordless vacuum cleaner that is equipped with a lithium-ion battery. The battery is \n",
            "charged via a USB cable. The vacuum cleaner is equipped with a 2-in-1 brush and a motorized brush. The brush is used \n",
            "to clean hard floors and the motorized brush is used to clean carpets. The vacuum cleaner is equipped with a dust \n",
            "container that can be emptied via a dust container. The vacuum cleaner is equipped with a LED display that shows the \n",
            "remaining battery capacity.\n",
            "Prompt: \n",
            "Create a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless \n",
            "Vacuum Cleaner\n",
            "Response:\n",
            "The same lack of detail and logical flaws in detail where details are available persists. If this fine tuned model is \n",
            "used for product description generation in a real-world scenario, this is not acceptable output.\n",
            "55THE BIG BOOK OF GENERATIVE AI\n",
            "HYPERPARAMETER COMBINATION #3: QLoRA with r=8 and targeting all linear layers\n",
            "Given that doubling r does not seemingly result in any perceivable increase in output quality, it is worth \n",
            "changing the other important knob. i.e., targeting all linear layers instead of just the attention blocks. Here, the \n",
            "LoRA hyperparameters are r=8 and target_layers are  'q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_\n",
            "proj' and 'lm_head'. This increases the number of parameters updated to 12,994,560 and increases the training \n",
            "time to roughly 15.5 minutes.\n",
            "Qualitative Evaluation \n",
            "Prompting the model with the same prompts yield the following:\n",
            "Prompt: \n",
            "Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category:  \n",
            "Optical Mouse\n",
            "Response:\n",
            "Prompt: \n",
            "Create a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless \n",
            "Vacuum Cleaner\n",
            "Response:\n",
            "Now it is possible to see a somewhat longer coherent description of the fictitious optical mouse and there are \n",
            "no logical flaws in the description of the vacuum cleaner. The product descriptions are not only logical, but \n",
            "relevant. Just as a reminder, these relatively high-quality results are obtained by fine-tuning less than a 1% of the \n",
            "model’s weights with a total dataset of 5000 such prompt-description pairs formatted in a consistent manner.\n",
            " \n",
            "1\n",
            "2\n",
            "3\n",
            "The Corelogic Smooth Mouse is a wireless optical mouse with a smooth surface. The mouse is equipped with a 1000 DPI \n",
            "sensor and a 1000 Hz polling rate. The mouse is equipped with a 2.4 GHz wireless connection and a USB receiver. The \n",
            "mouse is available in black and white.\n",
            " \n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "The Hoover Lightspeed cordless vacuum cleaner is a high-performance device that is easy to use. The device is \n",
            "equipped with a lithium-ion battery that provides up to 40 minutes of runtime. The device is equipped with a \n",
            "high-performance motor that provides a suction power of up to 200 watts. The device is equipped with a 2-in-1 dusting \n",
            "brush and a crevice tool. The device is equipped with a LED display that shows the remaining battery charge.\n",
            "56THE BIG BOOK OF GENERATIVE AI\n",
            "HYPERPARAMETER COMBINATION #4: LoRA with  r=8 and targeting all linear transformer layers\n",
            "It is also worth exploring whether the quality of output from the model improves if the pretrained model is \n",
            "frozen in 8-bit instead of 4-bit. In other words, replicating the exact fine-tuning process using LoRA instead of \n",
            "QLoRA. Here, the LoRA hyperparameters are kept the same as before, in the new-found optimal configuration, \n",
            "i.e., r=8 and targeting all linear transformer layers during the adaptation process.\n",
            "Qualitative Evaluation \n",
            "The results for the two prompts used throughout the article are as given below:\n",
            "Prompt: \n",
            "Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category:  \n",
            "Optical Mouse\n",
            "Response:\n",
            "Prompt: \n",
            "Create a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless \n",
            "Vacuum Cleaner\n",
            "Response:\n",
            "Again, there isn’t much of an improvement in the quality of the output text. \n",
            " \n",
            "1\n",
            "2\n",
            "3\n",
            "The Corelogic Smooth Mouse is a high-quality optical mouse with a smooth surface. The mouse is equipped with a 1000 \n",
            "DPI sensor and a 1000 Hz polling rate. The mouse is available in three colours: black, white and red. The mouse is \n",
            "equipped with a USB receiver and is compatible with Windows and Mac.\n",
            " \n",
            "1\n",
            "2\n",
            "3 \n",
            "4\n",
            "The Hoover Lightspeed cordless vacuum cleaner is a compact and lightweight device that is easy to use. The device is \n",
            "equipped with a lithium-ion battery that provides up to 40 minutes of cleaning time. The vacuum cleaner is equipped \n",
            "with a high-performance filter that ensures that the air is cleaned of dust and allergens. The device is equipped \n",
            "with a 2-in-1 dusting brush and a crevice tool that can be used to clean hard-to-reach areas.\n",
            "57THE BIG BOOK OF GENERATIVE AI\n",
            "KEY OBSERVATIONS\n",
            "Based on the above set of trials, and further evidence detailed in the excellent publication presenting QLoRA, \n",
            "it can be deduced that the value of r (the rank of matrices updated during adaptation) does not improve \n",
            "adaptation quality beyond a certain point. The biggest improvement is observed in targeting all linear layers \n",
            "in the adaptation process, as opposed to just the attention blocks, as commonly documented in technical \n",
            "literature detailing LoRA and QLoRA. The trials executed above and other empirical evidence suggest that \n",
            "QLoRA does not indeed suffer from any discernible reduction in quality of text generated, compared to LoRA.\n",
            "FURTHER CONSIDERATIONS FOR USING LORA ADAPTERS IN DEPLOYMENT\n",
            "It's important to optimize the usage of adapters and understand the limitations of the technique. The size of the \n",
            "LoRA adapter obtained through fine-tuning is typically just a few megabytes, while the pretrained base model \n",
            "can be several gigabytes in memory and on disk. During inference, both the adapter and the pretrained LLM \n",
            "need to be loaded, so the memory requirement remains similar.\n",
            "Furthermore, if the weights of the pre-trained LLM and the adapter aren’t merged, there will be a slight increase \n",
            "in inference latency. Fortunately, with the PEFT library, the process of merging the weights with the adapter can \n",
            "be done with a single line of code as shown here:\n",
            "The figure below outlines the process from fine-tuning an adapter to model deployment.\n",
            " \n",
            "1 merged_model = peft_model.merge_and_unload()\n",
            "58THE BIG BOOK OF GENERATIVE AI\n",
            "While the adapter pattern offers significant benefits, merging adapters is not a universal solution. One \n",
            "advantage of the adapter pattern is the ability to deploy a single large pretrained model with task-specific \n",
            "adapters. This allows for efficient inference by utilizing the pretrained model as a backbone for different \n",
            "tasks. However, merging weights makes this approach impossible. The decision to merge weights depends on \n",
            "the specific use case and acceptable inference latency. Nonetheless, LoRA/ QLoRA continues to be a highly \n",
            "effective method for parameter efficient fine-tuning and is widely used.\n",
            "CONCLUSION\n",
            "Low Rank Adaptation is a powerful fine-tuning technique that can yield great results if used with the right \n",
            "configuration. Choosing the correct value of rank and the layers of the neural network architecture to target \n",
            "during adaptation could decide the quality of the output from the fine-tuned model. QLoRA results in further \n",
            "memory savings while preserving the adaptation quality. Even when the fine-tuning is performed,  there are \n",
            "several important engineering considerations to ensure the adapted model is deployed in the correct manner.\n",
            "In summary, a concise table indicating the different combinations of LoRA parameters attempted, text quality \n",
            "output and number of parameters updated when fine-tuning OpenLLaMA-3b-v2 for 3 epochs on 5000 \n",
            "observations on a single A100 is shown below.\n",
            "Try this on Databricks! Clone the GitHub repository associated with the blog into a Databricks Repo to get \n",
            "started. More thoroughly documented examples to fine-tune models on Databricks are available here.\n",
            "R TARGET_MODULES BASE MODEL \n",
            "WEIGHTS QUALITY OF OUTPUT NUMBER OF PARAMETERS UPDATED  \n",
            "(IN MILLIONS)\n",
            "8 Attention blocks 4 low 2.662\n",
            "16 Attention blocks 4 low 5.324\n",
            "8 All linear layers 4 high 12.995\n",
            "8 All linear layers 8 high 12.995\n",
            "59THE BIG BOOK OF GENERATIVE AI\n",
            "Stage 4: Pretraining\n",
            "Pretraining a model from scratch refers to the process of training a language model on a large corpus of data \n",
            "(e.g., text, code) without using any prior knowledge or weights from an existing model. This is in contrast to fine-\n",
            "tuning, where an already pretrained model is further adapted to a specific task or dataset. The output of full \n",
            "pretraining is a base model that can be directly used or further fine-tuned for downstream tasks.\n",
            "WHEN TO USE PRETRAINING\n",
            "Choosing to pretrain an LLM from scratch is a significant commitment, both in terms of data and computational \n",
            "resources. Here are some scenarios where it makes sense:\n",
            "1. Unique data sources: If you possess a unique and extensive corpus of data that is distinct from what \n",
            "available pretrained LLMs have seen, it might be worth pretraining a model to capture this uniqueness\n",
            "2. Domain specificity: Organizations might want a base model tailored to their specific domain (e.g., \n",
            "medical, legal, code) to ensure even the foundational knowledge of the model is domain-specific\n",
            "3. Full control over training data: Pretraining from scratch offers transparency and control over the data \n",
            "the model is trained on. This may be essential for ensuring data security, privacy and custom tailoring of \n",
            "the model’s foundational knowledge.\n",
            "4. Avoiding third-party biases: Pretraining ensures that your LLM application does not inherit biases or \n",
            "limitations from third-party pretrained models.\n",
            "60THE BIG BOOK OF GENERATIVE AI\n",
            "PRETRAINING IN PRACTICE\n",
            "Given the resource-intensive nature of pretraining, careful planning and sophisticated tooling are required. \n",
            "Libraries like PyTorch FSDP and Deepspeed, mentioned in the fine-tuning section, are similarly required for \n",
            "their distributed training capabilities when pretraining an LLM from scratch. The following only scratches the \n",
            "surface on some of the considerations one must take into account when pretraining an LLM: \n",
            " ■ Large-scale data preprocessing: A pretrained model is only as good as the data it is trained on. Thus, \n",
            "it becomes vitally important to ensure robust data preprocessing is conducted prior to model training. \n",
            "Given the scale of the training data involved, this preprocessing typically requires distributed frameworks \n",
            "like Apache Spark™. Consideration must be given to factors such as dataset mix and deduplication \n",
            "techniques to ensure the model is exposed to a wide variety of unique data points.\n",
            " ■ Hyperparameter selection and tuning: Before executing full-scale training of an LLM, determining \n",
            "the set of optimal hyperparameters is crucial. Given the high computational cost associated with LLM \n",
            "training, extensive hyperparameter sweeps are not always feasible. Instead, informed decisions based \n",
            "on smaller-scale searches or prior research are employed. Once a promising set is identified, these \n",
            "hyperparameters are used for the full training run. Tooling like MLflow is essential to manage and track \n",
            "these experiments.\n",
            " ■ Maximizing resource utilization: Given the high costs associated with long-running distributed GPU \n",
            "training jobs, it is hugely important to maximize resource utilization. MosaicML’s composer is an example \n",
            "of a library that uses PyTorch FSDP with additional optimizations to maximize Model FLOPs Utilization \n",
            "(MFU) and Hardware FLOPs Utilization (HFU) during training.\n",
            " ■ Handling GPU failures: Training large models can run for days or even weeks. During such large-scale \n",
            "training for this length of time, hardware failures, especially GPU failures, can (and typically do) occur.  \n",
            "It is essential to have mechanisms in place to handle such failures gracefully. \n",
            " ■ Monitoring and evaluation: Close monitoring of the training process is essential. Saving model \n",
            "checkpoints regularly and evaluating validation sets not only act as safeguards but also provide insights \n",
            "into model performance and convergence trends.\n",
            "In cases where pretraining an LLM from scratch is required, Mosaic AI Training provides a platform to conduct \n",
            "training of multibillion-parameter models in a highly optimized and automated manner. Automatically handling \n",
            "GPU failures and resuming training without human intervention and leveraging Mosaic AI Streaming for efficient \n",
            "streaming of data into the training process are just some of the capabilities provided out of the box.\n",
            "61THE BIG BOOK OF GENERATIVE AI\n",
            "The Value of Training Models From Scratch on Databricks \n",
            "After diving into the details of starting a model’s training from scratch, why you might do it and the advanced \n",
            "tools needed, let’s look at a real-world example to show that training top-notch language models isn’t as \n",
            "complex or expensive as it might seem. This shift highlights that even organizations watching their budget can \n",
            "start training their own models, with Databricks providing the necessary support and infrastructure. Databricks \n",
            "stands out as uniquely capable to help customers train their own models from scratch, enabling them to fully \n",
            "own their AI assets.\n",
            "Pretraining Use Cases\n",
            "Training Stable Diffusion From Scratch for <$50K With MosaicML\n",
            "by Mihir Patel, Cory Stephenson, Landan Seguin, Austin Jacobson and Erica Ji Yuen\n",
            "We’ve replicated Stable Diffusion 2 for less than $50K, and we’ve open sourced the training code so you can \n",
            "too! This is a 3x cost reduction from our last blog post and an 8x reduction from the original Stable Diffusion 2, \n",
            "making training large-scale diffusion models from scratch more accessible than ever before.\n",
            "Today, we are excited to show the results of our own training run: under $50K to train Stable Diffusion 2 base1 \n",
            "from scratch in 7.45 days using the MosaicML platform.\n",
            "62THE BIG BOOK OF GENERATIVE AI\n",
            "Figure 1: Imagining mycelium couture. Integrating image generation into the design process pushes creative boundaries. All images in this mood board \n",
            "were created with our internal diffusion model trained from scratch on the MosaicML Platform.\n",
            "Training your own image generation model on your own data is now easy and accessible. By training your own \n",
            "diffusion models, you can:\n",
            " ■ Use your proprietary data\n",
            " ■ Tune the representations for certain art or photography styles\n",
            " ■ Avoid violating intellectual property laws so your models can be used commercially\n",
            "We’ve open sourced our code and methods to train a diffusion model from scratch so that you can train your \n",
            "own; check it out here! If you're interested in training your own models, contact us for a demo, and read on to \n",
            "learn more about our engineering setup!\n",
            "63THE BIG BOOK OF GENERATIVE AI\n",
            "SETUP\n",
            "Model: Our diffusion model is a ComposerModel \n",
            "composed of a Variational Autoencoder (VAE), a \n",
            "CLIP model, a U-Net, and a diffusion noise scheduler, \n",
            "all from the HuggingFace's Diffusers library. All of \n",
            "the model configurations were based on stabilityai/\n",
            "stable-diffusion-2-base.\n",
            "Figure 2: Getting creative and embracing serendipity. A variety of subjects, art, and photography styles are generated by our diffusion model.\n",
            "Figure 3: Simplified diagram of the diffusion model.\n",
            "64THE BIG BOOK OF GENERATIVE AI\n",
            "Data: We trained on a subset of LAION-5B that includes samples with English-only captions and an aesthetic \n",
            "score of 4.5+. Similar to Stable Diffusion 2 base, we did two phases of training based on the image resolution of \n",
            "the training data. For the first phase of training, we used all images with resolution >=256x256, amounting to 790 \n",
            "million image-caption samples. For the second phase of training, we only used images with resolution >=512x512, \n",
            "amounting to 300 million image-caption samples.\n",
            "Compute: Both phases of training ran on 128 NVIDIA A100 GPUs. The first training phase was run for 550k \n",
            "iterations in 1.6 days while the second phase was run for 850k iterations in 4.9 days, for a total of 20,051 A100 \n",
            "hours for training. In addition to the training time, we pre-computed the latents for the VAE and CLIP model \n",
            "to reduce training time and cost when making multiple passes over the dataset. Pre-computing the latents \n",
            "required an additional 3,784 A100 hours, resulting in 23,835 A100 hours in total. Assuming a cost of $2 / A100 \n",
            "hour, the total price tag is $47.7k.\n",
            "Tech Stack: We used Composer for our training framework, StreamingDataset to load our 100TB of data, and \n",
            "the MosaicML platform for overcoming infrastructure challenges when training and evaluating on 128 GPUs.\n",
            "Figure 4: Loss curve for our training run. Our platform caught two hardware failures and automatically restarted the run with no human intervention. \n",
            "The loss discontinuity is because phase 2 increases the resolution from 256x256 to 512x512.\n",
            "65THE BIG BOOK OF GENERATIVE AI\n",
            "CHALLENGES AND SOLUTIONS\n",
            "Whether for diffusion models or large language models, training at scale has significant challenges. We trained \n",
            "our diffusion model using the MosaicML platform, which addresses these challenges automatically so you can \n",
            "focus on training the best possible model. Below are three main challenges with large-scale training and how our \n",
            "platform solves them.\n",
            "INFRASTRUCTURE\n",
            "Training large models on large datasets requires significant compute. The MosaicML platform effortlessly \n",
            "orchestrates hundreds of GPUs on any cloud provider. For example, our primary training run took place on \n",
            "a cluster of 128 A100 GPUs. To ensure evaluating the model didn't slow training, we automatically kicked off \n",
            "evaluation runs at every checkpoint on different clusters using different cloud providers, seamlessly scaling up \n",
            "to 64 GPUs and back down to 8 GPUs depending on availability.\n",
            "Even after training is underway, software or hardware failures can halt training, leaving GPUs idle until someone \n",
            "notices or requiring someone on-call 24/7 to babysit the run. Thankfully, the Node Doctor and Watchdog \n",
            "features of the MosaicML platform automatically detect failed nodes and resume jobs as needed. With auto-\n",
            "resumption, we recover from failures and continue training with zero human intervention, avoiding expensive \n",
            "downtime and human babysitting. Just launch and train!\n",
            "EFFICIENT SOFTWARE\n",
            "Software is difficult to configure optimally. Our PyTorch-based Composer library maximizes training efficiency \n",
            "at scale. As shown in our previous blog post, Composer demonstrated excellent throughput scaling as the \n",
            "number of GPUs increased. For this update, we added further optimizations (Low Precision GroupNorm and Low \n",
            "Precision LayerNorm, Fully Sharded Data Parallel) to achieve near-perfect strong scaling up to 128 GPUs, bringing \n",
            "the cost down to $50k. We also used Composer's native Exponential Moving Average (EMA) algorithm, which \n",
            "allowed us to start EMA close to the end of training (iteration 800k of the final phase) to gain all the benefits of \n",
            "EMA while saving on memory and compute for the majority of training.\n",
            "66THE BIG BOOK OF GENERATIVE AI\n",
            "MANAGING 100TB OF DATA\n",
            "We trained with a subset of LAION-5B that contained 790 million samples, amounting to >100TB of data. The \n",
            "sheer size of the dataset makes it difficult to manage, especially when working with multiple clusters with \n",
            "separate local storage. The MosaicML StreamingDataset library makes working with massive datasets much \n",
            "simpler and faster. There were three key features of the StreamingDataset library that were especially useful for \n",
            "this training run:\n",
            "1. Mixing datasets stored in different locations. We bucketed samples based on image resolution into \n",
            "different datasets. At training time, we used the MosaicML StreamingDataset library to train on a mixture \n",
            "of resolutions from these datasets.\n",
            "2. Instant mid-epoch resumption. We were able to instantly resume training in the middle of an epoch. This \n",
            "saved hours by avoiding the need to iterate over the entire dataset to get back to where we left off.\n",
            "3. Elastic determinism. The MosaicML StreamingDataset library deterministically shuffles data, even when \n",
            "changing the number of GPUs used for training. This made it possible for us to exactly reproduce training \n",
            "runs, dramatically simplifying debugging.\n",
            "HUMAN EVALUATION RESUL TS\n",
            "Evaluating image generation models is difficult, and there is no substitute for human evaluation. In a blind human \n",
            "evaluation, we measured user preferences in image quality and prompt alignment between Stable Diffusion 2 \n",
            "and our diffusion model. Based on user preferences, we concluded that the two models were comparable in \n",
            "quality (see Figure 5) All images were generated based on prompts from the Drawbench benchmark proposed \n",
            "in the Imagen paper. For more details, see our follow-up blog post coming soon.\n",
            "67THE BIG BOOK OF GENERATIVE AI\n",
            "Figure 5: Results from our human evaluation of image quality (left) and prompt alignment (right). Error bars show 95% confidence intervals. In both ex-\n",
            "periments, the difference in user preference rates between the two models was comparable to the uncertainty in the measurement, so we conclude \n",
            "that the two models are of comparable overall quality.\n",
            "Deep Dive: How We Trained Stable Diffusion for Less Than $50K \n",
            "by Mihir Patel, Erica Ji Yuen, Cory Stephenson and Landan Seguin\n",
            "In our previous example, we showed how we used the MosaicML platform, Streaming datasets, and the \n",
            "Composer library to train a Stable Diffusion model from scratch for less than $50,000. Now, we do a deep dive \n",
            "into the technical details behind this speedup, demonstrating how we were able to replicate the Stable Diffusion \n",
            "2 base model in just 6.8 days.\n",
            "Try out our code here!\n",
            "Many organizations require high-performing large AI models tailored to their specific use cases. However, \n",
            "training such models is often prohibitively time-consuming and expensive, requiring vast amounts of \n",
            "computation and expertise. This is where MosaicML comes in: we provide a comprehensive solution that \n",
            "simplifies and accelerates the process of training these models.\n",
            "68THE BIG BOOK OF GENERATIVE AI\n",
            "In our previous blog post, we announced that we have trained a diffusion model comparable to Stable Diffusion \n",
            "2 from scratch for $47.7K. In this post, we dive into the technical details to highlight how we achieved an 8x \n",
            "speedup/cost reduction from the number reported by StabilityAI and a 3x cost reduction over our own \n",
            "baseline. All our code is open source and easy to modify for custom use cases. If you're interested in learning \n",
            "more about our stack, please contact us for a demo.\n",
            "ACCELERATING TRAINING\n",
            "We’ve introduced a variety of techniques, from fusions to sharding strategies, that dramatically speed up \n",
            "training and lower costs by almost 3x.\n",
            "Figure 1: Stable Diffusion 2 model architecture. For training, the VAE image encoder, CLIP text encoder and U-Net are used. For inference,  \n",
            "the CLIP Text Encoder, U-Net, and VAE image decoder are used. Only the U-Net weights are updated during training; CLIP and VAE are fixed.\n",
            "69THE BIG BOOK OF GENERATIVE AI\n",
            "XFORMERS FLASHATTENTION\n",
            "Figure 2: xFormers accelerates cross attention blocks in the U-Net.\n",
            "70THE BIG BOOK OF GENERATIVE AI\n",
            "The attention layers in the Stable Diffusion architecture can be slow with a naive implementation, so most \n",
            "codebases use faster implementations that rely on fused kernels. In our stack, we leverage xFormers \n",
            "FlashAttention.\n",
            "While this was enabled in our original blog post, we found an issue with the usage that resulted in extra memory \n",
            "being consumed on rank 0. After fixing this bug, we were able to increase our device microbatch size1 from 4 to \n",
            "8. This yielded a sizable speedup, since A100s are more efficient at larger matrix sizes.\n",
            "PRECOMPUTING LATENTS\n",
            "Figure 3: Two phase training with precomputed latents. \n",
            "First, all VAE and CLIP latents are precomputed and stored. \n",
            "Then, the U-Net diffusion model is trained using these \n",
            "precomputed latents.\n",
            "71THE BIG BOOK OF GENERATIVE AI\n",
            "Stable Diffusion is a combination of three models: a variational autoencoder (VAE), a text encoder (CLIP), and a \n",
            "U-Net. During diffusion training, only the U-Net is trained, and the other two models are used to compute the \n",
            "latent encodings of the image and text inputs. Standard training involves computing the VAE and CLIP latents for \n",
            "every batch, but this does a lot of duplicate work when training for multiple epochs: latents are re-computed for \n",
            "each image every time it is used. Instead, we precompute the latents once before training. Empirically, we have 2 \n",
            "epochs at 256 resolution and 5 epochs at 512 resolution, so we avoid 6 extra VAE and CLIP calls per image-text \n",
            "pair in the dataset.\n",
            "Additionally, when pre-computing the latents, we can lower the precision of the VAE and CLIP models to \n",
            "fp16. This could lead to numerical instability if we were training the VAE and CLIP and used this precision for \n",
            "the backward pass. However, since we're only using them for inference, we can safely lower the precision, \n",
            "which increases speed. The extra memory savings also let us use far larger batch sizes and improve hardware \n",
            "utilization during the latent precomputation.\n",
            "72THE BIG BOOK OF GENERATIVE AI\n",
            "LOW PRECISION LAYERNORM AND GROUPNORM\n",
            "Figure 4: Low Precision LayerNorm and Low Precision GroupNorm. Low precision gives faster training and lower memory usage, enabling larger \n",
            "microbatches.\n",
            "73THE BIG BOOK OF GENERATIVE AI\n",
            "Diffusion training is done in automatic mixed precision by default. This uses half precision (fp16) in most \n",
            "layers, but fp32 in a few numerically unstable layers like normalization and softmax. The Stable Diffusion U-Net \n",
            "architecture uses several LayerNorm and GroupNorm layers, which by default are run in fp32.\n",
            "Motivated by our finding that half precision LayerNorms are safe to use in language models, we decided to \n",
            "try out half precision LayerNorm and GroupNorm layers. This change resulted in identical loss curves and no \n",
            "instability in our experiments.\n",
            "While we did observe some throughput improvement, the real benefit was decreased memory usage. Now, \n",
            "along with removing the VAE and CLIP memory by precomputing latents, we have enough space on our 40GB \n",
            "A100 to increase our microbatch size from 8 to 16, 4x larger than what we started with!\n",
            "74THE BIG BOOK OF GENERATIVE AI\n",
            "FULLY SHARDED DATA PARALLELISM\n",
            "Figure 5: Fully Sharded Data Parallel with SHARD_GRAD_OP speeds up the gradient update step and enables linear scaling.\n",
            "75THE BIG BOOK OF GENERATIVE AI\n",
            "MosaicML Composer, our go-to training library, includes support for PyTorch Fully Sharded Data Parallelism \n",
            "(FSDP). We primarily use this to shard large scale models like 10B+ parameter LLMs that don't fit in a single \n",
            "device across hundreds of GPUs for incredibly fast training. Stable Diffusion doesn't require sharding since it  \n",
            "fits in a single GPU. However, some of the distributed features in FSDP are still useful for speeding up training  \n",
            "on a large number of GPUs.\n",
            "When batches don’t fit into memory, we do several forward and backward passes on smaller microbatches, \n",
            "followed by a single gradient update. If we use a small number of GPUs to train, we have far more forward and \n",
            "backward passes per gradient update, so the time spent on the gradient update doesn't matter. However, at \n",
            "128+ GPUs with a microbatch size of 16, we're only doing one forward and one backward pass for each gradient \n",
            "update. At this scale, the gradient update step starts to become a significant bottleneck.\n",
            "To tackle this problem, we use FSDP's SHARD_GRAD_OP mode. In normal training, each GPU communicates all \n",
            "its gradients to every other GPU, and then each GPU updates its local copy of the model. With this FSDP variant, \n",
            "each GPU only gets the gradients and updates the weights for a small part of the model before sending the \n",
            "updated weights for that part of the model to all of the other GPUs. By dividing the update step across all the \n",
            "GPUs, we can ensure the amount of work per GPU decreases as we increase the number of GPUs, helping us \n",
            "achieve linear scaling.\n",
            "76THE BIG BOOK OF GENERATIVE AI\n",
            "SCHEDULED EMA\n",
            "Figure 6: Loss curve of our training run with the scheduled exponential moving average (EMA) period highlighted.\n",
            "77THE BIG BOOK OF GENERATIVE AI\n",
            "Stable Diffusion 2 uses Exponential Moving Averaging (EMA), which maintains an exponential moving average \n",
            "of the weights. At every time step, the EMA model is updated by taking 0.9999 times the current EMA model \n",
            "plus 0.0001 times the new weights after the latest forward and backward pass. By default, the EMA algorithm is \n",
            "applied after every gradient update for the entire training period. However, this can be slow due to the memory \n",
            "operations required to read and write all the weights at every step.\n",
            "To avoid this costly procedure, we start with a key observation: since the old weights are decayed by a factor of \n",
            "0.9999 at every batch, the early iterations of training only contribute minimally to the final average. This means \n",
            "we only need to take the exponential moving average of the final few steps. Concretely, we train for 1,400,000 \n",
            "batches and only apply EMA for the final 50,000 steps, which is about 3.5% of the training period. The weights \n",
            "from the first 1,350,000 iterations decay away by (0.9999)^50000, so their aggregate contribution would have \n",
            "a weight of less than 1% in the final model. Using this technique, we can avoid adding overhead for 96.5% of \n",
            "training and still achieve a nearly equivalent EMA model.\n",
            "78THE BIG BOOK OF GENERATIVE AI\n",
            "FINAL TIME AND COST ESTIMATES\n",
            "Figure 7: Throughput at 512x512 images on 128 GPUs as each speedup optimization is enabled. We achieve a total cumulative speedup of 2.71x over \n",
            "the baseline.\n",
            "We’ve shown how we obtained nearly a 3x reduction in time and cost to train Stable Diffusion compared to our \n",
            "original results. With xFormers, precomputed latents, low precision LayerNorm, low precision GroupNorm, FSDP, \n",
            "and scheduled EMA, Table 1 shows it's possible to train Stable Diffusion in just 6.79 days using 21,000 A100-\n",
            "hours for a total cost of less than $42,000. We estimated these times and costs by measuring throughput for \n",
            "training 1.1 billion 256x256 images and 1.7 billion 512x512 images with a max tokenized length of 77 at a global \n",
            "batch size of 2048, as detailed in the Stable Diffusion 2 base model card. This is slightly cheaper than our \n",
            "previously reported run with a cost of $47.7k as it does not account for any time spent on evaluation or restarts \n",
            "due to hardware failures.\n",
            "79THE BIG BOOK OF GENERATIVE AI\n",
            "NUMBER  \n",
            "OF A100S\n",
            "THROUGHPUT  \n",
            "FOR U-NET \n",
            "@ 256X256 \n",
            "(IMAGES / \n",
            "SECOND)\n",
            "THROUGHPUT \n",
            "FOR U-NET \n",
            "@ 512X512 \n",
            "(IMAGES / \n",
            "SECOND)\n",
            "THROUGHPUT \n",
            "FOR U-NET @ \n",
            "512X512 WITH \n",
            "EMA (IMAGES /  \n",
            "SECOND)\n",
            "DAYS TO TRAIN \n",
            "ON MOSAICML \n",
            "CLOUD\n",
            "APPROX. COST \n",
            "ON MOSAICML \n",
            "CLOUD\n",
            "8 1100 290 290 101.04 $38,800\n",
            "16 2180 585 580 50.29 $38,630\n",
            "32 4080 1195 1160 25.01 $38,420\n",
            "64 8530 2340 2220 12.63 $38,800\n",
            "128 11600 4590 3927 6.79 $41,710\n",
            "Table 1: Estimated time and cost to train a Stable Diffusion model on 1.1 billion images at 256x256 resolution, followed by 1.7 billion images at 512x512 \n",
            "resolution. Different rows show different numbers of NVIDIA 40GB A100 GPUs at a global batch size of 2048.\n",
            "These optimizations show that training image generation models from scratch is within reach for everyone. For \n",
            "updates on our latest work, join our Community Slack or follow us on Twitter. If your organization wants to start \n",
            "training diffusion models today, please schedule a demo online or email us at demo@mosaicml.com.\n",
            "1 When training large models with big batches that don't fit in memory in a single pass, each batch is divided into smaller microbatches. On each \n",
            "device, we can do a forward and backward pass for each microbatch and sum the gradients at the end to compute a gradient update equivalent to \n",
            "a single forward and backward pass with the entire batch all at once.\n",
            "80THE BIG BOOK OF GENERATIVE AI\n",
            "Stage 5: LLM Evaluation\n",
            "Constant evaluation and monitoring of deployed large language models (LLMs) and generative AI applications \n",
            "are crucial due to the dynamic nature of both the data they interact with and the environments in which they \n",
            "operate. These systems learn from vast datasets and can evolve over time, potentially leading to shifts in \n",
            "performance, accuracy or even the emergence of biases. Continuous monitoring ensures that any deviation \n",
            "from expected behavior can be detected and corrected promptly, maintaining the integrity and reliability \n",
            "of the AI application. As user needs and societal norms change, ongoing evaluation allows these models to \n",
            "adapt, ensuring their outputs remain relevant, appropriate and effective. This vigilance not only mitigates risks \n",
            "associated with AI deployments, such as ethical concerns and regulatory compliance, but also maximizes the \n",
            "value and utility these technologies bring to organizations and end users. \n",
            "Evaluating LLMs is a challenging and evolving domain, primarily because LLMs often demonstrate uneven \n",
            "capabilities across different tasks. An LLM might excel in one benchmark, but slight variations in the prompt \n",
            "or problem can drastically affect its performance. The dynamic nature of LLMs and their vast potential \n",
            "applications only amplify the challenge of establishing comprehensive evaluation standards.\n",
            "81THE BIG BOOK OF GENERATIVE AI\n",
            "Present challenges involved with evaluating LLM-powered applications include the following:\n",
            " ■ Variable performance: LLMs can be sensitive to prompt variations, demonstrating high proficiency in \n",
            "one task but faltering with slight deviations in prompts.\n",
            " ■ Lack of ground truth: Since most LLMs output natural language, it is very difficult to evaluate the outputs \n",
            "via traditional NLP metrics (BLEU, ROUGE, etc.). For example, suppose an LLM were used to summarize \n",
            "a news article. Two equally good summaries might have almost completely different words and word \n",
            "orders, so even defining a “ground truth” label becomes difficult or impossible.\n",
            " ■ Domain-specific evaluation: For domain-specific fine-tuned LLMs, popular generic benchmarks \n",
            "may not capture their nuanced capabilities. Such models are tailored for specialized tasks, making \n",
            "traditional metrics less relevant. This divergence often necessitates the development of domain-specific \n",
            "benchmarks and evaluation criteria. See the example of Replit’s code generation LLM. \n",
            " ■ Reliance on human judgment: It is often the case that LLM performance is being evaluated in domains \n",
            "where text is scarce or there is a reliance on subject matter expert knowledge. In such scenarios, \n",
            "evaluating LLM output can be costly and time-consuming.\n",
            "To help give examples of how this can be accomplished, here are two great examples of how you can monitor \n",
            "and evaluate your deployed LLMs and generative AI applications using Databricks.\n",
            "LLM Evaluation Examples\n",
            "Best Practices for LLM Evaluation of RAG Applications  \n",
            "A Case Study on the Databricks Documentation Bot\n",
            "by Quinn Leng, Kasey Uhlenhuth and Alkis Polyzotis\n",
            "Chatbots are the most widely adopted use case for leveraging the powerful chat and reasoning capabilities \n",
            "of large language models (LLM). The retrieval augmented generation (RAG) architecture is quickly becoming \n",
            "the industry standard for developing chatbots because it combines the benefits of a knowledge base (via a \n",
            "vector store) and generative models (e.g., GPT-3.5 and GPT-4) to reduce hallucinations, maintain up-to-date \n",
            "information, and leverage domain-specific knowledge. However, evaluating the quality of chatbot responses \n",
            "remains an unsolved problem today. With no industry standards defined, organizations resort to human grading \n",
            "(labeling) –which is time-consuming and hard to scale.\n",
            "82THE BIG BOOK OF GENERATIVE AI\n",
            "We applied theory to practice to help form best practices for LLM automated evaluation so you can deploy RAG \n",
            "applications to production quickly and with confidence. This blog represents the first in a series of investigations \n",
            "we’re running at Databricks to provide learnings on LLM evaluation. All research in this post was conducted by \n",
            "Quinn Leng, Senior Software Engineer at Databricks and creator of the Databricks Documentation AI Assistant. \n",
            "CHALLENGES WITH AUTO-EVALUATION IN PRACTICE\n",
            "Recently, the LLM community has been exploring the use of “LLMs as a judge” for automated evaluation with \n",
            "many using powerful LLMs such as GPT-4 to do the evaluation for their LLM outputs. The lmsys group’s research \n",
            "paper explores the feasibility and pros/cons of using various LLMs (GPT-4, ClaudeV1, GPT-3.5) as the judge for \n",
            "tasks in writing, math, and world knowledge.\n",
            "Despite all this great research, there are still many unanswered questions about how to apply LLM judges  \n",
            "in practice:\n",
            " ■ Alignment With Human Grading: Specifically for a document-Q&A chatbot, how well does an \n",
            "LLM judge’s grading reflect the actual human preference in terms of correctness, readability and \n",
            "comprehensiveness of the answers? \n",
            " ■ Accuracy Through Examples: What’s the effectiveness of providing a few grading examples to the LLM \n",
            "judge and how much does it increase the reliability and reusability of the LLM judge on different metrics?\n",
            " ■ Appropriate Grade Scales: What grading scale is recommended because different grading scales are \n",
            "used by different frameworks (e.g., AzureML uses 0 to 100 whereas langchain uses binary scales)?\n",
            " ■ Applicability Across Use Cases: With the same evaluation metric (e.g. correctness), to what extent can \n",
            "the evaluation metric be reused across different use cases (e.g. casual chat, content summarization, \n",
            "retrieval augmented generation)?\n",
            "83THE BIG BOOK OF GENERATIVE AI\n",
            "APPLYING EFFECTIVE AUTO-EVALUATION FOR RAG APPLICATIONS\n",
            "We explored the possible options for the questions outlined above in the context of our own chatbot \n",
            "application at Databricks. We believe that our findings generalize and can thus help your team effectively \n",
            "evaluate RAG-based chatbots at a lower cost and faster speed:\n",
            " ■ LLM-as-a-judge agrees with human grading on over 80% of judgments. Using LLMs-as-a-judge for our \n",
            "document-based chatbot evaluation was as effective as human judges, matching the exact score in over \n",
            "80% of judgments and being within a 1-score distance (using a scale of 0-3) in over 95% of judgments.\n",
            " ■ Save costs by using GPT-3.5 with examples. GPT-3.5 can be used as an LLM judge if you provide \n",
            "examples for each grading score. Because of the context size limit it’s only practical to use a low-\n",
            "precision grading scale. Using GPT-3.5 with examples instead of GPT-4 drives down the cost of LLM judge \n",
            "by 10x and improves the speed by more than 3x.\n",
            " ■ Use low-precision grading scales for easier interpretation. We found lower-precision grading scores like 0, \n",
            "1, 2, 3 or even binary (0, 1) can largely retain precision compared to higher precision scales like 0 to 10.0 or \n",
            "0 to 100.0, while making it considerably easier to provide grading rubrics to both human annotators and \n",
            "LLM judges. Using a lower precision scale also allows consistency of grading scales among different LLM \n",
            "judges (e.g., between GPT-4 and claude2).\n",
            " ■ RAG applications require their own benchmarks. A model might have good performance on a published \n",
            "specialized benchmark (e.g. casual chat, math, or creative writing) but that doesn’t guarantee good \n",
            "performance on other tasks (e.g. answering questions from a given context). Benchmarks should only be \n",
            "used if the use case matches, i.e., a RAG application should only be evaluated with a RAG benchmark.\n",
            "Based on our research, we recommend the following procedure when using an LLM judge: \n",
            " ■ Use a 1-5 grading scale\n",
            " ■ Use GPT-4 as an LLM judge with no examples to understand grading rules\n",
            " ■ Switch your LLM judge to GPT-3.5 with one example per score\n",
            "84THE BIG BOOK OF GENERATIVE AI\n",
            "OUR METHODOLOGY FOR ESTABLISHING BEST PRACTICES\n",
            "The remainder of this post will walk through the series of experiments we conducted to form these  \n",
            "best practices. \n",
            "EXPERIMENT SETUP\n",
            "85THE BIG BOOK OF GENERATIVE AI\n",
            " The experiment had three steps: \n",
            "1. Generate evaluation dataset: We created a dataset from 100 questions and context from Databricks \n",
            "documents. The context represents (chunks of) documents that are relevant to the question. \n",
            "2. Generate answer sheets: Using the evaluation dataset, we prompted different language models to \n",
            "generate answers and stored the question-context-answer pairs in a dataset called “answer sheets”. In \n",
            "this investigation, we used GPT-4, GPT-3.5, Claude-v1, Llama2-70b-chat, Vicuna-33b, and mpt-30b-chat.\n",
            "3. Generate grades: Given the answer sheets, we used various LLMs to generate grades and reasoning \n",
            "for the grades. The grades are a composite score of Correctness (weighted: 60%), Comprehensiveness \n",
            "(weighted: 20%) and Readability (weighted: 20%). We chose this weighting scheme to reflect our \n",
            "preference for Correctness in the generated answers. Other applications may tune these weights \n",
            "differently but we expect Correctness to remain a dominant factor.\n",
            "86THE BIG BOOK OF GENERATIVE AI\n",
            "Additionally, the following techniques were used to avoid positional bias and improve reliability:\n",
            " ■ Low temperature (temperature 0.1) to ensure reproducibility\n",
            " ■ Single-answer grading instead of pairwise comparison\n",
            " ■ Chain of thoughts to let the LLM reason about the grading process before giving the final score\n",
            " ■ Few-shots generation where the LLM is provided with several examples in the grading rubric for each \n",
            "score value on each factor (Correctness, Comprehensiveness, Readability)\n",
            "EXPERIMENT 1: ALIGNMENT WITH HUMAN GRADING\n",
            "To confirm the level of agreement between human annotators and LLM judges, we sent answer sheets  \n",
            "(grading scale 0-3) from gpt-3.5-turbo and vicuna-33b to a labeling company to collect human labels,  \n",
            "and then compared the result with GPT-4’s grading output. Below are the findings:\n",
            "Human and GPT-4 judges can reach above 80% agreement on the correctness and readability score.  \n",
            "And if we lower the requirement to be smaller or equal than 1 score difference, the agreement level can  \n",
            "reach above 95%. The Comprehensiveness metric has less alignment, which matches what we’ve heard  \n",
            "from business stakeholders who shared that “comprehensive” seems more subjective than metrics like \n",
            "Correctness or Readability.\n",
            "87THE BIG BOOK OF GENERATIVE AI\n",
            "EXPERIMENT 2: ACCURACY THROUGH EXAMPLES\n",
            "The lmsys paper uses this prompt to instruct the LLM judge to evaluate based on the helpfulness, relevance, \n",
            "accuracy, depth, creativity, and level of detail of the response. However, the paper doesn’t share specifics on the \n",
            "grading rubric. From our research, we found many factors can significantly affect the final score, for example:\n",
            " ■ The importance of different factors: Helpfulness, Relevance, Accuracy, Depth, Creativity\n",
            " ■ The interpretation of factors like Helpfulness is ambiguous \n",
            " ■ If different factors conflict with each other, where an answer is helpful but is not accurate \n",
            "We developed a rubric for instructing an LLM judge for a given grading scale, by trying the following:\n",
            "1. Original Prompt: Here is the original prompt used in the lmsys paper:\n",
            "We adapted the original lmsys paper prompt to emit our metrics about correctness, comprehensiveness and \n",
            "readability, and also prompt the judge to provide one line justification before giving each score (to benefit from \n",
            "chain-of-thought reasoning). Below are the zero-shot version of the prompt which doesn’t provide any example, \n",
            "and the few-shot version of the prompt which provides one example for each score. Then we used the same \n",
            "answer sheets as input and compared the graded results from the two prompt types.\n",
            "2. Zero Shot Learning: Require the LLM judge to emit our metrics about correctness, comprehensiveness \n",
            "and readability, and also prompt the judge to provide one line justification for each score.\n",
            " \n",
            "Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question \n",
            "displayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and \n",
            "level of detail of the response. Begin your evaluation by providing a short explanation. Be as objective as possible. After \n",
            "providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format\n",
            " \n",
            "Please act as an impartial judge and evaluate the quality of the provided answer which attempts to answer the provided \n",
            "question based on a provided context. \n",
            "  You'll be given a function grading_function which you'll call for each provided context, question and answer to submit your \n",
            "reasoning and score for the correctness, comprehensiveness and readability of the answer\n",
            "88THE BIG BOOK OF GENERATIVE AI\n",
            "3. Few Shots Learning: We adapted the zero shot prompt to provide explicit examples for each score in the \n",
            "scale. The new prompt:\n",
            " \n",
            "Please act as an impartial judge and evaluate the quality of the provided answer which attempts to answer the provided \n",
            "question based on a provided context.\n",
            "  You'll be given a function grading_function which you'll call for each provided context, question and answer to submit your \n",
            "reasoning and score for the correctness, comprehensiveness and readability of the answer. \n",
            "  \n",
            "  Below is your grading rubric: \n",
            "- Correctness: If the answer correctly answer the question, below are the details for different scores:\n",
            "  - Score 0: the answer is completely incorrect, doesn’t mention anything about the question or is completely contrary to the \n",
            "correct answer.\n",
            "      - For example, when asked “How to terminate a databricks cluster”, the answer is empty string, or content that’s \n",
            "completely irrelevant, or sorry I don’t know the answer.\n",
            "  - Score 1: the answer provides some relevance to the question and answers one aspect of the question correctly.\n",
            "      - Example:\n",
            "          - Question: How to terminate a databricks cluster\n",
            "          - Answer: Databricks cluster is a cloud-based computing environment that allows users to process big data and run \n",
            "distributed data processing tasks efficiently.\n",
            "          - Or answer:  In the Databricks workspace, navigate to the \"Clusters\" tab. And then this is a hard question that I \n",
            "need to think more about it\n",
            "  - Score 2: the answer mostly answer the question but is missing or hallucinating on one critical aspect.\n",
            "      - Example:\n",
            "          - Question: How to terminate a databricks cluster”\n",
            "          - Answer: “In the Databricks workspace, navigate to the \"Clusters\" tab.\n",
            "          Find the cluster you want to terminate from the list of active clusters.\n",
            "          And then you’ll find a button to terminate all clusters at once”\n",
            "  - Score 3: the answer correctly answer the question and not missing any major aspect\n",
            "      - Example:\n",
            "          - Question: How to terminate a databricks cluster\n",
            "          - Answer: In the Databricks workspace, navigate to the \"Clusters\" tab.\n",
            "          Find the cluster you want to terminate from the list of active clusters.\n",
            "          Click on the down-arrow next to the cluster name to open the cluster details.\n",
            "          Click on the \"Terminate\" button. A confirmation dialog will appear. Click \"Terminate\" again to confirm the action.”\n",
            "- Comprehensiveness: How comprehensive is the answer, does it fully answer all aspects of the question and provide \n",
            "comprehensive explanation and other necessary information. Below are the details for different scores:\n",
            "  - Score 0: typically if the answer is completely incorrect, then the comprehensiveness is also zero score.\n",
            "  - Score 1: if the answer is correct but too short to fully answer the question, then we can give score 1 for \n",
            "comprehensiveness.\n",
            "      - Example:\n",
            "          - Question: How to use databricks API to create a cluster?\n",
            "          - Answer: First, you will need a Databricks access token with the appropriate permissions. You can generate this \n",
            "token through the Databricks UI under the 'User Settings' option. And then (the rest is missing)\n",
            "  - Score 2: the answer is correct and roughly answer the main aspects of the question, but it’s missing description about \n",
            "details. Or is completely missing details about one minor aspect.  \n",
            "89THE BIG BOOK OF GENERATIVE AI\n",
            "      - Example:\n",
            "          - Question: How to use databricks API to create a cluster?\n",
            "          - Answer: You will need a Databricks access token with the appropriate permissions. Then you’ll need to set up the \n",
            "request URL, then you can make the HTTP Request. Then you can handle the request response.\n",
            "      - Example:\n",
            "          - Question: How to use databricks API to create a cluster?\n",
            "          - Answer: You will need a Databricks access token with the appropriate permissions. Then you’ll need to set up the \n",
            "request URL, then you can make the HTTP Request. Then you can handle the request response.\n",
            "  - Score 3: the answer is correct, and covers all the main aspects of the question\n",
            "- Readability: How readable is the answer, does it have redundant information or incomplete information that hurts the \n",
            "readability of the answer.\n",
            "  - Score 0: the answer is completely unreadable, e.g. fully of symbols that’s hard to read; e.g. keeps repeating the words \n",
            "that it’s very hard to understand the meaning of the paragraph. No meaningful information can be extracted from the answer.\n",
            "  - Score 1: the answer is slightly readable, there are irrelevant symbols or repeated words, but it can roughly form a \n",
            "meaningful sentence that cover some aspects of the answer.\n",
            "      - Example:\n",
            "          - Question: How to use databricks API to create a cluster?\n",
            "          - Answer: You you  you  you  you  you  will need a Databricks access token with the appropriate permissions. And \n",
            "then then you’ll need to set up the request URL, then you can make the HTTP Request. Then Then Then Then Then Then Then Then \n",
            "Then\n",
            "  - Score 2: the answer is correct and mostly readable, but there is one obvious piece that’s affecting the readability \n",
            "(mentioning of irrelevant pieces, repeated words)\n",
            "      - Example:\n",
            "          - Question: How to terminate a databricks cluster\n",
            "          - Answer: In the Databricks workspace, navigate to the \"Clusters\" tab.\n",
            "          Find the cluster you want to terminate from the list of active clusters.\n",
            "          Click on the down-arrow next to the cluster name to open the cluster details.\n",
            "          Click on the \"Terminate\" button…………………………………..\n",
            "          A confirmation dialog will appear. Click \"Terminate\" again to confirm the action.\n",
            "  - Score 3: the answer is correct and reader friendly, no obvious piece that affect readability.\n",
            "- Then final rating:\n",
            "    - Ratio: 60% correctness + 20% comprehensiveness + 20% readability\n",
            "From this experiment, we learned several things:\n",
            " ■ Using the Few Shots prompt with GPT-4 didn’t make an obvious difference in the consistency of results. \n",
            "When we included the detailed grading rubric with examples we didn’t see a noticeable improvement in \n",
            "GPT-4’s grading results across different LLM models. Interestingly, it caused a slight variance in the range \n",
            "of the scores. \n",
            "90THE BIG BOOK OF GENERATIVE AI\n",
            "91THE BIG BOOK OF GENERATIVE AI\n",
            " ■ Including few examples for GPT-3.5-turbo-16k significantly improves the consistency of the scores,  \n",
            "and makes the result usable. Including detailed grading rubric/examples has very obvious improvement \n",
            "on the grading result from GPT-3.5. Though the actual average score value is slightly different between \n",
            "GPT-4 and GPT-3.5 (score 3.0 vs score 2.6), the ranking and precision remains fairly consistent\n",
            " ■ On the contrary, using GPT-3.5 without a grading rubric gets very inconsistent results and is  \n",
            "completely unusable\n",
            " ■ Note that we are using GPT-3.5-turbo-16k instead of GPT-3.5-turbo since the prompt can be larger than \n",
            "4k tokens. \n",
            "92THE BIG BOOK OF GENERATIVE AI\n",
            "EXPERIMENT 3: APPROPRIATE GRADE SCALES\n",
            "The LLM-as-judge paper uses a non-integer 0~10 scale (i.e., float) for the grading scale; in other words, it uses \n",
            "a high precision rubric for the final score. We found these high-precision scales cause issues downstream with \n",
            "the following:\n",
            " ■ Consistency: Evaluators–both human and LLM–struggled to hold the same standard for the same score \n",
            "when grading on high precision. As a result, we found that output scores are less consistent across judges \n",
            "if you move from low-precision to high-precision scales. \n",
            " ■ Explainability: Additionally, if we want to cross-validate the LLM-judged results with human-judged \n",
            "results we must provide instructions on how to grade answers. It is very difficult to provide accurate \n",
            "instructions for each “score” in a high-precision grading scale–for example, what’s a good example for an \n",
            "answer that’s scored at 5.1 as compared to 5.6? \n",
            "93THE BIG BOOK OF GENERATIVE AI\n",
            "We experimented with various low-precision grading scales to provide guidance on the “best” one to use, \n",
            "ultimately we recommend an integer scale of 0-3 or 0-4 (if you want to stick to the Likert scale). We tried  \n",
            "0-10, 1-5, 0-3, and 0-1 and learned:\n",
            " ■ Binary grading works for simple metrics like “usability” or “good/bad”.\n",
            " ■ Scales like 0-10 are difficult to come up with distinguishing criteria between all scores.\n",
            "94THE BIG BOOK OF GENERATIVE AI\n",
            "As shown in these plots, both GPT-4 and GPT-3.5 can retain consistent ranking of results using different  \n",
            "low-precision grading scales, thus using a lower grading scale like 0~3 or 1~5 can balance the precision  \n",
            "with explainability).\n",
            "Thus, we recommend 0-3 or 1-5 as a grading scale to make it easier to align with human labels, reason about \n",
            "scoring criteria, and provide examples for each score in the range. \n",
            "95THE BIG BOOK OF GENERATIVE AI\n",
            "EXPERIMENT 4: APPLICABILITY ACROSS USE CASES\n",
            "The LLM-as-judge paper shows that both LLM and human judgment ranks the Vicuna-13B model as a close \n",
            "competitor to GPT-3.5:\n",
            "However, when we benchmarked the set of models for our document Q&A use cases, we found that even the \n",
            "much larger Vicuna-33B model has a noticeably worse performance than GPT-3.5 when answering questions \n",
            "based on context. These findings are also verified by GPT-4, GPT-3.5 and human judges (as mentioned in \n",
            "Experiment 1) which all agree that Vicuna-33B is performing worse than GPT-3.5.\n",
            "Figure 4: Average win rate of nine models under different judges on Chatbot Arena.\n",
            "96THE BIG BOOK OF GENERATIVE AI\n",
            "We looked closer at the benchmark dataset proposed by the paper and found that the 3 categories of tasks \n",
            "(writing, math, knowledge) don’t directly reflect or contribute to the model’s ability to synthesize an answer \n",
            "based on a context. Instead, intuitively, document Q&A use cases need benchmarks on reading comprehension \n",
            "and instruction following. Thus evaluation results can’t be transferred between use cases and we need to build \n",
            "use-case-specific benchmarks in order to properly evaluate how good a model can meet customer needs.\n",
            "97THE BIG BOOK OF GENERATIVE AI\n",
            "USE MLFLOW TO LEVERAGE OUR BEST PRACTICES\n",
            "With the experiments above, we explored how different factors can significantly affect the evaluation of a \n",
            "chatbot and confirmed that LLM as a judge can largely reflect human preferences for the document Q&A use \n",
            "case. At Databricks, we are evolving the MLflow Evaluation API to help your team effectively evaluate your LLM \n",
            "applications based on these findings. MLflow 2.4 introduced the Evaluation API for LLMs to compare various \n",
            "models’ text output side-by-side, MLflow 2.6 introduced LLM-based metrics for evaluation like toxicity and \n",
            "perplexity, and we’re working to support LLM-as-a-judge in the near future!\n",
            "In the meantime, we compiled the list of resources we referenced in our research below:\n",
            " ■ Doc_qa repository\n",
            " ■ The code and data we used to conduct the experiments\n",
            " ■ LLM-as-Judge Research paper from lmsys group \n",
            " ■ The paper is the first research for using LLM as judge for the casual chat use cases, it extensively \n",
            "explored the feasibility and pros and cons of using LLM (GPT-4, ClaudeV1, GPT-3.5) as the judge for \n",
            "tasks in writing, math, world knowledge\n",
            "Offline LLM Evaluation: Step-by-Step GenAI Application Assessment on Databricks\n",
            "by Abe Omorogbe, Liang Zhang, Sunish Sheth, Corey Zumar, Maheswaran Venkatachalam, Emil Lysgaard  \n",
            "and Mathias Christiansen\n",
            "BACKGROUND\n",
            "In an era where retrieval augmented generation (RAG) is revolutionizing the way we interact with AI-driven \n",
            "applications, ensuring the efficiency and effectiveness of these systems has never been more essential. \n",
            "Databricks and MLflow are at the forefront of this innovation, offering streamlined solutions for the critical \n",
            "evaluation of GenAI applications. \n",
            "This blog post guides you through the simple and effective process of leveraging the Databricks Data \n",
            "Intelligence Platform to enhance and evaluate the quality of the three core components of your GenAI \n",
            "applications: Prompts, Retrieval System, and Foundation LLM, ensuring that your GenAI applications continue  \n",
            "to generate accurate results.\n",
            "98THE BIG BOOK OF GENERATIVE AI\n",
            "USE CASE\n",
            "We are going to be creating a QA chatbot that will answer questions from the MLflow documentation and then \n",
            "evaluate the results.\n",
            "99THE BIG BOOK OF GENERATIVE AI\n",
            "SET UP EXTERNAL MODELS IN DATABRICKS\n",
            "Databricks Model Serving feature can be used to manage, govern, and access external models from various \n",
            "large language model (LLM) providers, such as Azure OpenAI GPT, Anthropic Claude, or AWS Bedrock, within \n",
            "an organization. It offers a high-level interface that simplifies the interaction with these services by providing a \n",
            "unified endpoint to handle specific LLM related requests.\n",
            "Major advantages of using Model Serving:\n",
            " ■ Query Models Through a Unified Interface:  Simplifies the interface to call multiple LLMs in your \n",
            "organization. Query models through a unified OpenAI-compatible API and SDK and manage all models \n",
            "through a single UI.\n",
            " ■ Govern and Manage Models: Centralizes endpoint management of multiple LLMs in your organization.  \n",
            "This includes the ability to manage permissions and track usage limits.\n",
            " ■ Central Key Management: Centralizes API key management in a secure location, which enhances \n",
            "organizational security by minimizing key exposure in the system and code, and reduces the burden  \n",
            "on end-users.\n",
            "100THE BIG BOOK OF GENERATIVE AI\n",
            "CREATE A SERVING ENDPOINT WITH AN EXTERNAL MODEL IN DATABRICKS\n",
            " \n",
            "1\n",
            "2\n",
            "3 \n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "23\n",
            "25\n",
            "26\n",
            "import mlflow\n",
            "import mlflow.deployments\n",
            "client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
            "endpoint_name = f\"test-endpoint-{uuid.uuid4()}\"\n",
            "client.create_endpoint(\n",
            "name=endpoint_name,\n",
            "config={\n",
            "        \"served_entities\": [\n",
            "            {\n",
            "                \"name\": \"test\",\n",
            "                \"external_model\": {\n",
            "                    \"name\": \"gpt-3.5-turbo-instruct\",\n",
            "                    \"provider\": \"openai\",\n",
            "                    \"task\": \"llm/v1/completions\",\n",
            "                    \"openai_config\": {\n",
            "                        \"openai_api_type\": \"azure\",\n",
            "                        \"openai_api_key\": \"{{secrets/<your-scope-name>/<your-key-name>}}\", ## Use Databricks Secrets. \n",
            "                        \"openai_api_base\": \"https://<your-endpoint>.openai.azure.com/\",\n",
            "                        \"openai_deployment_name\": \"<your-deployment-name>\",\n",
            "                        \"openai_api_version\": \"2023-05-15\",\n",
            "                    },\n",
            "                },\n",
            "            }\n",
            "        ],\n",
            "     },\n",
            ")\n",
            "101THE BIG BOOK OF GENERATIVE AI\n",
            "EXPLORE PROMPTS WITH THE DATABRICKS AI PLAYGROUND\n",
            "In this section, we will understand: How well do different prompts perform with the chosen LLM?\n",
            "We recently introduced the Databricks AI Playground, which provides a best-in-class experience for crafting the \n",
            "perfect prompt. With no code required, you can try out multiple LLMs served as Endpoints in Databricks, and \n",
            "test different parameters and prompts.\n",
            "Major advantages of the Databricks AI Playground are:\n",
            " ■ Quick Testing: Quickly test deployed models directly in Databricks.\n",
            " ■ Easy Comparison: Central location to compare multiple models on different prompts and parameters for \n",
            "comparison and selection.\n",
            "USING DATABRICKS AI PLAYGROUND\n",
            "We delve into testing relevant prompts with OpenAI GPT 3.5 Turbo, leveraging the Databricks AI Playground. \n",
            "102THE BIG BOOK OF GENERATIVE AI\n",
            "COMPARING DIFFERENT PROMPTS AND PARAMETERS\n",
            "In the Playground, you are able to compare the output of multiple prompts to see which gives better results. \n",
            "Directly in the Playground, you can try several prompts,  models, and parameters to figure out which \n",
            "combination provides the best results. The model and parameters combo can then be added to the GenAI  \n",
            "app and used for answer generation with the right context.\n",
            "103THE BIG BOOK OF GENERATIVE AI\n",
            "ADDING MODEL AND PARAMETERS TO YOUR GENAI APPLICATION\n",
            "After playing with a few prompts and parameters, you can use the same settings and model in your  \n",
            "GenAI application.\n",
            "104THE BIG BOOK OF GENERATIVE AI\n",
            "Example of how to import the same external model in LangChain. We will cover how we turn this into a GenAI \n",
            "POC in the next section.\n",
            "CREATE GENAI POC WITH LANGCHAIN AND LOG WITH MLFLOW\n",
            "Now that we have found a good model and prompt parameters for your use case, we are going to create a \n",
            "sample GenAI app that is a QA chatbot that will answer questions from the MLflow documentation using a \n",
            "vector database, embedding model with the Databricks Foundation Model API and Azure OpenAI GPT 3.5 as \n",
            "the generation model.\n",
            " \n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "from langchain.llms import Databricks\n",
            "llm = Databricks(\n",
            "    endpoint_name=\"<endpoint-name>\",\n",
            "    extra_params={\"temperature\": 0.1,\n",
            "                 \"top_p\": 0.1,\n",
            "                 \"max_tokens\": 500,\n",
            "                 } #parameters used in AI Playground\n",
            ")\n",
            "105THE BIG BOOK OF GENERATIVE AI\n",
            "CREATE A SAMPLE GENAI APP WITH LANGCHAIN USING DOCS FROM THE MLFLOW WEBSITE\n",
            " \n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "import os\n",
            "import pandas as pd\n",
            "import mlflow\n",
            "import chromadb\n",
            "from langchain.chains import RetrievalQA\n",
            "from langchain.document_loaders import WebBaseLoader\n",
            "from langchain.llms import Databricks\n",
            "from langchain.embeddings.databricks import DatabricksEmbeddings\n",
            "from langchain.text_splitter import CharacterTextSplitter\n",
            "from langchain.vectorstores import Chroma\n",
            "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
            "loader = WebBaseLoader(\n",
            "    [ \n",
            "     \"https://mlflow.org/docs/latest/index.html\",\n",
            "     \"https://mlflow.org/docs/latest/tracking/autolog.html\", \n",
            "     \"https://mlflow.org/docs/latest/getting-started/tracking-server-overview/index.html\",\n",
            "     \"https://mlflow.org/docs/latest/python_api/mlflow.deployments.html\" ])\n",
            "documents = loader.load()\n",
            "CHUNK_SIZE = 1000\n",
            "text_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=0)\n",
            "texts = text_splitter.split_documents(documents)\n",
            "llm = Databricks(\n",
            "    endpoint_name=\"<endpoint-name>\",\n",
            "    extra_params={\"temperature\": 0.1,\n",
            "                 \"top_p\": 0.1,\n",
            "                 \"max_tokens\": 500,\n",
            "                 } #parameters used in AI Playground\n",
            ")\n",
            "# create the embedding function using Databricks Foundation Model APIs\n",
            "embedding_function = DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\")\n",
            "docsearch = Chroma.from_documents(texts, embedding_function)\n",
            "qa = RetrievalQA.from_chain_type(\n",
            "    llm=llm,\n",
            "    chain_type=\"stuff\",\n",
            "    retriever=docsearch.as_retriever(fetch_k=3),\n",
            "    return_source_documents=True,\n",
            ")\n",
            "106THE BIG BOOK OF GENERATIVE AI\n",
            "For customers wanting to scale the retriever used in their GenAI application, we advise using Databricks Vector \n",
            "Search, a serverless similarity search engine that allows you to store a vector representation of your data, \n",
            "including metadata, in a vector database.\n",
            "EVALUATION OF RETRIEVAL SYSTEM WITH MLFLOW\n",
            "In this section, we will understand: How well does the retriever work with a given query?\n",
            "In MLflow 2.9.1, Evaluation for retrievers was introduced and provides a way for you to assess the efficiency \n",
            "of their retriever with the MLflow evaluate API. You can use this API to evaluate the effectiveness of your \n",
            "embedding model, the top K threshold choice, or the chunking strategy.\n",
            "CREATING A GROUND TRUTH DATASET\n",
            "Curating a ground truth dataset for evaluating your GenAI often involves the meticulous task of manually \n",
            "annotating test sets, a process that demands both time and domain expertise. In this blog, we’re taking a \n",
            "different route. We’re leveraging the power of an LLM to generate synthetic data for testing, offering a quick-\n",
            "start approach to get a sense of your GenAI app’s retrieval capability, and a warm-up for all the in-depth \n",
            "evaluation work that may follow. To our readers and customers, we emphasize the importance of crafting a \n",
            "dataset that mirrors the expected inputs and outputs of your GenAI application. It’s a journey worth taking for \n",
            "the incredible insights you’ll gain!\n",
            "You can explore with the full dataset but let's demo with a subset of the generated data. The question column \n",
            "contains all the questions that will be evaluated and the source column is the expected source for the answer \n",
            "for the questions as an ordered list of strings.\n",
            "107THE BIG BOOK OF GENERATIVE AI\n",
            " \n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "eval_data = pd.DataFrame(\n",
            "    {\n",
            "        \"question\": [\n",
            "            \"What is MLflow?\",\n",
            "            \"What is Databricks?\",\n",
            "            \"How to serve a model on Databricks?\",\n",
            "            \"How to enable MLflow Autologging for my workspace by default?\",\n",
            "        ],\n",
            "        \"source\": [\n",
            "            [\"https://mlflow.org/docs/latest/index.html\"],\n",
            "            [\"https://mlflow.org/docs/latest/getting-started/tracking-server-overview/index.html\"],\n",
            "            [\"https://mlflow.org/docs/latest/python_api/mlflow.deployments.html\"],\n",
            "            [\"https://mlflow.org/docs/latest/tracking/autolog.html\"],\n",
            "        ],\n",
            "    }\n",
            ")\n",
            "EVALUATE THE EMBEDDING MODEL WITH MLFLOW\n",
            "The quality of your embedding model is pivotal for accurate retrieval. In MLflow 2.9.0, we introduced three built-\n",
            "in metrics mlflow.metrics.precision_at_k(k), mlflow.metrics.recall_at_k(k) and mlflow.metrics.ndcg_at_k(k) \n",
            "to help determine how effective your retriever is at predicting the most relevant results for you. For example; \n",
            "Suppose the vector database returns 10 results (k=10), and out of these 10 results, 4 are relevant to your query. \n",
            "The precision_at_10 would be 4/10 or 40%. \n",
            "108THE BIG BOOK OF GENERATIVE AI\n",
            " \n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "def evaluate_embedding(embedding_function):\n",
            "    CHUNK_SIZE = 1000\n",
            "    list_of_documents = loader.load()\n",
            "    text_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=0)\n",
            "    docs = text_splitter.split_documents(list_of_documents)\n",
            "    retriever = Chroma.from_documents(docs, embedding_function).as_retriever()\n",
            "    def retrieve_doc_ids(question: str) -> List[str]:\n",
            "        docs = retriever.get_relevant_documents(question)\n",
            "        doc_ids = [doc.metadata[\"source\"] for doc in docs]\n",
            "        return doc_ids\n",
            "    def retriever_model_function(question_df: pd.DataFrame) -> pd.Series:\n",
            "        return question_df[\"question\"].apply(retrieve_doc_ids)\n",
            "    with mlflow.start_run() as run:\n",
            "        evaluate_results = mlflow.evaluate(\n",
            "                model=retriever_model_function,\n",
            "                data=eval_data,\n",
            "                model_type=\"retriever\",\n",
            "                targets=\"source\",\n",
            "                evaluators=\"default\",\n",
            "            )\n",
            "    return evaluate_results\n",
            "result1 = evaluate_embedding(DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\"))result2 = evaluate_embed-\n",
            "ding(<another-embedding-function>)\n",
            "eval_results_of_retriever_df_bge = result1.tables[\"eval_results_table\"]\n",
            "display(eval_results_of_retriever_df_bge)\n",
            "109THE BIG BOOK OF GENERATIVE AI\n",
            "The evaluation will return a table with the results of your evaluation for each question. i.e., for this test, we can \n",
            "see that the retriever seems to performing great for the questions \"How to enable MLflow Autologging for my \n",
            "workspace by default?” with a Precision @ K score is 1, and is not retrieving any of the right documentation for \n",
            "the questions \"What is MLflow?” since the precision @ K score is 0. With this insight, we can debug the retriever \n",
            "and improve the retriever for questions like “What is MLflow?”\n",
            "Evaluation results when using databricks-bge-large-en embedding model\n",
            "EVALUATE RETRIEVER WITH DIFFERENT TOP K VALUES WITH MLFLOW\n",
            "You can quickly calculate the metrics for different Ks by specifying the extra_metrics argument.\n",
            " \n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "with mlflow.start_run() as run:\n",
            "        evaluate_results = mlflow.evaluate(\n",
            "        data=eval_results_of_retriever_df_bge,\n",
            "        targets=\"source\",\n",
            "        predictions=\"outputs\",\n",
            "        evaluators=\"default\",\n",
            "        extra_metrics=[\n",
            "            mlflow.metrics.precision_at_k(1),\n",
            "            mlflow.metrics.precision_at_k(2),\n",
            "            mlflow.metrics.precision_at_k(3),\n",
            "            mlflow.metrics.recall_at_k(1),\n",
            "            mlflow.metrics.recall_at_k(2),\n",
            "            mlflow.metrics.recall_at_k(3),\n",
            "            mlflow.metrics.ndcg_at_k(1),\n",
            "            mlflow.metrics.ndcg_at_k(2),\n",
            "            mlflow.metrics.ndcg_at_k(3),\n",
            "        ],\n",
            "    )\n",
            "display(evaluate_results.tables[\"eval_results_table\"])\n",
            "110THE BIG BOOK OF GENERATIVE AI\n",
            "The evaluation will return a table with the results of your evaluation for each question, and you can better \n",
            "understand which K value to use when retrieving documents. i.e., for this test we can see changing the top K \n",
            "value can positively affect the precision of the retriever for questions like “What is Databricks?”\n",
            "Evaluation result with all precision at K values\n",
            "111THE BIG BOOK OF GENERATIVE AI\n",
            "EVALUATE THE CHUNKING STRATEGY WITH MLFLOW\n",
            "The effectiveness of your chunking strategy is critical. We explore how MLflow can assist in this evaluation, \n",
            "focusing on the retrieval model type and its impact on overall performance.\n",
            " \n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "def evaluate_chunk_size(chunk_size):\n",
            "  list_of_documents = loader.load()\n",
            "  text_splitter = CharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=0)\n",
            "  docs = text_splitter.split_documents(list_of_documents)\n",
            "  embedding_function = DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\")\n",
            "  retriever = Chroma.from_documents(docs, embedding_function).as_retriever()\n",
            "  \n",
            "  def retrieve_doc_ids(question: str) -> List[str]:\n",
            "    docs = retriever.get_relevant_documents(question)\n",
            "    doc_ids = [doc.metadata[\"source\"] for doc in docs]\n",
            "    return doc_ids\n",
            "   \n",
            "  def retriever_model_function(question_df: pd.DataFrame) -> pd.Series:\n",
            "    return question_df[\"question\"].apply(retrieve_doc_ids)\n",
            "  with mlflow.start_run() as run:\n",
            "      evaluate_results = mlflow.evaluate(\n",
            "          model=retriever_model_function,\n",
            "          data=eval_data,\n",
            "          model_type=\"retriever\",\n",
            "          targets=\"source\",\n",
            "          evaluators=\"default\",\n",
            "      )\n",
            "  return evaluate_results\n",
            "result1 = evaluate_chunk_size(500)\n",
            "result2 = evaluate_chunk_size(2000)\n",
            "display(result1.tables[\"eval_results_table\"])\n",
            "display(result2.tables[\"eval_results_table\"])\n",
            "112THE BIG BOOK OF GENERATIVE AI\n",
            "The evaluation will return 2 tables with the results of your evaluation for each question using 2 different chunk \n",
            "sizes, and you can better understand which chunk size to use when retrieving documents (i.e., for this example, \n",
            "it seems like changing the chunk size did not affect any metric).\n",
            "Evaluation result with Chunk size of 1000\n",
            "Evaluation result with Chunk size of 2000\n",
            "Check out the in-depth notebook on retrieval evaluation\n",
            "113THE BIG BOOK OF GENERATIVE AI\n",
            "EVALUATION OF GENAI RESUL TS WITH MLFLOW\n",
            "In this section, we will understand: How good is the response of the GenAI app with a given prompt and context?\n",
            "Assessing the quality of generated responses is key. We will augment the manual process of evaluating with \n",
            "questions and answers by leveraging MLflow's QA metrics, and comparing them against a GPT-4 model as a \n",
            "benchmark to understand the effectiveness of the generated answers. \n",
            "Using an LLM like GPT-4 as a judge to assist in evaluation can offer several benefits, here are some key benefits:\n",
            " ■ Rapid and Scalable Experimentation:  In many situations, we think LLM judges represent a sweet-spot: \n",
            "they can evaluate unstructured outputs (like a response from a chat-bot) automatically, rapidly, and  \n",
            "at low-cost.  \n",
            " ■ Cost-Effective: By automating some evaluations with LLMs, we consider it a worthy companion to human \n",
            "evaluation, which is slower and more expensive but represents the gold standard of model evaluation.\n",
            "USE MLFLOW EVALUATE AND LLM AS A JUDGE\n",
            "We take some sample questions and use the LLM as a judge, and inspect the results with MLflow, providing a \n",
            "comprehensive analysis of the outcome with built-in metrics. We are going to judge the GenAI app on relevance \n",
            "(how relevant is the output with respect to both the input and the context).\n",
            "Create a simple function that runs each input through the chain\n",
            " \n",
            "1\n",
            "2\n",
            "def model(input_df):\n",
            "    return input_df[\"questions\"].map(qa).tolist()\n",
            " \n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "eval_df = pd.DataFrame(\n",
            "    {\n",
            "        \"questions\": [\n",
            "            \"What is MLflow?\",\n",
            "            \"What is Databricks?\",\n",
            "            \"How to serve a model on Databricks?\",\n",
            "            \"How to enable MLflow Autologging for my workspace by default?\",\n",
            "        ],\n",
            "    }\n",
            ")\n",
            "114THE BIG BOOK OF GENERATIVE AI\n",
            "Use relevance metric to determine the relevance of the answer and context. There are other metrics you can \n",
            "use too.\n",
            " \n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "from mlflow.deployments import set_deployments_target\n",
            "from  mlflow.metrics.genai.metric_definitions import relevance\n",
            "set_deployments_target(\"databricks\") #To retrieve all endpoint in your Databricks Workspace\n",
            "relevance_metric = relevance(model=f\"endpoints:/{endpoint_name}\") #You can also use any model you have hosted on Da-\n",
            "tabricks, models from the Marketplace or models in the Foundation model API\n",
            "with mlflow.start_run():\n",
            "    results =  mlflow.evaluate(\n",
            "        model,\n",
            "        eval_df,\n",
            "        model_type=\"question-answering\",\n",
            "        evaluators=\"default\",\n",
            "        predictions=\"result\",\n",
            "        extra_metrics=[relevance_metric, mlflow.metrics.latency()],\n",
            "        evaluator_config={\n",
            "            \"col_mapping\": {\n",
            "                \"inputs\": \"questions\",\n",
            "                \"context\": \"source_documents\",\n",
            "            }\n",
            "        }\n",
            "    )\n",
            "    print(results.metrics)\n",
            "115THE BIG BOOK OF GENERATIVE AI\n",
            "In your Databricks Workspace, you can compare and evaluate all your inputs and outputs, as well as the source \n",
            "documents, relevance and any other metrics you added to your evaluation function.\n",
            "Check out more in depth notebooks on LLM evaluation\n",
            "116THE BIG BOOK OF GENERATIVE AI\n",
            "Summary Whether you’re looking to disrupt traditional industries, enhance creative endeavors or solve complex problems \n",
            "in novel ways, the potential applications of generative AI are limited only by your imagination and willingness to \n",
            "experiment. Remember, every significant advancement in this field began with a simple idea and the courage to \n",
            "explore it further.\n",
            "For those seeking more knowledge or simply curious about the latest developments in the realm of generative \n",
            "AI, we’ve provided some resources on training, demos and product information. \n",
            "GenAI Training\n",
            "Generative AI Engineer Learning Pathway: Take self-paced, on-demand and instructor-led courses on \n",
            "generative AI\n",
            "Free LLM Course (edX): In-depth course to learn GenAI and LLMs inside and out\n",
            "GenAI Webinar: Learn how to take control of your GenAI app performance, privacy and cost, and drive value \n",
            "with generative AI\n",
            "Additional Resources\n",
            "Big Book of MLOps: A deep dive into the architectures and technologies behind MLOps — including LLMs  \n",
            "and GenAI \n",
            "Mosaic AI: Product page covering the features of Mosaic AI within Databricks\n",
            "117Build Production-Quality GenAI Applications — See How\n",
            "Create high-quality generative AI applications and ensure your output is accurate, \n",
            "governed and safe. See why over 10,000 organizations worldwide rely on Databricks for \n",
            "all their workloads from BI to AI — test-drive the full Databricks Platform free for 14 days.\n",
            "About Databricks\n",
            "Databricks is the data and AI company. More than 10,000 organizations worldwide — \n",
            "including Comcast, Condé Nast, Grammarly and over 50% of the Fortune 500 — rely on \n",
            "the Databricks Data Intelligence Platform to unify and democratize data, analytics and \n",
            "AI. Databricks is headquartered in San Francisco, with offices around the globe, and was \n",
            "founded by the original creators of Lakehouse, Apache Spark™, Delta Lake and MLflow.  \n",
            "To learn more, follow Databricks on LinkedIn, X and Facebook.\n",
            "Try Databricks free Take Generative AI Fundamentals On-Demand Training\n",
            "© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark and the Spark \n",
            "logo are trademarks of the Apache Software Foundation . Privacy Policy  | Terms of Use\n",
            "------------\n",
            "\n",
            "Create questions that will prepare the coders or programmers for their tests.\n",
            "Make sure not to lose any important information.\n",
            "\n",
            "QUESTIONS:\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "## Questions to Prepare Coders/Programmers for Generative AI Exams:\n",
            "\n",
            "Based on \"The Big Book of Generative AI,\" here are questions categorized by topic, designed to test understanding and practical application:\n",
            "\n",
            "\n",
            "**I. Foundation Models & DBRX:**\n",
            "\n",
            "1. **Conceptual:**  What are the key differences between proprietary and open-source foundation models?  Discuss the advantages and disadvantages of each, referencing examples from the text (GPT-3.5, Gemini, Llama2-70B, DBRX).\n",
            "\n",
            "2. **DBRX Specifics:** Describe DBRX's architecture (transformer-based, decoder-only, MoE). Explain the significance of its fine-grained MoE architecture in terms of performance (inference speed, training efficiency, parameter count) compared to other open-source models (Mixtral, Grok-1).  Include specific numbers from the text to support your answer.\n",
            "\n",
            "3. **Benchmarking:**  Compare DBRX Instruct's performance against leading open-source models (Mixtral Instruct, Grok-1, Llama2-70B) and closed models (GPT-3.5, Gemini 1.0 Pro, Mistral Medium) across various benchmarks (MMLU, HumanEval, GSM8k, Hugging Face Open LLM Leaderboard, Databricks Model Gauntlet).  What benchmarks does DBRX excel in, and where does it fall short?  What conclusions can you draw about DBRX's strengths and weaknesses based on these comparisons?\n",
            "\n",
            "4. **Practical Application:** How can Databricks customers access and utilize DBRX? Describe the different options available (APIs, Hugging Face, Databricks Marketplace, Mosaic AI Model Serving).\n",
            "\n",
            "5. **Training Methodology:** Summarize the process Databricks used to train DBRX, highlighting the tools and technologies employed (Apache Spark, Unity Catalog, MLflow, MegaBlocks, LLM Foundry, Composer, Streaming, Mosaic AI Training).  What were some of the key scientific and performance challenges overcome during the training process?\n",
            "\n",
            "\n",
            "**II. Prompt Engineering:**\n",
            "\n",
            "1. **Use Case:** Explain how prompt engineering can be used to automate the analysis of product reviews using large language models.  Provide specific examples of questions that could be asked of the LLM to extract actionable insights.\n",
            "\n",
            "2. **Solution Accelerator:** Describe the \"Solution Accelerator for summarizing product reviews\" mentioned in the text. What dataset was used? What were the key challenges addressed, and how was the solution implemented using Databricks?  What were the cost and performance benefits compared to using a third-party service?\n",
            "\n",
            "3. **Challenges & Mitigation:** What are the challenges associated with using LLMs for tasks like product review summarization (accuracy, bias)? How can these challenges be mitigated?\n",
            "\n",
            "\n",
            "**III. Retrieval Augmented Generation (RAG):**\n",
            "\n",
            "1. **Conceptual:** Explain the concept of RAG. How does it work, and what are its benefits (reduced hallucinations, improved accuracy, cost-effectiveness)?  What are its limitations?\n",
            "\n",
            "2. **Use Case:** Describe the use case presented for improving RAG application response quality with real-time structured data.  What specific technologies were used (Databricks Vector Search, Databricks Feature & Function Serving, LangChain)?  Provide an example of how this approach could be applied in a different industry (e.g., healthcare, finance).\n",
            "\n",
            "3. **Data Handling:** What are the key considerations for data preparation and management when implementing RAG?  How can tools like Databricks Vector Search and Unity Catalog help address these challenges?\n",
            "\n",
            "\n",
            "**IV. Fine-Tuning:**\n",
            "\n",
            "1. **Conceptual:** Explain the process of fine-tuning a foundation model.  What are the advantages of fine-tuning over using off-the-shelf models?  When is fine-tuning an appropriate approach?\n",
            "\n",
            "2. **Use Case (AI-Generated Documentation):** Detail the experience described in creating a bespoke LLM for AI-generated documentation. What were the initial challenges encountered with using SaaS-based LLMs (quality, performance, cost)? How were these challenges addressed by building a bespoke model?  What model was chosen (MPT-7B), and why?  What tools and technologies were used in the development and deployment process (Databricks LLM fine-tuning, Unity Catalog, Delta Sharing, Databricks optimized LLM serving)?  Include cost and time figures.\n",
            "\n",
            "3. **Use Case (LoRA):** Explain the concept of Low-Rank Adaptation (LoRA) and Quantized LoRA (QLoRA) for efficient fine-tuning.  What are the advantages of these methods compared to full fine-tuning?  Describe the experiment conducted using OpenLLaMA-3b-v2, including the dataset used, the hyperparameters explored (r, target_modules), and the key findings regarding optimal parameter selection.  What are the trade-offs between using LoRA and QLoRA?  How are LoRA adapters used in deployment (merging weights)?\n",
            "\n",
            "\n",
            "**V. Pretraining:**\n",
            "\n",
            "1. **Conceptual:** Explain the process of pretraining a language model from scratch.  When is pretraining a suitable approach?  What are the key challenges and considerations involved in pretraining (data preprocessing, hyperparameter tuning, resource utilization, handling GPU failures, monitoring and evaluation)?\n",
            "\n",
            "2. **Use Case (Stable Diffusion):** Describe the experience of training Stable Diffusion from scratch for under $50K using the MosaicML platform.  What model architecture, dataset, and compute resources were used?  What were the key challenges encountered, and how were they addressed using the MosaicML platform, StreamingDataset, and Composer library?  What optimizations were implemented to reduce training time and cost (xFormers FlashAttention, precomputing latents, low-precision LayerNorm and GroupNorm, FSDP, scheduled EMA)?\n",
            "\n",
            "\n",
            "**VI. LLM Evaluation:**\n",
            "\n",
            "1. **Challenges:** Discuss the challenges involved in evaluating LLMs, including variable performance, lack of ground truth, domain-specific evaluation, and reliance on human judgment.\n",
            "\n",
            "2. **Best Practices (RAG Applications):** Summarize the best practices for LLM evaluation of RAG applications, as described in the case study on the Databricks Documentation Bot.  What are the key findings regarding the alignment of LLM-as-a-judge with human grading, the effectiveness of providing examples to the LLM judge, appropriate grade scales, and the applicability of evaluation metrics across different use cases?  Describe the methodology used to establish these best practices.\n",
            "\n",
            "3. **Offline LLM Evaluation (MLflow):**  Describe the step-by-step process for offline LLM evaluation on Databricks using MLflow, focusing on the evaluation of prompts, the retrieval system, and the foundation LLM.  What tools and techniques were used (Databricks Model Serving, Databricks AI Playground, LangChain, Databricks Vector Search, MLflow Evaluate API)?  Explain how different metrics (precision@k, recall@k, NDCG@k, relevance, latency) can be used to assess the performance of each component.  How can synthetic data be used to create a ground truth dataset for evaluation?\n",
            "\n",
            "\n",
            "These questions cover the key concepts and practical aspects of generative AI development as presented in the provided text.  They are designed to encourage critical thinking and application of knowledge, preparing coders/programmers for a comprehensive exam.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "8Pn2p0gQttFA"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n"
      ],
      "metadata": {
        "id": "hjbgoQvitwjN"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embedding_ques = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/embedding-001\",   # this is the correct one\n",
        "    google_api_key=gemini_api\n",
        ")\n"
      ],
      "metadata": {
        "id": "V-eN9S4H0KkT"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPenCir81OXu",
        "outputId": "8edc8304-4323-45ca-bf0e-928cbd6c3a34"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore=FAISS.from_documents(doc_ques_gen,embedding_ques)"
      ],
      "metadata": {
        "id": "JC0vHSC-1sXt"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_answer_gen=llm_que_gen_pipeline = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=gemini_api,\n",
        "    temperature=0.2\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "46o6wQYZ1pZo"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ques"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "P1UgDAXS2Kuh",
        "outputId": "7b62928e-8d25-4dbc-e003-d256651523c2"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'## Questions to Prepare Coders/Programmers for Generative AI Exams:\\n\\nBased on \"The Big Book of Generative AI,\" here are questions categorized by topic, designed to test understanding and practical application:\\n\\n\\n**I. Foundation Models & DBRX:**\\n\\n1. **Conceptual:**  What are the key differences between proprietary and open-source foundation models?  Discuss the advantages and disadvantages of each, referencing examples from the text (GPT-3.5, Gemini, Llama2-70B, DBRX).\\n\\n2. **DBRX Specifics:** Describe DBRX\\'s architecture (transformer-based, decoder-only, MoE). Explain the significance of its fine-grained MoE architecture in terms of performance (inference speed, training efficiency, parameter count) compared to other open-source models (Mixtral, Grok-1).  Include specific numbers from the text to support your answer.\\n\\n3. **Benchmarking:**  Compare DBRX Instruct\\'s performance against leading open-source models (Mixtral Instruct, Grok-1, Llama2-70B) and closed models (GPT-3.5, Gemini 1.0 Pro, Mistral Medium) across various benchmarks (MMLU, HumanEval, GSM8k, Hugging Face Open LLM Leaderboard, Databricks Model Gauntlet).  What benchmarks does DBRX excel in, and where does it fall short?  What conclusions can you draw about DBRX\\'s strengths and weaknesses based on these comparisons?\\n\\n4. **Practical Application:** How can Databricks customers access and utilize DBRX? Describe the different options available (APIs, Hugging Face, Databricks Marketplace, Mosaic AI Model Serving).\\n\\n5. **Training Methodology:** Summarize the process Databricks used to train DBRX, highlighting the tools and technologies employed (Apache Spark, Unity Catalog, MLflow, MegaBlocks, LLM Foundry, Composer, Streaming, Mosaic AI Training).  What were some of the key scientific and performance challenges overcome during the training process?\\n\\n\\n**II. Prompt Engineering:**\\n\\n1. **Use Case:** Explain how prompt engineering can be used to automate the analysis of product reviews using large language models.  Provide specific examples of questions that could be asked of the LLM to extract actionable insights.\\n\\n2. **Solution Accelerator:** Describe the \"Solution Accelerator for summarizing product reviews\" mentioned in the text. What dataset was used? What were the key challenges addressed, and how was the solution implemented using Databricks?  What were the cost and performance benefits compared to using a third-party service?\\n\\n3. **Challenges & Mitigation:** What are the challenges associated with using LLMs for tasks like product review summarization (accuracy, bias)? How can these challenges be mitigated?\\n\\n\\n**III. Retrieval Augmented Generation (RAG):**\\n\\n1. **Conceptual:** Explain the concept of RAG. How does it work, and what are its benefits (reduced hallucinations, improved accuracy, cost-effectiveness)?  What are its limitations?\\n\\n2. **Use Case:** Describe the use case presented for improving RAG application response quality with real-time structured data.  What specific technologies were used (Databricks Vector Search, Databricks Feature & Function Serving, LangChain)?  Provide an example of how this approach could be applied in a different industry (e.g., healthcare, finance).\\n\\n3. **Data Handling:** What are the key considerations for data preparation and management when implementing RAG?  How can tools like Databricks Vector Search and Unity Catalog help address these challenges?\\n\\n\\n**IV. Fine-Tuning:**\\n\\n1. **Conceptual:** Explain the process of fine-tuning a foundation model.  What are the advantages of fine-tuning over using off-the-shelf models?  When is fine-tuning an appropriate approach?\\n\\n2. **Use Case (AI-Generated Documentation):** Detail the experience described in creating a bespoke LLM for AI-generated documentation. What were the initial challenges encountered with using SaaS-based LLMs (quality, performance, cost)? How were these challenges addressed by building a bespoke model?  What model was chosen (MPT-7B), and why?  What tools and technologies were used in the development and deployment process (Databricks LLM fine-tuning, Unity Catalog, Delta Sharing, Databricks optimized LLM serving)?  Include cost and time figures.\\n\\n3. **Use Case (LoRA):** Explain the concept of Low-Rank Adaptation (LoRA) and Quantized LoRA (QLoRA) for efficient fine-tuning.  What are the advantages of these methods compared to full fine-tuning?  Describe the experiment conducted using OpenLLaMA-3b-v2, including the dataset used, the hyperparameters explored (r, target_modules), and the key findings regarding optimal parameter selection.  What are the trade-offs between using LoRA and QLoRA?  How are LoRA adapters used in deployment (merging weights)?\\n\\n\\n**V. Pretraining:**\\n\\n1. **Conceptual:** Explain the process of pretraining a language model from scratch.  When is pretraining a suitable approach?  What are the key challenges and considerations involved in pretraining (data preprocessing, hyperparameter tuning, resource utilization, handling GPU failures, monitoring and evaluation)?\\n\\n2. **Use Case (Stable Diffusion):** Describe the experience of training Stable Diffusion from scratch for under $50K using the MosaicML platform.  What model architecture, dataset, and compute resources were used?  What were the key challenges encountered, and how were they addressed using the MosaicML platform, StreamingDataset, and Composer library?  What optimizations were implemented to reduce training time and cost (xFormers FlashAttention, precomputing latents, low-precision LayerNorm and GroupNorm, FSDP, scheduled EMA)?\\n\\n\\n**VI. LLM Evaluation:**\\n\\n1. **Challenges:** Discuss the challenges involved in evaluating LLMs, including variable performance, lack of ground truth, domain-specific evaluation, and reliance on human judgment.\\n\\n2. **Best Practices (RAG Applications):** Summarize the best practices for LLM evaluation of RAG applications, as described in the case study on the Databricks Documentation Bot.  What are the key findings regarding the alignment of LLM-as-a-judge with human grading, the effectiveness of providing examples to the LLM judge, appropriate grade scales, and the applicability of evaluation metrics across different use cases?  Describe the methodology used to establish these best practices.\\n\\n3. **Offline LLM Evaluation (MLflow):**  Describe the step-by-step process for offline LLM evaluation on Databricks using MLflow, focusing on the evaluation of prompts, the retrieval system, and the foundation LLM.  What tools and techniques were used (Databricks Model Serving, Databricks AI Playground, LangChain, Databricks Vector Search, MLflow Evaluate API)?  Explain how different metrics (precision@k, recall@k, NDCG@k, relevance, latency) can be used to assess the performance of each component.  How can synthetic data be used to create a ground truth dataset for evaluation?\\n\\n\\nThese questions cover the key concepts and practical aspects of generative AI development as presented in the provided text.  They are designed to encourage critical thinking and application of knowledge, preparing coders/programmers for a comprehensive exam.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ques_list=ques.split(\"\\n\")"
      ],
      "metadata": {
        "id": "vwnTopoG2RMX"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ques_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9qU4_3K2WnF",
        "outputId": "3c80cb86-3660-4c52-f0d9-e20826e4aa49"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['## Questions to Prepare Coders/Programmers for Generative AI Exams:',\n",
              " '',\n",
              " 'Based on \"The Big Book of Generative AI,\" here are questions categorized by topic, designed to test understanding and practical application:',\n",
              " '',\n",
              " '',\n",
              " '**I. Foundation Models & DBRX:**',\n",
              " '',\n",
              " '1. **Conceptual:**  What are the key differences between proprietary and open-source foundation models?  Discuss the advantages and disadvantages of each, referencing examples from the text (GPT-3.5, Gemini, Llama2-70B, DBRX).',\n",
              " '',\n",
              " \"2. **DBRX Specifics:** Describe DBRX's architecture (transformer-based, decoder-only, MoE). Explain the significance of its fine-grained MoE architecture in terms of performance (inference speed, training efficiency, parameter count) compared to other open-source models (Mixtral, Grok-1).  Include specific numbers from the text to support your answer.\",\n",
              " '',\n",
              " \"3. **Benchmarking:**  Compare DBRX Instruct's performance against leading open-source models (Mixtral Instruct, Grok-1, Llama2-70B) and closed models (GPT-3.5, Gemini 1.0 Pro, Mistral Medium) across various benchmarks (MMLU, HumanEval, GSM8k, Hugging Face Open LLM Leaderboard, Databricks Model Gauntlet).  What benchmarks does DBRX excel in, and where does it fall short?  What conclusions can you draw about DBRX's strengths and weaknesses based on these comparisons?\",\n",
              " '',\n",
              " '4. **Practical Application:** How can Databricks customers access and utilize DBRX? Describe the different options available (APIs, Hugging Face, Databricks Marketplace, Mosaic AI Model Serving).',\n",
              " '',\n",
              " '5. **Training Methodology:** Summarize the process Databricks used to train DBRX, highlighting the tools and technologies employed (Apache Spark, Unity Catalog, MLflow, MegaBlocks, LLM Foundry, Composer, Streaming, Mosaic AI Training).  What were some of the key scientific and performance challenges overcome during the training process?',\n",
              " '',\n",
              " '',\n",
              " '**II. Prompt Engineering:**',\n",
              " '',\n",
              " '1. **Use Case:** Explain how prompt engineering can be used to automate the analysis of product reviews using large language models.  Provide specific examples of questions that could be asked of the LLM to extract actionable insights.',\n",
              " '',\n",
              " '2. **Solution Accelerator:** Describe the \"Solution Accelerator for summarizing product reviews\" mentioned in the text. What dataset was used? What were the key challenges addressed, and how was the solution implemented using Databricks?  What were the cost and performance benefits compared to using a third-party service?',\n",
              " '',\n",
              " '3. **Challenges & Mitigation:** What are the challenges associated with using LLMs for tasks like product review summarization (accuracy, bias)? How can these challenges be mitigated?',\n",
              " '',\n",
              " '',\n",
              " '**III. Retrieval Augmented Generation (RAG):**',\n",
              " '',\n",
              " '1. **Conceptual:** Explain the concept of RAG. How does it work, and what are its benefits (reduced hallucinations, improved accuracy, cost-effectiveness)?  What are its limitations?',\n",
              " '',\n",
              " '2. **Use Case:** Describe the use case presented for improving RAG application response quality with real-time structured data.  What specific technologies were used (Databricks Vector Search, Databricks Feature & Function Serving, LangChain)?  Provide an example of how this approach could be applied in a different industry (e.g., healthcare, finance).',\n",
              " '',\n",
              " '3. **Data Handling:** What are the key considerations for data preparation and management when implementing RAG?  How can tools like Databricks Vector Search and Unity Catalog help address these challenges?',\n",
              " '',\n",
              " '',\n",
              " '**IV. Fine-Tuning:**',\n",
              " '',\n",
              " '1. **Conceptual:** Explain the process of fine-tuning a foundation model.  What are the advantages of fine-tuning over using off-the-shelf models?  When is fine-tuning an appropriate approach?',\n",
              " '',\n",
              " '2. **Use Case (AI-Generated Documentation):** Detail the experience described in creating a bespoke LLM for AI-generated documentation. What were the initial challenges encountered with using SaaS-based LLMs (quality, performance, cost)? How were these challenges addressed by building a bespoke model?  What model was chosen (MPT-7B), and why?  What tools and technologies were used in the development and deployment process (Databricks LLM fine-tuning, Unity Catalog, Delta Sharing, Databricks optimized LLM serving)?  Include cost and time figures.',\n",
              " '',\n",
              " '3. **Use Case (LoRA):** Explain the concept of Low-Rank Adaptation (LoRA) and Quantized LoRA (QLoRA) for efficient fine-tuning.  What are the advantages of these methods compared to full fine-tuning?  Describe the experiment conducted using OpenLLaMA-3b-v2, including the dataset used, the hyperparameters explored (r, target_modules), and the key findings regarding optimal parameter selection.  What are the trade-offs between using LoRA and QLoRA?  How are LoRA adapters used in deployment (merging weights)?',\n",
              " '',\n",
              " '',\n",
              " '**V. Pretraining:**',\n",
              " '',\n",
              " '1. **Conceptual:** Explain the process of pretraining a language model from scratch.  When is pretraining a suitable approach?  What are the key challenges and considerations involved in pretraining (data preprocessing, hyperparameter tuning, resource utilization, handling GPU failures, monitoring and evaluation)?',\n",
              " '',\n",
              " '2. **Use Case (Stable Diffusion):** Describe the experience of training Stable Diffusion from scratch for under $50K using the MosaicML platform.  What model architecture, dataset, and compute resources were used?  What were the key challenges encountered, and how were they addressed using the MosaicML platform, StreamingDataset, and Composer library?  What optimizations were implemented to reduce training time and cost (xFormers FlashAttention, precomputing latents, low-precision LayerNorm and GroupNorm, FSDP, scheduled EMA)?',\n",
              " '',\n",
              " '',\n",
              " '**VI. LLM Evaluation:**',\n",
              " '',\n",
              " '1. **Challenges:** Discuss the challenges involved in evaluating LLMs, including variable performance, lack of ground truth, domain-specific evaluation, and reliance on human judgment.',\n",
              " '',\n",
              " '2. **Best Practices (RAG Applications):** Summarize the best practices for LLM evaluation of RAG applications, as described in the case study on the Databricks Documentation Bot.  What are the key findings regarding the alignment of LLM-as-a-judge with human grading, the effectiveness of providing examples to the LLM judge, appropriate grade scales, and the applicability of evaluation metrics across different use cases?  Describe the methodology used to establish these best practices.',\n",
              " '',\n",
              " '3. **Offline LLM Evaluation (MLflow):**  Describe the step-by-step process for offline LLM evaluation on Databricks using MLflow, focusing on the evaluation of prompts, the retrieval system, and the foundation LLM.  What tools and techniques were used (Databricks Model Serving, Databricks AI Playground, LangChain, Databricks Vector Search, MLflow Evaluate API)?  Explain how different metrics (precision@k, recall@k, NDCG@k, relevance, latency) can be used to assess the performance of each component.  How can synthetic data be used to create a ground truth dataset for evaluation?',\n",
              " '',\n",
              " '',\n",
              " 'These questions cover the key concepts and practical aspects of generative AI development as presented in the provided text.  They are designed to encourage critical thinking and application of knowledge, preparing coders/programmers for a comprehensive exam.']"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "nTc4CfNV2YQx"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans_gen_chain=RetrievalQA.from_chain_type(llm=llm_answer_gen,\n",
        "                                           chain_type=\"stuff\",\n",
        "                                           retriever=vectorstore.as_retriever(),\n",
        "                                           verbose=True)"
      ],
      "metadata": {
        "id": "W28SqCV62kn4"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for question in ques_list:\n",
        "  # Check if the item is not an empty string and potentially add other checks if needed\n",
        "  if question and question.strip() and not question.strip().startswith('##') and not question.strip().startswith('**'):\n",
        "    print(\"Question:\", question)\n",
        "    answer = ans_gen_chain.run(question)\n",
        "    print(\"Answer:\", answer)\n",
        "    print(\"\\n-----------------------\\n\")\n",
        "    #it will save ans to file\n",
        "    with open(\"answer.txt\", \"a\") as f:\n",
        "      f.write(\"Question:\" + question + \"\\n\")\n",
        "      f.write(\"Answer:\" + answer + \"\\n \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QarsVuF2vEh",
        "outputId": "e3e91ae1-e005-4c63-ca04-2d7d8b10a46d"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Based on \"The Big Book of Generative AI,\" here are questions categorized by topic, designed to test understanding and practical application:\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: Please provide the questions categorized by topic.  I'm ready to answer them based on my understanding of \"The Big Book of Generative AI\".\n",
            "\n",
            "-----------------------\n",
            "\n",
            "Question: 1. **Conceptual:**  What are the key differences between proprietary and open-source foundation models?  Discuss the advantages and disadvantages of each, referencing examples from the text (GPT-3.5, Gemini, Llama2-70B, DBRX).\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: The key difference between proprietary and open-source foundation models lies in access and control.  Proprietary models, such as GPT-3.5 and Gemini, offer historically superior performance but require users to send their data to a third party and lack control over the underlying model, which is frequently updated and changed.  Open-source models, like Llama2-70B and DBRX, provide users with full control, allowing them to run the model on their own terms with their own governance and data privacy.  However, open-source models may lag behind proprietary models in terms of raw performance.\n",
            "\n",
            "**Proprietary Models (e.g., GPT-3.5, Gemini):**\n",
            "\n",
            "* **Advantages:** Generally higher performance on standard benchmarks at the time of release.  The text mentions DBRX surpassing GPT-3.5 and being competitive with Gemini 1.0 Pro on several benchmarks.\n",
            "* **Disadvantages:** Lack of control over the model and its updates. Data privacy concerns due to the need to send data to a third party.  Potential cost associated with API usage.\n",
            "\n",
            "**Open-Source Models (e.g., Llama2-70B, DBRX):**\n",
            "\n",
            "* **Advantages:** Full control over the model, allowing for customization and adaptation to specific needs and data privacy regulations. Ability to run the model on-premises. Potential cost savings compared to proprietary APIs, especially at scale.  The text highlights DBRX's cost-effectiveness and efficiency gains compared to proprietary models.\n",
            "* **Disadvantages:** May have lower performance compared to proprietary models, although the gap is narrowing. Requires more technical expertise to set up and manage.  The text notes that proprietary models historically had a performance edge.\n",
            "\n",
            "-----------------------\n",
            "\n",
            "Question: 2. **DBRX Specifics:** Describe DBRX's architecture (transformer-based, decoder-only, MoE). Explain the significance of its fine-grained MoE architecture in terms of performance (inference speed, training efficiency, parameter count) compared to other open-source models (Mixtral, Grok-1).  Include specific numbers from the text to support your answer.\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: DBRX is a transformer-based, decoder-only large language model (LLM) trained using next-token prediction.  Its key architectural feature is a fine-grained mixture-of-experts (MoE) architecture with 132B total parameters, of which 36B are active on any given input.  This contrasts with other open-source MoE models like Mixtral and Grok-1, which are coarser-grained.  DBRX uses 16 experts and chooses 4 at a time, while Mixtral and Grok-1 have 8 experts and choose 2. This difference provides DBRX with 65x more possible combinations of experts, leading to improved model quality.\n",
            "\n",
            "This fine-grained MoE architecture results in significant performance improvements:\n",
            "\n",
            "* **Inference speed:** Inference is up to 2x faster than LLaMA2-70B.  On an optimized platform, DBRX can generate text at up to 150 tok/s/user.\n",
            "* **Training efficiency:** Training MoEs is about 2x more FLOP-efficient than training dense models for the same final model quality.  The overall recipe for DBRX (data, architecture, optimization) matches the quality of Databricks' previous-generation MPT models with nearly 4x less compute.\n",
            "* **Parameter count:** DBRX is about 40% of the size of Grok-1 in terms of both total and active parameter counts, despite outperforming it on several benchmarks.  DBRX outperforms Grok-1 on HumanEval and GSM8k benchmarks, even though Grok-1 has 2.4x as many parameters.\n",
            "\n",
            "-----------------------\n",
            "\n",
            "Question: 3. **Benchmarking:**  Compare DBRX Instruct's performance against leading open-source models (Mixtral Instruct, Grok-1, Llama2-70B) and closed models (GPT-3.5, Gemini 1.0 Pro, Mistral Medium) across various benchmarks (MMLU, HumanEval, GSM8k, Hugging Face Open LLM Leaderboard, Databricks Model Gauntlet).  What benchmarks does DBRX excel in, and where does it fall short?  What conclusions can you draw about DBRX's strengths and weaknesses based on these comparisons?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: DBRX Instruct outperforms leading open-source models on composite benchmarks (Hugging Face Open LLM Leaderboard and Databricks Model Gauntlet), programming (HumanEval), and mathematics (GSM8k) benchmarks, and MMLU.  It surpasses all chat or instruction-fine-tuned models on standard benchmarks.  Specifically, it outperforms Grok-1 on HumanEval and GSM8k despite having fewer parameters.  It even surpasses CodeLLaMA-70B Instruct on HumanEval, a model specifically designed for programming.\n",
            "\n",
            "Compared to leading closed models, DBRX Instruct surpasses GPT-3.5 across nearly all benchmarks, particularly excelling in programming and mathematical reasoning.  It's competitive with Gemini 1.0 Pro and Mistral Medium, showing strengths in some areas while Gemini 1.0 Pro and Mistral Medium are stronger in others.\n",
            "\n",
            "In long-context tasks and RAG benchmarks (KV-Pairs, HotpotQAXL, Natural Questions, HotPotQA), DBRX Instruct performs comparably to Mixtral Instruct and Llama2-70B Chat, and is competitive with GPT-3.5 Turbo, though GPT-4 Turbo generally performs best.\n",
            "\n",
            "In summary, DBRX's strengths lie in its performance on composite benchmarks, programming, and mathematics, surpassing many open-source and even some closed models despite its relatively smaller size and efficient architecture.  However, it falls slightly short of the top-performing closed models like GPT-4 Turbo, especially in long-context tasks.  The comparisons suggest DBRX offers a strong balance between performance and efficiency, making it a competitive open-source option.\n",
            "\n",
            "-----------------------\n",
            "\n",
            "Question: 4. **Practical Application:** How can Databricks customers access and utilize DBRX? Describe the different options available (APIs, Hugging Face, Databricks Marketplace, Mosaic AI Model Serving).\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: Databricks customers can access and utilize DBRX in several ways:\n",
            "\n",
            "*   **Databricks Foundation Model APIs:**  Customers can use DBRX via APIs, offering pay-as-you-go pricing and access through the AI Playground chat interface.  For production applications, a provisioned throughput option provides performance guarantees, support for fine-tuned models, and enhanced security and compliance.\n",
            "\n",
            "*   **Hugging Face:** The base model (DBRX Base) and the fine-tuned model (DBRX Instruct) weights are available on Hugging Face under an open license.\n",
            "\n",
            "*   **Databricks Marketplace:**  Customers can download DBRX from the Databricks Marketplace and privately host it by deploying the model on Model Serving.\n",
            "\n",
            "*   **Mosaic AI Model Serving:** DBRX can be hosted on Mosaic AI Model Serving, enabling text generation at up to 150 tok/s/user.\n",
            "\n",
            "-----------------------\n",
            "\n",
            "Question: 5. **Training Methodology:** Summarize the process Databricks used to train DBRX, highlighting the tools and technologies employed (Apache Spark, Unity Catalog, MLflow, MegaBlocks, LLM Foundry, Composer, Streaming, Mosaic AI Training).  What were some of the key scientific and performance challenges overcome during the training process?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: Databricks trained DBRX using a suite of their own tools.  The process involved managing and governing training data with Unity Catalog, processing and cleaning data using Apache Spark and Databricks notebooks, and training the model using optimized versions of their open-source training libraries: MegaBlocks, LLM Foundry, Composer, and Streaming.  Large-scale model training and fine-tuning across thousands of GPUs was managed using their Mosaic AI Training service, with results logged using MLflow.  Human feedback for quality and safety improvements was collected through Mosaic AI Model Serving and Inference Tables.  Manual experimentation was done using the Databricks Playground.\n",
            "\n",
            "The training of mixture-of-experts models presented scientific and performance challenges.  These included building a pipeline robust enough to repeatedly train DBRX-class models efficiently.  Specific details on the challenges aren't explicitly stated, but the text implies difficulties related to the complexity of the MoE architecture and the scale of the training process.\n",
            "\n",
            "-----------------------\n",
            "\n",
            "Question: 1. **Use Case:** Explain how prompt engineering can be used to automate the analysis of product reviews using large language models.  Provide specific examples of questions that could be asked of the LLM to extract actionable insights.\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: Prompt engineering allows businesses to automate the analysis of product reviews using large language models (LLMs) by crafting specific prompts that guide the LLM to extract actionable insights.  Instead of manually reading and summarizing reviews, businesses can feed reviews to the LLM and ask targeted questions.\n",
            "\n",
            "Specific examples of questions that could be asked of the LLM to extract actionable insights include:\n",
            "\n",
            "*   **What are the top three points of negative feedback found across these reviews?** This helps identify recurring issues and areas for product improvement.\n",
            "*   **What features do our customers like best about this product?** This highlights successful aspects of the product and areas to emphasize in marketing.\n",
            "*   **Do customers feel they are receiving sufficient value from the product relative to what they are being asked to pay?** This assesses customer perception of price-to-value and informs pricing strategies.\n",
            "*   **Are there any reviews that are especially negative or are using inappropriate language?** This helps identify reviews requiring immediate attention or moderation.\n",
            "*   **Summarize the overall sentiment of these reviews (positive, negative, neutral).** This provides a high-level overview of customer opinion.\n",
            "*   **Categorize the reviews based on the main topics discussed (e.g., features, usability, customer service).** This organizes feedback for easier analysis.\n",
            "*   **Identify recurring themes or patterns in the negative reviews.** This goes beyond simple keyword searches to uncover underlying problems.\n",
            "*   **What specific suggestions do customers offer for improving the product?** This provides concrete ideas for product development.\n",
            "\n",
            "By asking these types of questions, businesses can leverage LLMs to efficiently analyze large volumes of product reviews, identify key trends, and gain valuable insights for product improvement and marketing.\n",
            "\n",
            "-----------------------\n",
            "\n",
            "Question: 2. **Solution Accelerator:** Describe the \"Solution Accelerator for summarizing product reviews\" mentioned in the text. What dataset was used? What were the key challenges addressed, and how was the solution implemented using Databricks?  What were the cost and performance benefits compared to using a third-party service?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: The Solution Accelerator for summarizing product reviews uses an LLM to automate the analysis of large volumes of customer feedback.  It addresses the challenges of scale and consistency in processing product reviews, allowing product managers to focus on responding to issues rather than simply detecting them.\n",
            "\n",
            "The accelerator uses the Amazon Product Reviews Dataset, containing 51 million user-generated reviews across 2 million distinct books.  This dataset provides a large and diverse range of reviewer content, presenting a scaling challenge representative of many organizations.\n",
            "\n",
            "The solution was implemented using Databricks, which provided the necessary data processing workflow to receive incoming reviews, prepare them for submission to the model, and capture model output for further analysis. Databricks' unified data platform addressed both data engineering and data science requirements without data replication.  The LLM was run in the local Databricks environment, allowing for scalability and cost savings compared to a third-party service.\n",
            "\n",
            "Compared to a third-party service, the Databricks-based solution reduced costs by approximately one-third.  It also enabled faster processing, handling up to 760,000 reviews per hour in one test.  This scalability was achieved without the constraints imposed by an external service.\n",
            "\n",
            "-----------------------\n",
            "\n",
            "Question: 3. **Challenges & Mitigation:** What are the challenges associated with using LLMs for tasks like product review summarization (accuracy, bias)? How can these challenges be mitigated?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: The provided text mentions challenges of accuracy and bias when using LLMs for product review summarization.  It states that standards for identifying inaccuracies and bias are evolving, and that fine-tuning models using approved content can help ensure outputs align with an organization's expectations.  However, it also notes that there is no ideal solution yet.  The text suggests that the results are expected to be better than human-driven processes or simpler models, and that the impact of an errant model can be easily managed when the summaries are for internal consumption.\n",
            "\n",
            "-----------------------\n",
            "\n",
            "Question: 1. **Conceptual:** Explain the concept of RAG. How does it work, and what are its benefits (reduced hallucinations, improved accuracy, cost-effectiveness)?  What are its limitations?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: Retrieval Augmented Generation (RAG) is a technique that enhances the capabilities of off-the-shelf AI systems, specifically large language models (LLMs), by incorporating external knowledge sources.  It works by retrieving relevant information from a knowledge base (e.g., a database, document collection) based on the user's query. This retrieved information is then provided as context to the LLM alongside the original query. The LLM then uses this augmented context to generate a more informed and accurate response.\n",
            "\n",
            "Benefits of RAG include:\n",
            "\n",
            "* **Reduced hallucinations:** By grounding the LLM's response in factual information, RAG minimizes the likelihood of the model generating fabricated or nonsensical information (\"hallucinations\").\n",
            "* **Improved accuracy:**  The inclusion of relevant context leads to more accurate and reliable responses, as the LLM is not solely relying on its pre-trained knowledge.\n",
            "* **Cost-effectiveness:** For many organizations, using RAG with an off-the-shelf LLM is more cost-effective than training a large, custom LLM from scratch, which requires significant computational resources and data.\n",
            "\n",
            "\n",
            "Limitations of RAG:\n",
            "\n",
            "* **Data quality dependence:** The accuracy of RAG's output is heavily reliant on the quality and relevance of the information in the knowledge base. Inaccurate or incomplete data will lead to flawed responses.\n",
            "* **Retrieval limitations:** The effectiveness of RAG depends on the retrieval system's ability to accurately identify and retrieve the most pertinent information.  Poor retrieval can result in the LLM missing crucial context.\n",
            "* **Context window limitations:** LLMs have a limited context window, meaning they can only process a certain amount of text at once.  If the relevant information is too extensive, it may not all be included in the context provided to the LLM.\n",
            "* **Data security and privacy concerns:**  If sensitive data is included in the knowledge base, appropriate security measures must be implemented to protect it.\n",
            "\n",
            "-----------------------\n",
            "\n",
            "Question: 2. **Use Case:** Describe the use case presented for improving RAG application response quality with real-time structured data.  What specific technologies were used (Databricks Vector Search, Databricks Feature & Function Serving, LangChain)?  Provide an example of how this approach could be applied in a different industry (e.g., healthcare, finance).\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: The use case presented for improving RAG application response quality with real-time structured data focuses on a travel planning chatbot.  The chatbot uses real-time structured data (like hotel prices and user preferences such as \"ocean view\" or \"family friendly\") alongside unstructured data (hotel descriptions) to provide personalized recommendations within a user's budget.\n",
            "\n",
            "The specific technologies used were:\n",
            "\n",
            "*   **Databricks Vector Search:** Used to search for relevant context from unstructured data (hotel descriptions).\n",
            "*   **Databricks Feature & Function Serving:** Used to serve real-time structured data (hotel prices, user preferences).\n",
            "*   **LangChain:** Used as the framework to integrate the structured and unstructured data with the LLM to generate the chatbot's responses.\n",
            "\n",
            "An example of how this approach could be applied in the healthcare industry:  A patient care chatbot could use Databricks Vector Search to access unstructured patient notes and medical literature.  Simultaneously, Databricks Feature & Function Serving could provide real-time structured data such as a patient's current vital signs, medication history, and allergies from the Electronic Health Record (EHR). LangChain would then integrate this information with an LLM to generate responses to doctor's queries or provide patients with personalized health information and recommendations.\n",
            "\n",
            "-----------------------\n",
            "\n",
            "Question: 3. **Data Handling:** What are the key considerations for data preparation and management when implementing RAG?  How can tools like Databricks Vector Search and Unity Catalog help address these challenges?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
            "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 250000\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 15\n",
            "}\n",
            "].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: When implementing RAG, key data preparation and management considerations include:\n",
            "\n",
            "*   **Data Consolidation and Cleansing:**  Consolidating relevant data from various sources and cleansing it to ensure accuracy and consistency is crucial.  Data needs to be formatted appropriately for downstream models.  Often, this requires splicing large datasets into smaller, manageable segments.\n",
            "\n",
            "*   **Data Storage and Size:** Data must be stored in sizes appropriate for downstream models.  This often involves segmenting data.\n",
            "\n",
            "*   **Data Security and Governance:**  With RAG, you're often incorporating sensitive data.  Robust security and access controls are essential to protect this information.\n",
            "\n",
            "Databricks Vector Search and Unity Catalog help address these challenges:\n",
            "\n",
            "*   **Databricks Vector Search:** Enables the quick setup of a vector database to store and search data efficiently.\n",
            "\n",
            "*   **Unity Catalog:** Provides granular access controls and data lineage capabilities, ensuring data security and enabling organizations to track data from creation to use.  Any Unity Catalog table with primary keys can be used to serve features in GenAI applications using Databricks Online Tables.\n",
            "\n",
            "-----------------------\n",
            "\n",
            "Question: 1. **Conceptual:** Explain the process of fine-tuning a foundation model.  What are the advantages of fine-tuning over using off-the-shelf models?  When is fine-tuning an appropriate approach?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: Fine-tuning a foundation model involves taking a pre-trained large language model (LLM) and further training it on a more specific dataset.  This dataset is usually smaller and more focused than the massive datasets used for initial pre-training.  The goal is to adapt the general capabilities of the foundation model to a particular task or domain, improving its performance and accuracy on that specific application.\n",
            "\n",
            "Advantages of fine-tuning over using off-the-shelf models include:\n",
            "\n",
            "* **Improved performance and accuracy:** Fine-tuning allows the model to specialize in a particular task or domain, leading to better results than using a general-purpose model.\n",
            "* **Reduced hallucinations:** Fine-tuning on a relevant dataset can help reduce the likelihood of the model generating nonsensical or inaccurate outputs (hallucinations).\n",
            "* **Better domain-specific intelligence:** The model gains a deeper understanding of the specific domain's terminology, concepts, and nuances.\n",
            "* **Cost-effectiveness (potentially):** While fine-tuning requires computational resources, it can be more cost-effective than using a large, powerful off-the-shelf model for a specific task, especially if the task is relatively narrow.\n",
            "\n",
            "\n",
            "Fine-tuning is an appropriate approach when:\n",
            "\n",
            "* You have a specific task or domain in mind.\n",
            "* You have access to a relevant dataset for fine-tuning.\n",
            "* You need improved performance and accuracy over what off-the-shelf models can provide.\n",
            "* You want more control over the model's behavior and output.\n",
            "* Data security and privacy are paramount, as fine-tuning can be done on your own infrastructure with your own data.\n",
            "\n",
            "The decision of whether to fine-tune or use an off-the-shelf model depends on the specific application, available resources, and desired level of performance.\n",
            "\n",
            "-----------------------\n",
            "\n",
            "Question: 2. **Use Case (AI-Generated Documentation):** Detail the experience described in creating a bespoke LLM for AI-generated documentation. What were the initial challenges encountered with using SaaS-based LLMs (quality, performance, cost)? How were these challenges addressed by building a bespoke model?  What model was chosen (MPT-7B), and why?  What tools and technologies were used in the development and deployment process (Databricks LLM fine-tuning, Unity Catalog, Delta Sharing, Databricks optimized LLM serving)?  Include cost and time figures.\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: The use case describes the experience of creating a bespoke LLM for AI-generated documentation.  Initially, a SaaS-based LLM was used, but three challenges arose:\n",
            "\n",
            "1.  **Quality:** The quality of generated documentation was inconsistent and sometimes degraded due to updates to the SaaS LLM.  There was limited control to improve quality beyond basic prompting.\n",
            "\n",
            "2.  **Performance (throughput):**  API quotas limited the throughput, making it too slow to generate documentation for a large number of tables across many organizations.\n",
            "\n",
            "3.  **Cost:** The SaaS LLM's cost was prohibitive unless customers were charged for the feature.\n",
            "\n",
            "To address these challenges, a bespoke model was built. This took two engineers one month and less than $1,000 in compute cost.  The bespoke model improved quality, increased throughput, and significantly reduced cost (more than a 10-fold reduction).\n",
            "\n",
            "The MPT-7B model was chosen because it offered the best combination of quality and inference performance compared to Llama2-7B.  Larger models were considered but deemed unnecessary for this specific task due to increased serving costs and latency.  The smaller model also fit comfortably on more readily available A100 GPUs, leading to higher inference throughput.\n",
            "\n",
            "The following tools and technologies were used:\n",
            "\n",
            "*   **Databricks LLM fine-tuning:**  Simplified the fine-tuning process.\n",
            "*   **Unity Catalog:** Provided governance for both data and models, offering end-to-end lineage traceability.\n",
            "*   **Delta Sharing:** Distributed the model to production regions for faster serving.\n",
            "*   **Databricks optimized LLM serving:**  Improved throughput and latency compared to traditional serving.\n",
            "\n",
            "The project's total cost was less than $1,000 (a few dollars per fine-tuning run), and the final result was a more than 10-fold reduction in cost compared to the SaaS LLM.  The time investment was one month for two engineers.  The MPT-7B model was fine-tuned on approximately 3600 synthetically generated examples in under 15 minutes.  A human evaluation with 62 examples was also conducted.\n",
            "\n",
            "-----------------------\n",
            "\n",
            "Question: 3. **Use Case (LoRA):** Explain the concept of Low-Rank Adaptation (LoRA) and Quantized LoRA (QLoRA) for efficient fine-tuning.  What are the advantages of these methods compared to full fine-tuning?  Describe the experiment conducted using OpenLLaMA-3b-v2, including the dataset used, the hyperparameters explored (r, target_modules), and the key findings regarding optimal parameter selection.  What are the trade-offs between using LoRA and QLoRA?  How are LoRA adapters used in deployment (merging weights)?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
            "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 250000\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 32\n",
            "}\n",
            "].\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
            "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 250000\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 30\n",
            "}\n",
            "].\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
            "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 250000\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 26\n",
            "}\n",
            "].\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
            "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-1.5-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 250000\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 18\n",
            "}\n",
            "].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning method for large language models (LLMs).  Instead of fine-tuning all the weights in the model's weight matrix, LoRA fine-tunes two smaller matrices that approximate the larger matrix. These smaller matrices constitute a LoRA adapter. This adapter is then loaded onto the pre-trained model and used for inference.  The advantage of LoRA over full fine-tuning is that it requires significantly fewer parameters to be updated, resulting in faster training and reduced computational costs.\n",
            "\n",
            "Quantized LoRA (QLoRA) is an even more memory-efficient variant of LoRA.  In QLoRA, the pre-trained model is loaded into GPU memory as quantized 4-bit weights (compared to 8-bit in LoRA), further reducing memory requirements and improving training speed.\n",
            "\n",
            "An experiment was conducted using the OpenLLaMA-3b-v2 model and the Red Dot Design Award Product Descriptions dataset.  The goal was to fine-tune the model to generate product descriptions.  The hyperparameters explored were:\n",
            "\n",
            "*   **r:** The rank of the low-rank matrices (8 and 16 were tested).  Higher r values mean more parameters are updated, potentially improving quality but increasing training time.\n",
            "*   **target_modules:** The modules in the model architecture targeted for adaptation.  Two options were tested:  only the attention blocks (\"q_proj\", \"v_proj\") and all linear layers. Targeting all linear layers increased training time but potentially improved quality.\n",
            "\n",
            "Key findings:\n",
            "\n",
            "*   Increasing `r` beyond a certain point did not significantly improve output quality.\n",
            "*   Targeting all linear layers during adaptation yielded better results than targeting only attention blocks.\n",
            "*   QLoRA did not show a discernible reduction in output quality compared to LoRA, offering significant memory savings.\n",
            "\n",
            "The trade-off between LoRA and QLoRA is primarily memory efficiency. QLoRA uses less memory due to 4-bit quantization, leading to faster training, but it might introduce a slight performance decrease compared to LoRA (though the experiment showed this decrease to be negligible).\n",
            "\n",
            "LoRA adapters can be used in deployment in two ways:\n",
            "\n",
            "1.  **Without merging weights:** The adapter and the pre-trained model are loaded separately during inference. This allows for easy swapping of adapters for different tasks but increases inference latency slightly.\n",
            "2.  **With merging weights:** The adapter's weights are merged with the pre-trained model's weights. This simplifies inference and reduces latency but makes it impossible to easily switch between different adapters.  The merging process can be done with a single line of code using the PEFT library.\n",
            "\n",
            "-----------------------\n",
            "\n",
            "Question: 1. **Conceptual:** Explain the process of pretraining a language model from scratch.  When is pretraining a suitable approach?  What are the key challenges and considerations involved in pretraining (data preprocessing, hyperparameter tuning, resource utilization, handling GPU failures, monitoring and evaluation)?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: Pretraining a language model from scratch involves training a model on a massive corpus of text and/or code data without using any pre-existing weights from another model.  The goal is to create a base model that captures general language understanding and can then be fine-tuned for specific tasks.\n",
            "\n",
            "Pretraining is a suitable approach when:\n",
            "\n",
            "* **Unique data sources:** You have a unique and extensive dataset significantly different from those used to train existing models.\n",
            "* **Domain specificity:** You need a base model tailored to a specific domain (e.g., medical, legal) to ensure domain-specific knowledge is foundational.\n",
            "* **Full control over training data:**  Transparency and control over training data are crucial for security, privacy, and avoiding biases from third-party models.\n",
            "* **Avoiding third-party biases:**  You want to avoid inheriting biases or limitations present in publicly available pretrained models.\n",
            "\n",
            "\n",
            "Key challenges and considerations in pretraining:\n",
            "\n",
            "* **Data preprocessing:**  Requires robust preprocessing of massive datasets (often terabytes or more). This typically involves distributed frameworks like Apache Spark™ and considerations like dataset mix, deduplication, and cleaning to ensure data quality and diversity.\n",
            "\n",
            "* **Hyperparameter tuning:**  Determining optimal hyperparameters is crucial due to the high computational cost of pretraining.  Informed decisions based on smaller-scale searches or prior research are often necessary, followed by full training runs with the chosen parameters.  Tools like MLflow are essential for managing and tracking these experiments.\n",
            "\n",
            "* **Resource utilization:**  Maximizing resource utilization is critical due to the high cost of distributed GPU training. Libraries like MosaicML’s composer optimize model and hardware FLOPs utilization (MFU and HFU) during training.\n",
            "\n",
            "* **Handling GPU failures:**  Long training runs (days or weeks) are susceptible to hardware failures (especially GPUs). Mechanisms to handle failures gracefully (e.g., automatic restarts and checkpointing) are essential.\n",
            "\n",
            "* **Monitoring and evaluation:**  Close monitoring is crucial, including regular saving of model checkpoints and evaluation on validation sets to track performance, convergence, and identify potential issues.\n",
            "\n",
            "-----------------------\n",
            "\n",
            "Question: 2. **Use Case (Stable Diffusion):** Describe the experience of training Stable Diffusion from scratch for under $50K using the MosaicML platform.  What model architecture, dataset, and compute resources were used?  What were the key challenges encountered, and how were they addressed using the MosaicML platform, StreamingDataset, and Composer library?  What optimizations were implemented to reduce training time and cost (xFormers FlashAttention, precomputing latents, low-precision LayerNorm and GroupNorm, FSDP, scheduled EMA)?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: Training Stable Diffusion from scratch for under $50K using the MosaicML platform involved replicating the Stable Diffusion 2 base model.\n",
            "\n",
            "**Model Architecture:** The diffusion model was a ComposerModel composed of a Variational Autoencoder (VAE), a CLIP model, a U-Net, and a diffusion noise scheduler, all from HuggingFace's Diffusers library.  The configurations were based on stabilityai/stable-diffusion-2-base.\n",
            "\n",
            "**Dataset:**  A subset of LAION-5B was used, including samples with English-only captions and an aesthetic score of 4.5+. Training was done in two phases based on image resolution: Phase 1 used images with resolution >=256x256 (790 million samples), and Phase 2 used images with resolution >=512x512 (300 million samples).\n",
            "\n",
            "**Compute Resources:** Both training phases ran on 128 NVIDIA A100 GPUs.  Pre-computing latents required an additional 3,784 A100 hours.  The total compute cost was estimated at $47.7K, assuming $2/A100 hour.\n",
            "\n",
            "**Key Challenges and Solutions:**\n",
            "\n",
            "* **Infrastructure:** The MosaicML platform orchestrated the 128 A100 GPUs and handled automatic scaling for evaluation runs.  The platform's Node Doctor and Watchdog features automatically detected and recovered from hardware failures.\n",
            "* **Efficient Software:** The Composer library maximized training efficiency. Optimizations like Low Precision GroupNorm and LayerNorm, and Fully Sharded Data Parallel (FSDP) were added to achieve near-perfect strong scaling up to 128 GPUs.\n",
            "* **Managing 100TB of Data:** The StreamingDataset library simplified working with the massive dataset, enabling mixing datasets from different locations, instant mid-epoch resumption, and elastic determinism.\n",
            "\n",
            "**Optimizations:**\n",
            "\n",
            "* **xFormers FlashAttention:**  Improved cross-attention block speed in the U-Net, allowing an increase in microbatch size from 4 to 8.\n",
            "* **Precomputing Latents:**  Pre-computing VAE and CLIP latents avoided redundant computations, saving time and memory.  Lower precision (fp16) was used for pre-computation.\n",
            "* **Low-Precision LayerNorm and GroupNorm:**  Using half-precision (fp16) for LayerNorm and GroupNorm layers decreased memory usage, enabling a 4x increase in microbatch size (from 8 to 16).\n",
            "* **FSDP:**  While not strictly necessary for sharding this model, FSDP's SHARD_GRAD_OP mode sped up the gradient update step, improving scaling efficiency.\n",
            "* **Scheduled EMA:**  Applying Exponential Moving Averaging (EMA) only for the final 50,000 steps (3.5% of training) saved significant time and memory.\n",
            "\n",
            "The result was a Stable Diffusion model comparable in quality to the original, trained significantly faster and cheaper than previous attempts.\n",
            "\n",
            "-----------------------\n",
            "\n",
            "Question: 1. **Challenges:** Discuss the challenges involved in evaluating LLMs, including variable performance, lack of ground truth, domain-specific evaluation, and reliance on human judgment.\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: Evaluating LLMs presents several challenges:\n",
            "\n",
            "*   **Variable Performance:** LLMs exhibit sensitivity to prompt variations, performing well on one task but poorly on slightly altered versions.  This inconsistency makes it difficult to establish reliable benchmarks.\n",
            "\n",
            "*   **Lack of Ground Truth:**  LLMs often generate natural language outputs, making it hard to define a single \"correct\" answer.  Traditional NLP metrics struggle to capture the nuances of good versus bad responses.\n",
            "\n",
            "*   **Domain-Specific Evaluation:**  LLMs fine-tuned for specific domains may not be accurately assessed using generic benchmarks.  Domain-specific evaluations are needed to capture their specialized capabilities.\n",
            "\n",
            "*   **Reliance on Human Judgment:**  Evaluating LLM performance in specialized areas often requires human expertise, making the process costly and time-consuming.\n",
            "\n",
            "-----------------------\n",
            "\n",
            "Question: 2. **Best Practices (RAG Applications):** Summarize the best practices for LLM evaluation of RAG applications, as described in the case study on the Databricks Documentation Bot.  What are the key findings regarding the alignment of LLM-as-a-judge with human grading, the effectiveness of providing examples to the LLM judge, appropriate grade scales, and the applicability of evaluation metrics across different use cases?  Describe the methodology used to establish these best practices.\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: The Databricks Documentation Bot case study outlines these best practices for LLM evaluation of RAG applications:\n",
            "\n",
            "**Key Findings:**\n",
            "\n",
            "* **LLM-as-a-judge alignment with human grading:**  LLM judges (using GPT-4) showed over 80% agreement with human graders on correctness and readability for a document-based chatbot.  Agreement increased to over 95% when allowing for a one-score difference (on a 0-3 scale). Comprehensiveness showed less alignment, suggesting subjectivity in that metric.  GPT-3.5, when provided with examples, achieved similar results at a significantly lower cost and higher speed.\n",
            "\n",
            "* **Effectiveness of providing examples:** Providing examples to the LLM judge (GPT-3.5) significantly improved the consistency and usability of its grading. GPT-4 showed less benefit from examples.\n",
            "\n",
            "* **Appropriate grade scales:** Lower-precision grading scales (0-3 or 0-4) were found to be more consistent and easier to interpret than higher-precision scales (0-10 or 0-100) for both human and LLM judges.  Binary grading (0-1) is suitable for simpler metrics.\n",
            "\n",
            "* **Applicability of evaluation metrics across use cases:** Evaluation metrics cannot be directly transferred between different use cases.  RAG applications require their own benchmarks focusing on reading comprehension and instruction following, as opposed to generic benchmarks for casual chat, math, or creative writing.\n",
            "\n",
            "\n",
            "**Methodology:**\n",
            "\n",
            "The study used a three-step process:\n",
            "\n",
            "1. **Generate evaluation dataset:** A dataset of 100 questions and relevant contexts from Databricks documents was created.\n",
            "\n",
            "2. **Generate answer sheets:** Different LLMs (GPT-4, GPT-3.5, Claude-v1, Llama2-70b-chat, Vicuna-33b, and mpt-30b-chat) generated answers to the questions based on the provided contexts.\n",
            "\n",
            "3. **Generate grades:**  LLMs were used to grade the answers based on correctness, comprehensiveness, and readability (weighted 60%, 20%, and 20%, respectively).  Techniques to improve reliability included low temperature, single-answer grading, chain-of-thought prompting, and few-shot learning (providing examples in the grading rubric).  Results were compared against human grading from a labeling company.  Experiments varied the LLM judge, the inclusion of examples, and the grading scale to determine best practices.\n",
            "\n",
            "-----------------------\n",
            "\n",
            "Question: 3. **Offline LLM Evaluation (MLflow):**  Describe the step-by-step process for offline LLM evaluation on Databricks using MLflow, focusing on the evaluation of prompts, the retrieval system, and the foundation LLM.  What tools and techniques were used (Databricks Model Serving, Databricks AI Playground, LangChain, Databricks Vector Search, MLflow Evaluate API)?  Explain how different metrics (precision@k, recall@k, NDCG@k, relevance, latency) can be used to assess the performance of each component.  How can synthetic data be used to create a ground truth dataset for evaluation?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: The provided text describes a step-by-step process for offline LLM evaluation on Databricks using MLflow, focusing on evaluating prompts, the retrieval system, and the foundation LLM.  Here's a summary based on the document:\n",
            "\n",
            "**Step 1: Set up External Models in Databricks (Databricks Model Serving)**\n",
            "\n",
            "*   Use Databricks Model Serving to manage and access external LLMs (e.g., Azure OpenAI GPT, Anthropic Claude, AWS Bedrock). This provides a unified interface for querying multiple LLMs and centralizes API key management.\n",
            "\n",
            "**Step 2: Explore Prompts with the Databricks AI Playground (Databricks AI Playground)**\n",
            "\n",
            "*   Use the Databricks AI Playground to experiment with different prompts and parameters for your chosen LLM without writing code.  This allows for quick testing and comparison of various prompt options to identify the most effective ones.\n",
            "\n",
            "**Step 3: Add Model and Parameters to Your GenAI Application (LangChain)**\n",
            "\n",
            "*   Integrate the best-performing prompt and LLM configuration identified in Step 2 into your GenAI application using a framework like LangChain.  The example uses LangChain to create a QA chatbot.\n",
            "\n",
            "**Step 4: Create a Sample GenAI App (LangChain, Databricks Foundation Model API, Databricks Vector Search)**\n",
            "\n",
            "*   Build a GenAI application (e.g., a QA chatbot) using LangChain. This example uses:\n",
            "    *   A vector database (Databricks Vector Search is recommended for scaling) to store and retrieve relevant documents.\n",
            "    *   An embedding model (Databricks Foundation Model API is used in the example) to generate vector representations of the documents and queries.\n",
            "    *   The chosen LLM (from Step 1) to generate answers based on the retrieved documents and the prompt.\n",
            "\n",
            "**Step 5: Evaluation of Retrieval System with MLflow (MLflow Evaluate API, Databricks Vector Search)**\n",
            "\n",
            "*   **Creating a Ground Truth Dataset:**  The document suggests using an LLM to generate synthetic data for a quick evaluation, but emphasizes the importance of creating a dataset that mirrors real-world usage for more thorough evaluation.  This synthetic dataset provides the \"ground truth\" for comparison.\n",
            "*   **Evaluation Metrics:** The `mlflow.evaluate` API is used with metrics like `precision@k`, `recall@k`, and `NDCG@k` to assess the retrieval system's performance.  These metrics measure how well the system retrieves relevant documents for a given query.  Different `k` values (number of top results considered) can be tested.\n",
            "\n",
            "**Step 6: Evaluation of GenAI Results with MLflow (MLflow Evaluate API)**\n",
            "\n",
            "*   The `mlflow.evaluate` API is used again, this time to evaluate the quality of the GenAI application's responses.  The example uses a `relevance` metric (defined within MLflow) to assess how relevant the generated answers are to the questions and context.  Latency is also measured.  The document also suggests using an LLM (like GPT-4) as a judge to provide additional evaluation.\n",
            "\n",
            "**Metrics Summary:**\n",
            "\n",
            "*   **`precision@k`:**  The proportion of relevant documents among the top `k` retrieved documents.\n",
            "*   **`recall@k`:** The proportion of relevant documents retrieved among all relevant documents.\n",
            "*   **`NDCG@k`:** Normalized Discounted Cumulative Gain, which considers the ranking of relevant documents.\n",
            "*   **`relevance`:** A metric (defined within MLflow) assessing the relevance of the generated answer to the question and context.\n",
            "*   **`latency`:** The time taken to generate a response.\n",
            "\n",
            "\n",
            "In short, this process uses Databricks' integrated tools to build and evaluate a GenAI application, leveraging MLflow for automated evaluation and tracking of results.  The process emphasizes iterative improvement by allowing for easy experimentation with prompts, LLMs, and retrieval strategies.\n",
            "\n",
            "-----------------------\n",
            "\n",
            "Question: These questions cover the key concepts and practical aspects of generative AI development as presented in the provided text.  They are designed to encourage critical thinking and application of knowledge, preparing coders/programmers for a comprehensive exam.\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: Okay, I understand.  Please provide the questions.  I will do my best to answer them based on the provided text.  If a question requires information not present in the text, I will state that I don't know.\n",
            "\n",
            "-----------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U_RxY1223ddH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}